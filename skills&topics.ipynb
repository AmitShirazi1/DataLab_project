{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8968777d-5809-4774-8736-754b28d5908c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### loading all_jobpostings_with_skills into a spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a104a8-f71e-449c-9df0-af0975b4287b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, StringType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import os\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SkillsTopicsScore\").getOrCreate()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "job_skills_spark = open_csv_file(spark, JOBS_PATH, 'all_jobpostings_with_skills.csv')\n",
    "# job_skills_spark.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af6cbc27-d6fa-4626-b594-2edec5d5cc49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### getting all unique skills from all_jobpostings_with_skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeb6b7ad-df8d-459d-bcb1-cf8912dc75b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Split the string in the 'skills' column by commas and explode it\n",
    "job_skills_spark_exploded = job_skills_spark.select(\"skills\").distinct().withColumn(\"exploded_skills\", F.explode(F.split(F.col(\"skills\"), \",\\s*\")))\n",
    "\n",
    "# Show the result\n",
    "unique_skills = job_skills_spark_exploded.select(\"exploded_skills\").distinct()\n",
    "# unique_skills.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d9c104d3-4d3f-459d-8284-799a934b37be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### getting all topics of all questions (unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0b3e7ef-c165-45d2-a9ed-963be756a022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, StringType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import os\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "problems_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_code_questions_with_topics.csv')\n",
    "# problems_spark.display()\n",
    "\n",
    "open_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions_with_topics.csv')\n",
    "# open_questions_spark.display()\n",
    "\n",
    "questions_topics_spark = problems_spark.select([\"question\", \"topics\"]).union(open_questions_spark.select([\"question\", \"topics\"]))\n",
    "# questions_topics_spark.display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0accb6d5-fd1e-46d2-b32e-23e3904e7712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### getting all unique topics from questions_topics_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dca8096-43b3-4917-8c20-561899048846",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Split the string in the 'topics' column by commas and explode it\n",
    "questions_topics_spark_exploded = questions_topics_spark.select(\"topics\").distinct().withColumn(\"exploded_topics\", F.explode(F.split(F.col(\"topics\"), \",\\s*\")))\n",
    "\n",
    "# Show the result\n",
    "unique_topics = questions_topics_spark_exploded.select(\"exploded_topics\").distinct()\n",
    "# unique_topics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "31b4d782-e38d-4e29-91c1-b894511b6399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Embeddings for each unique topic and unique skill and calculating cosine similarity between each pair of unique topic and unique skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9dd8c780-060d-430f-8b14-1b7417d91a99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Explode `skills` and `topics`\n",
    "job_skills_spark_exploded = job_skills_spark.withColumn(\n",
    "    \"exploded_skills\", F.explode(F.split(F.col(\"skills\"), \",\\\\s*\"))\n",
    ")\n",
    "\n",
    "questions_topics_spark_exploded = questions_topics_spark.withColumn(\n",
    "    \"exploded_topics\", F.explode(F.split(F.col(\"topics\"), \",\\\\s*\"))\n",
    ")\n",
    "\n",
    "# Step 2: Collect unique skills and topics for embedding\n",
    "unique_skills = [row[\"exploded_skills\"] for row in job_skills_spark_exploded.select(\"exploded_skills\").distinct().collect()]\n",
    "unique_topics = [row[\"exploded_topics\"] for row in questions_topics_spark_exploded.select(\"exploded_topics\").distinct().collect()]\n",
    "\n",
    "# Step 3: Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "skills_embeddings = model.encode(unique_skills)\n",
    "topics_embeddings = model.encode(unique_topics)\n",
    "\n",
    "# Step 4: Map skills and topics to their embeddings\n",
    "skills_to_embeddings = {skill: embedding for skill, embedding in zip(unique_skills, skills_embeddings)}\n",
    "topics_to_embeddings = {topic: embedding for topic, embedding in zip(unique_topics, topics_embeddings)}\n",
    "\n",
    "# Step 5: Define a UDF to calculate similarity\n",
    "def calculate_similarity(topic, skill):\n",
    "    topic_emb = topics_to_embeddings.get(topic) # topics_to_embeddings[topic]\n",
    "    skill_emb = skills_to_embeddings.get(skill)\n",
    "    if topic_emb is not None and skill_emb is not None:\n",
    "        return float(cosine_similarity([topic_emb], [skill_emb])[0][0])\n",
    "    return None\n",
    "\n",
    "similarity_udf = F.udf(calculate_similarity, \"double\")\n",
    "\n",
    "# Step 6: Cross join the exploded DataFrames and calculate similarity\n",
    "cross_joined = questions_topics_spark_exploded.crossJoin(job_skills_spark_exploded)\n",
    "similarity_scores = cross_joined.withColumn(\n",
    "    \"similarity_score\", similarity_udf(F.col(\"exploded_topics\"), F.col(\"exploded_skills\"))\n",
    ")\n",
    "\n",
    "# Step 7: Average similarity scores for each topic and job_title\n",
    "topic_job_avg = similarity_scores.groupBy(\"exploded_topics\", \"job_title\").agg(\n",
    "    F.avg(\"similarity_score\").alias(\"avg_similarity_per_topic\")\n",
    ")\n",
    "\n",
    "# Step 8: Average topic scores for each question and job_title\n",
    "# Join back with questions to map topics to questions\n",
    "question_topic_mapping = questions_topics_spark_exploded.select(\"question\", \"exploded_topics\").distinct()\n",
    "\n",
    "question_job_avg = topic_job_avg.join(\n",
    "    question_topic_mapping, on=\"exploded_topics\"\n",
    ").groupBy(\"question\", \"job_title\").agg(\n",
    "    F.avg(\"avg_similarity_per_topic\").alias(\"avg_similarity_per_question\")\n",
    ")\n",
    "question_job_avg.display()\n",
    "# Step 9: Show the results\n",
    "# question_job_avg.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b4c8a2-243d-454a-90a7-ba99188c3b73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "  --make sure it embedds each skill and each topic when bringing a list into an embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57ec970b-13c7-43e7-ae24-50522a136aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Step 1: Explode `skills` and `topics`\n",
    "job_skills_spark_exploded = job_skills_spark.withColumn(\n",
    "    \"exploded_skills\", F.explode(F.split(F.col(\"skills\"), \",\\\\s*\"))\n",
    ")\n",
    "\n",
    "questions_topics_spark_exploded = questions_topics_spark.withColumn(\n",
    "    \"exploded_topics\", F.explode(F.split(F.col(\"topics\"), \",\\\\s*\"))\n",
    ")\n",
    "\n",
    "# Step 2: Collect unique skills and topics for embedding\n",
    "unique_skills = [row[\"exploded_skills\"] for row in job_skills_spark_exploded.select(\"exploded_skills\").distinct().collect()]\n",
    "unique_topics = [row[\"exploded_topics\"] for row in questions_topics_spark_exploded.select(\"exploded_topics\").distinct().collect()]\n",
    "\n",
    "# Step 3: Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "skills_embeddings = model.encode(unique_skills)\n",
    "topics_embeddings = model.encode(unique_topics)\n",
    "\n",
    "# Step 4: Compute similarity in Python\n",
    "skills_df = pd.DataFrame({\"skill\": unique_skills, \"embedding\": list(skills_embeddings)})\n",
    "topics_df = pd.DataFrame({\"topic\": unique_topics, \"embedding\": list(topics_embeddings)})\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_records = []\n",
    "for _, topic_row in topics_df.iterrows():\n",
    "    for _, skill_row in skills_df.iterrows():\n",
    "        score = float(cosine_similarity([topic_row[\"embedding\"]], [skill_row[\"embedding\"]])[0][0])\n",
    "        similarity_records.append((topic_row[\"topic\"], skill_row[\"skill\"], score))\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_records, columns=[\"topic\", \"skill\", \"similarity_score\"])\n",
    "\n",
    "# Step 5: Create a Spark DataFrame from similarity_df\n",
    "similarity_spark = spark.createDataFrame(similarity_df)\n",
    "\n",
    "# Step 6: Aggregate scores\n",
    "# Join similarity scores with the exploded DataFrames\n",
    "# taking an average of the similarity scores over all skills of a job for each topic\n",
    "job_skills_mapping = job_skills_spark_exploded.select(\"job_title\", \"exploded_skills\").distinct()\n",
    "\n",
    "topic_job_avg = similarity_spark.join(\n",
    "    job_skills_mapping,\n",
    "    similarity_spark[\"skill\"] == job_skills_mapping[\"exploded_skills\"]\n",
    ").groupBy(\"topic\", \"job_title\").agg(\n",
    "    F.avg(\"similarity_score\").alias(\"avg_job_similarity_per_topic\")\n",
    ")\n",
    "\n",
    "question_topic_mapping = questions_topics_spark_exploded.select(\"question\", \"exploded_topics\").distinct()\n",
    "\n",
    "question_job_avg = topic_job_avg.join(\n",
    "    question_topic_mapping, topic_job_avg[\"topic\"] == question_topic_mapping[\"exploded_topics\"]\n",
    ").groupBy(\"question\", \"job_title\").agg(\n",
    "    F.avg(\"avg_job_similarity_per_topic\").alias(\"avg_job_similarity_per_question\")\n",
    ")\n",
    "\n",
    "# Step 7: Display results\n",
    "question_job_avg.display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8e2b98b-c0d8-4a6b-aca2-a85eef3ab1d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### trying on a smaller subset of job_skills_spark and questions_topics_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df023f8e-d9ea-4d06-92e6-53de2218f87c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"SubsetExample\").getOrCreate()\n",
    "\n",
    "# Create a smaller subset of job_skills_spark\n",
    "job_skills_data_subset = [\n",
    "    (\"Data Scientist\", \"Machine Learning, Python\"),\n",
    "    (\"Software Engineer\", \"Python, Java\"),\n",
    "]\n",
    "job_skills_spark_subset = spark.createDataFrame(job_skills_data_subset, [\"job_title\", \"skills\"])\n",
    "\n",
    "# Create a smaller subset of questions_topics_spark\n",
    "questions_topics_data_subset = [\n",
    "    (\"What is Python used for?\", \"Python, Programming\"),\n",
    "    (\"How does machine learning work?\", \"Machine Learning, AI\"),\n",
    "]\n",
    "questions_topics_spark_subset = spark.createDataFrame(questions_topics_data_subset, [\"question\", \"topics\"])\n",
    "\n",
    "# Explode skills and topics\n",
    "job_skills_spark_exploded = job_skills_spark_subset.withColumn(\n",
    "    \"exploded_skills\", F.explode(F.split(F.col(\"skills\"), \",\\\\s*\"))\n",
    ")\n",
    "\n",
    "questions_topics_spark_exploded = questions_topics_spark_subset.withColumn(\n",
    "    \"exploded_topics\", F.explode(F.split(F.col(\"topics\"), \",\\\\s*\"))\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Step 2: Collect unique skills and topics for embedding\n",
    "unique_skills = [row[\"exploded_skills\"] for row in job_skills_spark_exploded.select(\"exploded_skills\").distinct().collect()]\n",
    "unique_topics = [row[\"exploded_topics\"] for row in questions_topics_spark_exploded.select(\"exploded_topics\").distinct().collect()]\n",
    "\n",
    "# Step 3: Generate embeddings\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "skills_embeddings = model.encode(unique_skills)\n",
    "topics_embeddings = model.encode(unique_topics)\n",
    "\n",
    "# Step 4: Compute similarity in Python\n",
    "skills_df = pd.DataFrame({\"skill\": unique_skills, \"embedding\": list(skills_embeddings)})\n",
    "topics_df = pd.DataFrame({\"topic\": unique_topics, \"embedding\": list(topics_embeddings)})\n",
    "\n",
    "# Calculate similarity scores\n",
    "similarity_records = []\n",
    "for _, topic_row in topics_df.iterrows():\n",
    "    for _, skill_row in skills_df.iterrows():\n",
    "        score = float(cosine_similarity([topic_row[\"embedding\"]], [skill_row[\"embedding\"]])[0][0])\n",
    "        similarity_records.append((topic_row[\"topic\"], skill_row[\"skill\"], score))\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_records, columns=[\"topic\", \"skill\", \"similarity_score\"])\n",
    "\n",
    "# Step 5: Create a Spark DataFrame from similarity_df\n",
    "similarity_spark = spark.createDataFrame(similarity_df)\n",
    "\n",
    "# Step 6: Aggregate scores\n",
    "# Join similarity scores with the exploded DataFrames\n",
    "topic_job_avg = similarity_spark.join(\n",
    "    job_skills_spark_exploded,\n",
    "    similarity_spark[\"skill\"] == job_skills_spark_exploded[\"exploded_skills\"]\n",
    ").groupBy(\"topic\", \"job_title\").agg(\n",
    "    F.avg(\"similarity_score\").alias(\"avg_similarity_per_topic\")\n",
    ")\n",
    "\n",
    "question_topic_mapping = questions_topics_spark_exploded.select(\"question\", \"exploded_topics\").distinct()\n",
    "\n",
    "question_job_avg = topic_job_avg.join(\n",
    "    question_topic_mapping, topic_job_avg[\"topic\"] == question_topic_mapping[\"exploded_topics\"]\n",
    ").groupBy(\"question\", \"job_title\").agg(\n",
    "    F.avg(\"avg_similarity_per_topic\").alias(\"avg_similarity_per_question\")\n",
    ")\n",
    "\n",
    "# Step 7: Display results\n",
    "question_job_avg.show(truncate=False)\n",
    "\n",
    "\n",
    "# # Show the smaller datasets\n",
    "# print(\"Job Skills Spark Exploded (Subset):\")\n",
    "# job_skills_spark_exploded.show(truncate=False)\n",
    "\n",
    "# print(\"Questions Topics Spark Exploded (Subset):\")\n",
    "# questions_topics_spark_exploded.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1ed8cc85-a6ad-45c5-9cdf-f4ddeaecaaba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af51ec06-90c8-4324-9b3c-4adfde8c5293",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "skills = [row[\"exploded_skills\"] for row in unique_skills.collect()]\n",
    "topics = [row[\"exploded_topics\"] for row in unique_topics.collect()]\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and efficient\n",
    "\n",
    "# Generate embeddings for topics and skills\n",
    "skills_embeddings = model.encode(skills)\n",
    "topics_embeddings = model.encode(topics)\n",
    "\n",
    "# Calculate compatibility scores\n",
    "compatibility_scores = cosine_similarity(topics_embeddings, skills_embeddings)\n",
    "\n",
    "# print(compatibility_scores)\n",
    "\n",
    "# Display scores\n",
    "for i, topic in enumerate(topics):\n",
    "    print(f\"Topic: {topic}\")\n",
    "    for j, skill in enumerate(skills):\n",
    "        print(f\"  Skill: {skill} -> Score: {compatibility_scores[i][j]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1674bfde-a1a0-4344-87a9-5950a3875aba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "job_skills_spark\n",
    "questions_topics_spark\n",
    "# Map topics to their index in the topics list\n",
    "topic_to_index = {topic: idx for idx, topic in enumerate(topics)}\n",
    "\n",
    "# Initialize a dictionary to store the final scores\n",
    "question_scores = defaultdict(float)\n",
    "\n",
    "# Calculate the average score for each question\n",
    "for question, question_topics in question_topics_mapping.items():\n",
    "    topic_scores = []\n",
    "    for topic in question_topics:\n",
    "        if topic in topic_to_index:  # Ensure the topic exists in the topics list\n",
    "            topic_idx = topic_to_index[topic]\n",
    "            # Average the topic's score over all skills\n",
    "            avg_topic_score = compatibility_scores[topic_idx].mean()\n",
    "            topic_scores.append(avg_topic_score)\n",
    "    # Calculate the overall average score for the question\n",
    "    if topic_scores:  # Avoid division by zero\n",
    "        question_scores[question] = sum(topic_scores) / len(topic_scores)\n",
    "\n",
    "# Display the results\n",
    "for question, avg_score in question_scores.items():\n",
    "    print(f\"Question: {question} -> Average Compatibility Score: {avg_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bff586d-b355-4298-8e5f-e640c77e903b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "\n",
    "# 1. Define Your Data\n",
    "# Example labeled data: each entry has a question, a skill, and a compatibility label\n",
    "# data = [\n",
    "#     {\"question\": \"Sorting and searching algorithms\", \"skill\": \"Sorting algorithms\", \"label\": 1.0},\n",
    "#     {\"question\": \"Graph traversal\", \"skill\": \"Sorting algorithms\", \"label\": 0.0},\n",
    "#     {\"question\": \"Dynamic programming on trees\", \"skill\": \"Graph theory\", \"label\": 1.0},\n",
    "#     {\"question\": \"Binary search optimization\", \"skill\": \"Data structures\", \"label\": 0.5},\n",
    "#     {\"question\": \"Tree traversal\", \"skill\": \"Graph theory\", \"label\": 1.0},\n",
    "# ]\n",
    "\n",
    "\n",
    "# Collect the topics into a list\n",
    "skills = [row[\"exploded_skills\"] for row in unique_skills.collect()]\n",
    "topics = [row[\"exploded_topics\"] for row in unique_topics.collect()]\n",
    "\n",
    "# # test data with labeling option\n",
    "# data = []\n",
    "# for topic in topics:\n",
    "#     for skill in skills:\n",
    "#         # For demonstration purposes, use a dummy label. You should replace this with actual label logic.\n",
    "#         label = 1.0 if topic in skill else 0.0  # Example: if topic is in skill, label it as 1.0\n",
    "#         data.append({\"question\": topic, \"skill\": skill, \"label\": label})\n",
    "\n",
    "data = []\n",
    "for topic in topics:\n",
    "    for skill in skills:\n",
    "        data.append({\"topic\": topic, \"skill\": skill})\n",
    "\n",
    "# Convert to Dataset for easy manipulation\n",
    "dataset = Dataset.from_list(data)\n",
    "\n",
    "# 2. Convert Dataset to Sentence Transformers InputExample Format\n",
    "train_examples = [\n",
    "    InputExample(texts=[row[\"topic\"], row[\"skill\"]])\n",
    "    for row in data\n",
    "]\n",
    "\n",
    "# 3. Load Pre-trained Sentence Transformer Model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and effective for semantic similarity\n",
    "\n",
    "# 4. Create DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "# 5. Define Loss Function\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# 6. Fine-Tune the Model\n",
    "print(\"Starting fine-tuning...\")\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=3,  # Number of epochs for training\n",
    "    warmup_steps=100,  # Warmup steps for learning rate scheduler\n",
    "    show_progress_bar=True\n",
    ")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model_save_path = \"fine_tuned_question_skill_model\"\n",
    "model.save(model_save_path)\n",
    "print(f\"Model fine-tuned and saved to {model_save_path}\")\n",
    "\n",
    "# 7. Evaluate the Model\n",
    "# Test data for evaluation\n",
    "test_data = [\n",
    "    {\"question\": \"Graph traversal\", \"skill\": \"Graph theory\", \"label\": 1.0},\n",
    "    {\"question\": \"Sorting algorithms\", \"skill\": \"Graph theory\", \"label\": 0.0},\n",
    "    {\"question\": \"Dynamic programming\", \"skill\": \"Data structures\", \"label\": 0.5},\n",
    "]\n",
    "\n",
    "# Convert test data to format for predictions\n",
    "test_examples = [(row[\"question\"], row[\"skill\"]) for row in test_data]\n",
    "test_labels = [row[\"label\"] for row in test_data]\n",
    "\n",
    "# Get predictions and display results\n",
    "print(\"\\nEvaluating the fine-tuned model...\")\n",
    "for i, (question, skill) in enumerate(test_examples):\n",
    "    question_embedding = model.encode(question)\n",
    "    skill_embedding = model.encode(skill)\n",
    "    similarity_score = torch.nn.functional.cosine_similarity(\n",
    "        torch.tensor(question_embedding), torch.tensor(skill_embedding), dim=0\n",
    "    ).item()\n",
    "    print(f\"Question: '{question}' | Skill: '{skill}' | Predicted Score: {similarity_score:.2f} | True Label: {test_labels[i]}\")\n",
    "\n",
    "# The predicted scores should align more closely with the labels after fine-tuning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "326f05d7-6e17-4fce-b4ad-3ae632b03e18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Example data\n",
    "questions = [\"Dynamic programming on trees\", \"Sorting and searching algorithms\", \"Graph traversal\"]\n",
    "skills = [\"Graph theory\", \"Sorting algorithms\", \"Data structures\"]\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight and efficient\n",
    "\n",
    "# Generate embeddings for questions and skills\n",
    "questions_embeddings = model.encode(questions)\n",
    "skills_embeddings = model.encode(skills)\n",
    "\n",
    "# Calculate compatibility scores\n",
    "compatibility_scores = cosine_similarity(questions_embeddings, skills_embeddings)\n",
    "\n",
    "# Display scores\n",
    "for i, question in enumerate(questions):\n",
    "    print(f\"Question: {question}\")\n",
    "    for j, skill in enumerate(skills):\n",
    "        print(f\"  Skill: {skill} -> Score: {compatibility_scores[i][j]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61cf2729-7f8e-4bf8-b088-9c9a8613f175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### getting all unique topics from all_code_questions_with_topics and all_open_questions_with_topics\n",
    "* NO NEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bb122e4-c0d6-46f0-b61a-31e43883b40e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Split the string in the 'topics' column by commas and explode it\n",
    "problems_spark_exploded = problems_spark.select(\"topics\").distinct().withColumn(\"exploded_topics\", F.explode(F.split(F.col(\"topics\"), \",\\s*\")))\n",
    "\n",
    "# Show the result\n",
    "unique_topics_problems = problems_spark_exploded.select(\"exploded_topics\").distinct()\n",
    "unique_topics_problems.display()\n",
    "\n",
    "# Split the string in the 'topics' column by commas and explode it\n",
    "open_questions_spark_exploded = open_questions_spark.select(\"topics\").distinct().withColumn(\"exploded_topics\", F.explode(F.split(F.col(\"topics\"), \",\\s*\")))\n",
    "\n",
    "# Show the result\n",
    "unique_topics_open_questions = open_questions_spark_exploded.select(\"exploded_topics\").distinct()\n",
    "unique_topics_open_questions.display()\n",
    "\n",
    "unique_topics = unique_topics_problems.union(unique_topics_open_questions)\n",
    "unique_topics.display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "skills&topics",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

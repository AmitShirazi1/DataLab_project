{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96854a3d-e214-4eef-aa5a-9d2097f875d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, lit, udf, split\n",
    "from pyspark.sql.types import StringType\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UnifyDatasets\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "from consts import QUESTIONS_PATH, open_csv_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58aa2743-0aa8-44ed-a929-d59b46c08bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### All code questions with their solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd27b44-f503-4480-ae67-f56604e1695e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, regexp_extract, size\n",
    "\n",
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Remove HTML tags and normalize text from the given HTML content.\n",
    "    Handles missing or invalid content gracefully.\n",
    "    \"\"\"\n",
    "    if not isinstance(html_content, str):\n",
    "        return None\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    clean_text = soup.get_text(separator=\" \")\n",
    "    return \" \".join(clean_text.split())\n",
    "\n",
    "# Register the clean_html_content function as a UDF\n",
    "clean_html_content_udf = udf(clean_html_content, StringType())\n",
    "\n",
    "def get_solution_url(problem_number):\n",
    "    base_url = \"https://github.com/fishercoder1534/Leetcode/blob/master/src/main/java/com/fishercoder/solutions/\"\n",
    "    if problem_number < 1000:\n",
    "        folder = \"firstthousand\"\n",
    "    elif problem_number < 2000:\n",
    "        folder = \"secondthousand\"\n",
    "    elif problem_number < 3000:\n",
    "        folder = \"thirdthousand\"\n",
    "    else:\n",
    "        folder = \"fourththousand\"\n",
    "    return f\"{base_url}{folder}/_{problem_number}.java\"\n",
    "\n",
    "def fetch_solution(url):\n",
    "    \"\"\"\n",
    "    Fetch the raw content of the solution from the GitHub file URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the GitHub URL to the raw content URL\n",
    "        raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        \n",
    "        # Fetch the solution content\n",
    "        response = requests.get(raw_url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            # Failed to fetch solution - ignore\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching solution from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def unify_leetcode_datasets():\n",
    "    leetcode_problems_content = open_csv_file(spark, QUESTIONS_PATH, \"leetcode_problems_data.csv\") \\\n",
    "        .drop(\"title\", \"likes\", \"dislikes\") \\\n",
    "        .withColumnRenamed(\"slug\", \"formatted_title\") \\\n",
    "        .withColumn(\"formatted_title\", lower(col(\"formatted_title\"))) \\\n",
    "        .withColumn(\"question\", clean_html_content_udf(col(\"content\"))) \\\n",
    "        .drop(\"content\") \\\n",
    "        .filter(col(\"question\").isNotNull())\n",
    "    # Original columns: question_id,title,content,difficulty,likes,dislikes,slug.\n",
    "    # New columns: question_id,formatted_title,question,difficulty.\n",
    "    \n",
    "    leetcode_problems_meta = open_csv_file(spark, QUESTIONS_PATH, \"leetcode_problems_metadata.csv\") \\\n",
    "        .drop(\"page_number\", \"is_premium\", \"title\", \"accepted\", \"submission\", \"solution\", \"discussion_count\", \"likes\", \"dislikes\") \\\n",
    "        .withColumn(\"formatted_title\", split(col(\"problem_URL\"), \"/\").getItem(4)) \\\n",
    "        .drop(\"problem_URL\") \\\n",
    "        .withColumnRenamed(\"id\", \"question_id\") \\\n",
    "        .withColumnRenamed(\"problem_description\", \"question\") \\\n",
    "        .withColumnRenamed(\"topic_tags\", \"topics\") \\\n",
    "        .filter(col(\"question\").isNotNull())\n",
    "    # Original columns: id,page_number,is_premium,title,problem_description,topic_tags,difficulty,similar_questions,no_similar_questions,acceptance,accepted,submission,solution,discussion_count,likes,dislikes,problem_URL,solution_URL.\n",
    "    # New columns: question_id,question,topics,difficulty,similar_questions,no_similar_questions,acceptance, formatted_title,solution_URL.\n",
    "    \n",
    "    pattern = r\"/_([0-9]+)\\.java\"\n",
    "    leetcode_links = open_csv_file(spark, QUESTIONS_PATH, \"leetcode_problems&solutions_links.csv\") \\\n",
    "        .drop(\"name\") \\\n",
    "        .withColumn(\"formatted_title\", split(col(\"link\"), \"/\").getItem(4)) \\\n",
    "        .withColumn(\n",
    "            \"solution_idx\",\n",
    "            regexp_extract(col(\"solution\"), pattern, 1)  # Extract the first capturing group\n",
    "        ) \\\n",
    "        .drop(\"link\", \"solution\")\n",
    "    # Original columns: name,link,difficulty,solution.\n",
    "    # New columns: formatted_title,difficulty,solution_idx.\n",
    "\n",
    "    # Fetch solutions\n",
    "    for row in leetcode_links.collect():\n",
    "        idx = row[\"solution_idx\"]\n",
    "        if idx and idx.isdigit():\n",
    "            solution_url = get_solution_url(int(idx))\n",
    "            solution_content = fetch_solution(solution_url)\n",
    "            leetcode_links = leetcode_links.withColumn(\"solution\", lit(solution_content))\n",
    "    leetcode_links = leetcode_links.drop(\"solution_idx\")\n",
    "    # New columns: formatted_title,difficulty,solution.\n",
    "    \n",
    "    # Merge datasets\n",
    "    merge1 = leetcode_problems_content.join(leetcode_problems_meta, [\"question_id\", \"formatted_title\", \"question\", \"difficulty\"], \"outer\")\n",
    "        # .withColumn(\"solution\", lit(None).cast(StringType()))\n",
    "    merge2 = merge1.join(leetcode_links, [\"formatted_title\", \"difficulty\"], \"outer\") \\\n",
    "        .filter(col(\"question\").isNotNull())\n",
    "\n",
    "    return merge2\n",
    "\n",
    "leetcode_with_solutions = unify_leetcode_datasets()\n",
    "display(leetcode_with_solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f449ae6-1943-412f-843a-ebd39844477c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### General & DS open questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e54eb8b-c839-4893-a0ea-0a1a622d5851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "def unify_open_questions_datasets():\n",
    "    \"\"\" Load and preprocess the open-ended question datasets. \"\"\"\n",
    "    data_science_questions = open_csv_file(spark, QUESTIONS_PATH, \"open_questions_data_science.csv\") \\\n",
    "        .withColumnRenamed(\"DESCRIPTION\", \"question\") \\\n",
    "        .withColumnRenamed(\"ID\", \"question_id\") \\\n",
    "        .withColumn(\"category\", lit(\"Data Science\")) \\\n",
    "        .withColumn(\"topics\", lit(\"Data Science\"))\n",
    "    # Original columns: ID,DESCRIPTION\n",
    "    # New columns: question_id,question,category,topics\n",
    "    \n",
    "    num_ds_rows = data_science_questions.count()\n",
    "    window_spec = Window.orderBy(lit(0))\n",
    "\n",
    "    general_questions = open_csv_file(spark, QUESTIONS_PATH, \"general_open_questions.csv\") \\\n",
    "        .withColumn(\"index\", row_number().over(window_spec) - 1) \\\n",
    "        .withColumn(\"question_id\", lit(num_ds_rows) + col(\"index\") + 1) \\\n",
    "        .withColumn(\"category\", lit(\"General\")) \\\n",
    "        .withColumn(\"topics\", lit(\"Soft Skills\")) \\\n",
    "        .drop(\"index\")\n",
    "    # Original columns: question\n",
    "    # New columns: question_id,question,category,topics\n",
    "\n",
    "    # Concatenate both into one database\n",
    "    open_questions_df = data_science_questions.unionByName(general_questions)\n",
    "    \n",
    "    return open_questions_df\n",
    "\n",
    "open_questions_df = unify_open_questions_datasets()\n",
    "display(open_questions_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ffe429d3-8bc5-42fe-8c35-b59cd9b0c9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Job postings data from all our resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780be169-6e2a-48c9-9de9-9ee182167418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "import os\n",
    "from consts import JOBS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a594a9-f78f-495b-a983-a0d5e1cb2550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, create_map, lit, when\n",
    "from itertools import chain\n",
    "\n",
    "seniority_mapping = {\n",
    "    \"Not Applicable\": 0,\n",
    "    \"Non pertinent\": 0,\n",
    "    \"Di-angkop\": 0,\n",
    "    \"Stagiaire / Alternant\": 0,\n",
    "    \"Internship\": 0,\n",
    "    \"Entry level\": 1,\n",
    "    \"Premier emploi\": 1,\n",
    "    \"Associate\": 1,\n",
    "    \"Mid-Senior level\": 2,\n",
    "    \"ConfirmÃ©\": 2,\n",
    "    \"Manager\": 2,\n",
    "    \"Director\": 2,\n",
    "    \"Executive\": 2,\n",
    "}\n",
    "# Convert the dictionary into a format compatible with create_map\n",
    "mapping_expr = create_map([lit(x) for x in chain(*seniority_mapping.items())])\n",
    "\n",
    "# Replace all occurrences of '-1' in the dataset with None\n",
    "def replace_minus_one_with_none(df):\n",
    "    for column in df.columns:\n",
    "        df = df.withColumn(column, when(col(column) == \"-1\", None).otherwise(col(column)))\n",
    "    return df\n",
    "\n",
    "def unify_jobpostings_datasets():\n",
    "    job_descriptions_and_skills = open_csv_file(spark, JOBS_PATH, \"job_descriptions_and_skills.csv\") \\\n",
    "        .withColumnRenamed(\"category\", \"field\") \\\n",
    "        .withColumnRenamed(\"job_description\", \"job_summary\") \\\n",
    "        .withColumnRenamed(\"job_skill_set\", \"skills\")\n",
    "    # Original columns: job_id,category,job_title,job_description,job_skill_set\n",
    "    # New columns: job_id,field,job_title,job_summary,skills\n",
    "\n",
    "    linkedin_hightech_jobs = open_csv_file(spark, JOBS_PATH, \"linkedin_hightech_jobs.csv\") \\\n",
    "        .drop(\"url\", \"company_id\", \"job_location\", \"job_employment_type\", \"job_base_pay_range\", \"company_url\", \"job_posted_time\", \"job_num_applicants\", \"discovery_input\") \\\n",
    "        .withColumnRenamed(\"job_posting_id\", \"job_id\") \\\n",
    "        .withColumnRenamed(\"job_function\", \"field\") \\\n",
    "        .withColumnRenamed(\"job_industries\", \"company_industry\") \\\n",
    "        .withColumn(\"level\", mapping_expr.getItem(col(\"job_seniority_level\"))) \\\n",
    "        .drop(\"job_seniority_level\")\n",
    "    # Original columns: url,job_posting_id,job_title,company_name,company_id,job_location,job_summary,apply_link,job_seniority_level,job_function,job_employment_type,job_industries,job_base_pay_range,company_url,job_posted_time,job_num_applicants,discovery_input\n",
    "    # New columns: job_id,job_title,company_name,job_summary,apply_link,level,field,company_industry\n",
    "\n",
    "    indeed_jobs = open_csv_file(spark, JOBS_PATH, \"indeed_jobs.csv\") \\\n",
    "        .drop(\"JOB_URL\", \"DATE_OF_POSTING\", \"WEBSITE\", \"SALARY\", \"REMOTE\", \"CITIES\", \"STATE\", \"COUNTRY\", \"JOB_TYPE\", \"ZIPCODE\", \"WEBSITEPOSTING\") \\\n",
    "        .withColumnRenamed(\"JOB_TITLE\", \"job_title\") \\\n",
    "        .withColumnRenamed(\"COMPANY\", \"company_name\") \\\n",
    "        .withColumnRenamed(\"INDUSTRY\", \"company_industry\") \\\n",
    "        .withColumnRenamed(\"JOB_DESCRIPTION\", \"job_summary\")\n",
    "    # Original columns: JOB_URL,DATE_OF_POSTING,JOB_TITLE,COMPANY,WEBSITE,INDUSTRY,SALARY,REMOTE,CITIES,STATE,COUNTRY,JOB_TYPE,ZIPCODE,JOB_DESCRIPTION,WEBSITEPOSTING\n",
    "    # New columns: job_title,company_name,company_industry,job_summary\n",
    "\n",
    "    glassdoor_data_jobs = open_csv_file(spark, JOBS_PATH, \"glassdoor_data_jobs_and_company_info.csv\") \\\n",
    "        .drop(\"Salary Estimate\", \"Rating\", \"Location\", \"Size\", \"Founded\", \"Type of ownership\", \"Revenue\") \\\n",
    "        .withColumnRenamed(\"Job Title\", \"job_title\") \\\n",
    "        .withColumnRenamed(\"Job Description\", \"job_summary\") \\\n",
    "        .withColumnRenamed(\"Company Name\", \"company_name\") \\\n",
    "        .withColumnRenamed(\"Industry\", \"company_industry\") \\\n",
    "        .withColumnRenamed(\"Sector\", \"field\")\n",
    "    glassdoor_data_jobs = replace_minus_one_with_none(glassdoor_data_jobs)\n",
    "    glassdoor_data_jobs = glassdoor_data_jobs.withColumn(\"company_name\", split(col(\"company_name\"), \"\\n\").getItem(0))\n",
    "    # Original columns: Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Size,Founded,Type of ownership,Industry,Sector,Revenue\n",
    "    # New columns: job_title,job_summary,company_name,company_industry,field\n",
    "\n",
    "    linkedin_data_jobs = open_csv_file(spark, JOBS_PATH, \"linkedin_data_jobs.csv\") \\\n",
    "        .drop(\"Employment type\", \"company_id\", \"context\", \"date\", \"education\", \"location\", \"months_experience\", \"sal_high\", \"sal_low\", \"salary\") \\\n",
    "        .withColumnRenamed(\"Industries\", \"company_industry\") \\\n",
    "        .withColumnRenamed(\"Job function\", \"field\") \\\n",
    "        .withColumnRenamed(\"company\", \"company_name\") \\\n",
    "        .withColumnRenamed(\"description\", \"job_summary\") \\\n",
    "        .withColumnRenamed(\"post_id\", \"job_id\") \\\n",
    "        .withColumnRenamed(\"post_url\", \"post_link\") \\\n",
    "        .withColumnRenamed(\"title\", \"job_title\") \\\n",
    "        .withColumn(\"level\", mapping_expr.getItem(col(\"Seniority level\"))) \\\n",
    "        .drop(\"Seniority level\")\n",
    "    # Original columns: Employment type, Industries, Job function, Seniority level, company, company_id, context, date, description, education, location, months_experience, post_id, post_url, sal_high, sal_low, salary, title\n",
    "    # New columns: company_industry,field,level,company_name,job_summary,job_id,post_link,job_title\n",
    "\n",
    "    merge1 = job_descriptions_and_skills.join(linkedin_hightech_jobs, [\"job_id\", \"field\", \"job_title\", \"job_summary\"], \"outer\")\n",
    "    merge2 = merge1.join(indeed_jobs, [\"job_title\", \"company_name\", \"company_industry\", \"job_summary\"], \"outer\")\n",
    "    merge3 = merge2.join(glassdoor_data_jobs, [\"job_title\", \"job_summary\", \"company_name\", \"company_industry\", \"field\"], \"outer\")\n",
    "    merge4 = merge3.join(linkedin_data_jobs, [\"company_industry\", \"field\", \"level\", \"company_name\", \"job_summary\", \"job_id\", \"job_title\"], \"outer\")\n",
    "    \n",
    "    return merge4\n",
    "\n",
    "all_jobpostings = unify_jobpostings_datasets()\n",
    "display(all_jobpostings)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unify_datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

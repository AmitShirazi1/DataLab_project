{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00e916bb-2966-484d-9f43-02fade5d48b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "curr_dir = os.getcwd()\n",
    "data_path = os.path.join(curr_dir, \"data/\")\n",
    "questions_data_path = os.path.join(data_path, \"questions_and_answers/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acd27b44-f503-4480-ae67-f56604e1695e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html_content(html_content):\n",
    "    \"\"\"\n",
    "    Remove HTML tags and normalize text from the given HTML content.\n",
    "    Handles missing or invalid content gracefully.\n",
    "    \"\"\"\n",
    "    if not isinstance(html_content, str):\n",
    "        # If content is not a string (e.g., NaN)\n",
    "        return\n",
    "    \n",
    "    # Parse the content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract plain text\n",
    "    clean_text = soup.get_text(separator=\" \")\n",
    "    \n",
    "    # Normalize whitespace\n",
    "    clean_text = \" \".join(clean_text.split())\n",
    "    return clean_text\n",
    "\n",
    "def get_solution_url(problem_number):\n",
    "    \"\"\"\n",
    "    Determine the GitHub solution file URL based on the problem number.\n",
    "    \"\"\"\n",
    "    base_url = \"https://github.com/fishercoder1534/Leetcode/blob/master/src/main/java/com/fishercoder/solutions/\"\n",
    "    \n",
    "    # Determine the folder\n",
    "    if problem_number < 1000:\n",
    "        folder = \"firstthousand\"\n",
    "    elif problem_number < 2000:\n",
    "        folder = \"secondthousand\"\n",
    "    elif problem_number < 3000:\n",
    "        folder = \"thirdthousand\"\n",
    "    else:\n",
    "        folder = \"fourththousand\"\n",
    "    \n",
    "    # Construct the URL\n",
    "    return f\"{base_url}{folder}/_{problem_number}.java\"\n",
    "\n",
    "\n",
    "def fetch_solution(url):\n",
    "    \"\"\"\n",
    "    Fetch the raw content of the solution from the GitHub file URL.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert the GitHub URL to the raw content URL\n",
    "        raw_url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\"/blob/\", \"/\")\n",
    "        \n",
    "        # Fetch the solution content\n",
    "        response = requests.get(raw_url)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            # Failed to fetch solution - ignore\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching solution from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def unify_leetcode_datasets():\n",
    "    # Load the LeetCode problems dataset\n",
    "    leetcode_problem_content = pd.read_csv(os.path.join(questions_data_path, \"leetcode_problems_data.csv\")) \\\n",
    "        .drop(columns=[\"title\", \"likes\", \"dislikes\"]) \\\n",
    "        .rename(columns={\"slug\": \"formatted_title\",\n",
    "                         \"content\": \"question\"}) \\\n",
    "        .assign(formatted_title=lambda df: df[\"formatted_title\"].apply(lambda x: x.lower()))\n",
    "    leetcode_problem_content[\"question\"] = leetcode_problem_content[\"question\"].apply(clean_html_content)\n",
    "    # Original columns: question_id,title,content,difficulty,likes,dislikes,slug.\n",
    "    # New columns: question_id,formatted_title,question,difficulty.\n",
    "\n",
    "    leetcode_problem_meta = pd.read_csv(os.path.join(questions_data_path, \"leetcode_problems_metadata.csv\")) \\\n",
    "        .drop(columns=[\"page_number\", \"is_premium\", \"title\", \"accepted\", \"submission\", \"solution\", \"discussion_count\", \"likes\", \"dislikes\"]) \\\n",
    "        .assign(problem_URL=lambda df: df[\"problem_URL\"].apply(lambda x: x.split(\"/\")[-1])) \\\n",
    "        .rename(columns={\"id\": \"question_id\",\n",
    "                         \"problem_description\": \"question\",\n",
    "                         \"problem_URL\": \"formatted_title\",\n",
    "                         \"topic_tags\": \"topics\"})\n",
    "    # Original columns: id,page_number,is_premium,title,problem_description,topic_tags,difficulty,similar_questions,no_similar_questions,acceptance,accepted,submission,solution,discussion_count,likes,dislikes,problem_URL,solution_URL.\n",
    "    # New columns: question_id,question,topics,difficulty,similar_questions,no_similar_questions,acceptance, formatted_title,solution_URL.\n",
    "\n",
    "    leetcode_links = pd.read_csv(os.path.join(questions_data_path, \"leetcode_problems&solutions_links.csv\")) \\\n",
    "        .drop(columns=[\"name\"]) \\\n",
    "        .assign(problem_URL=lambda df: df[\"link\"].apply(lambda x: x.split(\"/\")[-2])) \\\n",
    "        .rename(columns={\"link\": \"formatted_title\",\n",
    "                         \"solution\": \"solution_URL\"})\n",
    "    # Original columns: name,link,difficulty,solution.\n",
    "    # New columns: formatted_title,difficulty,solution_URL.\n",
    "    \n",
    "    # Merge datasets\n",
    "    leetcode_combined = pd.merge(leetcode_problem_content, leetcode_problem_meta,\n",
    "                                 on=[\"question_id\", \"formatted_title\", \"question\", \"difficulty\"],\n",
    "                                 how=\"outer\")\n",
    "    \n",
    "    # Add a column for the solution\n",
    "    leetcode_combined[\"solution\"] = None\n",
    "    \n",
    "    # Fetch solutions\n",
    "    for index, row in leetcode_combined.iterrows():\n",
    "        problem_id = row[\"question_id\"]\n",
    "        if not pd.isna(problem_id):\n",
    "            solution_url = get_solution_url(int(problem_id))\n",
    "            solution_content = fetch_solution(solution_url)\n",
    "            leetcode_combined.at[index, \"solution\"] = solution_content\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    leetcode_combined.to_csv(os.path.join(questions_data_path, \"all_code_problems_with_solutions.csv\"), index=False)\n",
    "    print(\"Unified code questions dataset saved with solutions included.\")\n",
    "    \n",
    "    return leetcode_combined\n",
    "\n",
    "leetcode_with_solutions = unify_leetcode_datasets()\n",
    "leetcode_with_solutions.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b580332f-0f44-4220-83e2-4952612ae50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def unify_open_questions_datasets():  \n",
    "    \"\"\" Load and preprocess the open-ended question datasets. \"\"\"\n",
    "    data_science_questions = pd.read_csv(os.path.join(questions_data_path, \"open_questions_data_science.csv\")) \\\n",
    "        .rename(columns={\"DESCRIPTION\": \"question\",\n",
    "                         \"ID\": \"question_id\"})\n",
    "    # Original columns: ID,DESCRIPTION\n",
    "    # New columns: question_id,question\n",
    "    num_ds_rows = len(data_science_questions)\n",
    "\n",
    "    general_questions = pd.read_csv(os.path.join(questions_data_path, \"general_open_questions.csv\"))\n",
    "    general_questions[\"question_id\"] = num_ds_rows + general_questions.index + 1\n",
    "    # Original columns: question\n",
    "    # New columns: question_id,question\n",
    "    \n",
    "    data_science_questions[\"category\"] = \"Data Science\"\n",
    "    data_science_questions[\"topics\"] = \"Data Science\"\n",
    "    general_questions[\"category\"] = \"General\"\n",
    "    general_questions[\"topics\"] = \"Soft Skills\"\n",
    "    \n",
    "    # Concatenate both into one database\n",
    "    open_questions_df = pd.concat([data_science_questions, general_questions], ignore_index=True)\n",
    "    \n",
    "    # Save the unified database\n",
    "    open_questions_df.to_csv(os.path.join(questions_data_path, \"all_open_questions.csv\"), index=False)\n",
    "    print(\"Unified database created and saved.\")\n",
    "    \n",
    "    return open_questions_df\n",
    "\n",
    "open_questions_df = unify_open_questions_datasets()\n",
    "open_questions_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "780be169-6e2a-48c9-9de9-9ee182167418",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_path = os.path.join(data_path, \"jobs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1a594a9-f78f-495b-a983-a0d5e1cb2550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "seniority_mapping = {\n",
    "    \"Not Applicable\": 0,\n",
    "    \"Non pertinent\": 0,\n",
    "    \"Di-angkop\": 0,\n",
    "    \"Stagiaire / Alternant\": 1,\n",
    "    \"Internship\": 1,\n",
    "    \"Entry level\": 2,\n",
    "    \"Premier emploi\": 2,\n",
    "    \"Associate\": 3,\n",
    "    \"Mid-Senior level\": 4,\n",
    "    \"ConfirmÃ©\": 4,\n",
    "    \"Manager\": 5,\n",
    "    \"Director\": 6,\n",
    "    \"Executive\": 7,\n",
    "}\n",
    "\n",
    "def unify_jobpostings_datasets():\n",
    "    job_descriptions_and_skills = pd.read_csv(os.path.join(jobs_path, \"job_descriptions_and_skills.csv\")) \\\n",
    "        .rename(columns={\"category\": \"field\", \"job_description\": \"job_summary\", \"job_skill_set\": \"skills\"})\n",
    "    # Original columns: job_id,category,job_title,job_description,job_skill_set\n",
    "    # New columns: job_id,field,job_title,job_summary,skills\n",
    "\n",
    "    linkedin_hightech_jobs = pd.read_csv(os.path.join(jobs_path, \"linkedin_hightech_jobs.csv\")) \\\n",
    "        .drop(columns=[\"url\", \"company_id\", \"job_location\", \"job_employment_type\", \"job_base_pay_range\", \"company_url\", \"job_posted_time\", \"job_num_applicants\", \"discovery_input\"]) \\\n",
    "        .rename(columns={\"job_posting_id\": \"job_id\",\n",
    "                         \"job_function\": \"field\",\n",
    "                         \"job_industries\": \"company_industry\"})\n",
    "    linkedin_hightech_jobs[\"level\"] = linkedin_hightech_jobs[\"job_seniority_level\"].map(seniority_mapping) \\\n",
    "        .drop(columns=[\"job_seniority_level\"])\n",
    "    # Original columns: url,job_posting_id,job_title,company_name,company_id,job_location,job_summary,apply_link,job_seniority_level,job_function,job_employment_type,job_industries,job_base_pay_range,company_url,job_posted_time,job_num_applicants,discovery_input\n",
    "    # New columns: job_id,job_title,company_name,job_summary,apply_link,level,field,company_industry\n",
    "\n",
    "    indeed_jobs = pd.read_csv(os.path.join(jobs_path, \"indeed_jobs.csv\")) \\\n",
    "        .drop(columns=[\"JOB_URL\", \"DATE_OF_POSTING\", \"WEBSITE\", \"SALARY\", \"REMOTE\", \"CITIES\", \"STATE\", \"COUNTRY\", \"JOB_TYPE\", \"ZIPCODE\", \"WEBSITEPOSTING\"]) \\\n",
    "        .rename(columns={\"JOB_TITLE\": \"job_title\",\n",
    "                         \"COMPANY\": \"company_name\",\n",
    "                         \"INDUSTRY\": \"company_industry\",\n",
    "                         \"JOB_DESCRIPTION\": \"job_summary\"})\n",
    "    # Original columns: JOB_URL,DATE_OF_POSTING,JOB_TITLE,COMPANY,WEBSITE,INDUSTRY,SALARY,REMOTE,CITIES,STATE,COUNTRY,JOB_TYPE,ZIPCODE,JOB_DESCRIPTION,WEBSITEPOSTING\n",
    "    # New columns: job_title,company_name,company_industry,job_summary\n",
    "\n",
    "    glassdoor_data_jobs = pd.read_csv(os.path.join(jobs_path, \"glassdoor_data_jobs_and_company_info.csv\")) \\\n",
    "        .drop(columns=[\"Salary Estimate\", \"Rating\", \"Location\", \"Size\", \"Founded\", \"Type of ownership\", \"Revenue\"]) \\\n",
    "        .rename(columns={\"Job Title\": \"job_title\",\n",
    "                         \"Job Description\": \"job_summary\",\n",
    "                         \"Company Name\": \"company_name\",\n",
    "                         \"Industry\": \"company_industry\",\n",
    "                         \"Sector\": \"field\"})\n",
    "    glassdoor_data_jobs.replace(\"-1\", None, inplace=True)\n",
    "    glassdoor_data_jobs[\"company_name\"] = glassdoor_data_jobs[\"company_name\"].apply(lambda x: x.split(\"\\n\")[0])\n",
    "    # Original columns: Job Title,Salary Estimate,Job Description,Rating,Company Name,Location,Size,Founded,Type of ownership,Industry,Sector,Revenue\n",
    "    # New columns: job_title,job_summary,company_name,company_industry,field\n",
    "\n",
    "    linkedin_data_jobs = pd.read_csv(os.path.join(jobs_path, \"linkedin_data_jobs.csv\")) \\\n",
    "        .drop(columns=[\"Employment type\", \"company_id\", \"context\", \"date\", \"education\", \"location\", \"months_experience\", \"sal_high\", \"sal_low\", \"salary\"]) \\\n",
    "        .rename(columns={\"Industries\": \"company_industry\",\n",
    "                         \"Job function\": \"field\",\n",
    "                         \"company\": \"company_name\",\n",
    "                         \"description\": \"job_summary\",\n",
    "                         \"post_id\": \"job_id\",\n",
    "                         \"post_url\": \"post_link\",\n",
    "                         \"title\": \"job_title\"})\n",
    "    linkedin_data_jobs[\"level\"] = linkedin_data_jobs[\"Seniority level\"].map(seniority_mapping)\n",
    "    # Original columns: Employment type, Industries, Job function, Seniority level, company, company_id, context, date, description, education, location, months_experience, post_id, post_url, sal_high, sal_low, salary, title\n",
    "    # New columns: company_industry,field,level,company_name,job_summary,job_id,post_link,job_title\n",
    "\n",
    "    merge1 = pd.merge(job_descriptions_and_skills, linkedin_hightech_jobs,\n",
    "                             on=[\"job_id\",\"field\",\"job_title\",\"job_summary\"],\n",
    "                             how=\"outer\")\n",
    "    \n",
    "    merge2 = pd.merge(merge1, indeed_jobs,\n",
    "                      on=[\"job_title\",\"company_name\",\"company_industry\",\"job_summary\"],\n",
    "                      how=\"outer\")\n",
    "    \n",
    "    merge3 = pd.merge(merge2, glassdoor_data_jobs,\n",
    "                      on=[\"job_title\",\"job_summary\",\"company_name\",\"company_industry\",\"field\"],\n",
    "                      how=\"outer\")\n",
    "    \n",
    "    merge4 = pd.merge(merge3, linkedin_data_jobs,\n",
    "                      on=[\"company_industry\",\"field\",\"level\",\"company_name\",\"job_summary\",\"job_id\",\"job_title\"],\n",
    "                      how=\"outer\")\n",
    "    \n",
    "    # Save the unified database\n",
    "    merge4.to_csv(os.path.join(jobs_path, \"all_jobpostings.csv\"), index=False)\n",
    "    print(\"Unified database created and saved.\")\n",
    "    \n",
    "    return merge4\n",
    "\n",
    "all_jobpostings = unify_jobpostings_datasets()\n",
    "display(all_jobpostings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bcc575e-f1b8-4744-9daa-77241822b1c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You are creating a job interview simulator in Python. Before applying to an open position, an applicant could choose to use your simulator to practice on an interview for this job. This means the interview questions should be related to the job description and requirements. Also, the candidate should get a feedback on his performances in the simulation.\n",
    "\n",
    "You have several datasets to train on, these are their schemas:\n",
    "\"all_code_problems_with_solutions.csv\": \n",
    "question_id,content,difficulty,formatted_title,topic_tags,similar_questions,no_similar_questions,acceptance,solution_URL,solution.\n",
    "\"all_open_questions.csv\" \n",
    "question_id,question,category,topics.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "unify_datasets",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12f6c0c3-6f8b-47d0-8fd8-0deea17d70c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8382d6-2c2b-4ded-83fd-2f5611fb0bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Job and Interview Analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "job_skills_spark = open_csv_file(spark, JOBS_PATH, 'all_jobpostings_with_skills.csv')\n",
    "job_skills_spark.printSchema()\n",
    "\n",
    "code_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_code_questions_with_topics.csv')\n",
    "code_questions_spark.printSchema()\n",
    "\n",
    "open_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions_with_topics.csv')\n",
    "open_questions_spark.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3213a8e7-c0c9-4a57-89a2-069ea1ca414a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, avg\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "\n",
    "# Read Data from CSVs\n",
    "job_data = job_skills_spark\n",
    "code_questions_data = code_questions_spark\n",
    "open_questions_data = open_questions_spark\n",
    "\n",
    "# Helper Functions\n",
    "def preprocess_skills(data):\n",
    "    \"\"\"Splits skills into individual entries and counts their occurrences.\"\"\"\n",
    "    skills = data.withColumn(\"skill\", explode(split(col(\"skills\"), \", \"))) \\\n",
    "                 .groupBy(\"skill\") \\\n",
    "                 .count() \\\n",
    "                 .orderBy(col(\"count\").desc())\n",
    "    return skills\n",
    "\n",
    "# 1. Job Count by Field\n",
    "def job_count_by_field(data):\n",
    "    field_counts = data.groupBy(\"field\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "    field_counts.plot(kind=\"bar\", x=\"field\", y=\"count\", color=\"skyblue\")\n",
    "    plt.title(\"Job Count by Field\")\n",
    "    plt.xlabel(\"Field\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# 2. Most In-Demand Skills\n",
    "def most_in_demand_skills(data):\n",
    "    skills = preprocess_skills(data).toPandas()\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(zip(skills[\"skill\"], skills[\"count\"])))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most In-Demand Skills\")\n",
    "    plt.show()\n",
    "\n",
    "# 3. Skill Distribution by Field\n",
    "def skill_distribution_by_field(data):\n",
    "    skills_field = data.withColumn(\"skill\", explode(split(col(\"skills\"), \", \")))\n",
    "    skill_field_counts = skills_field.groupBy(\"field\", \"skill\").count().toPandas()\n",
    "    skill_pivot = skill_field_counts.pivot(index=\"field\", columns=\"skill\", values=\"count\").fillna(0)\n",
    "    skill_pivot.plot(kind=\"bar\", stacked=True, figsize=(12, 6))\n",
    "    plt.title(\"Skill Distribution by Field\")\n",
    "    plt.xlabel(\"Field\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# 4. Skills Comparison Across Levels\n",
    "def skills_comparison_across_levels(data):\n",
    "    level_skills = data.withColumn(\"skill\", explode(split(col(\"skills\"), \", \")))\n",
    "    level_pivot = level_skills.groupBy(\"level\", \"skill\").count().toPandas()\n",
    "    pivot_table = level_pivot.pivot(index=\"level\", columns=\"skill\", values=\"count\").fillna(0)\n",
    "    sns.heatmap(pivot_table, annot=False, cmap=\"coolwarm\", cbar=True)\n",
    "    plt.title(\"Skills Comparison Across Levels\")\n",
    "    plt.xlabel(\"Skills\")\n",
    "    plt.ylabel(\"Job Level\")\n",
    "    plt.show()\n",
    "\n",
    "# 5. Co-occurrence of Skills\n",
    "def cooccurrence_of_skills(data):\n",
    "    skills_pairs = data.withColumn(\"skills\", split(col(\"skills\"), \", \")).select(\"skills\").rdd.flatMap(\n",
    "        lambda row: [(a, b) for a in row.skills for b in row.skills if a != b]\n",
    "    )\n",
    "    G = nx.Graph()\n",
    "    G.add_edges_from(skills_pairs.collect())\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    nx.draw(G, with_labels=True, node_color=\"skyblue\", node_size=2000, font_size=10, edge_color=\"gray\")\n",
    "    plt.title(\"Co-occurrence of Skills\")\n",
    "    plt.show()\n",
    "\n",
    "# 6. Questions by Difficulty\n",
    "def questions_by_difficulty(data):\n",
    "    difficulty_counts = data.groupBy(\"difficulty\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "    difficulty_counts.plot(kind=\"pie\", y=\"count\", labels=difficulty_counts[\"difficulty\"], autopct=\"%1.1f%%\")\n",
    "    plt.title(\"Questions by Difficulty\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()\n",
    "\n",
    "# 7. Most Common Topics\n",
    "def most_common_topics(data):\n",
    "    topics_counts = data.withColumn(\"topic\", explode(split(col(\"topics\"), \", \"))) \\\n",
    "                        .groupBy(\"topic\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "    topics_counts.plot(kind=\"bar\", x=\"topic\", y=\"count\", color=\"orange\")\n",
    "    plt.title(\"Most Common Topics\")\n",
    "    plt.xlabel(\"Topics\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.show()\n",
    "\n",
    "# 8. Acceptance Rate Analysis\n",
    "def acceptance_rate_analysis(data):\n",
    "    scatter_data = data.select(\"difficulty\", \"acceptance\").toPandas()\n",
    "    plt.scatter(scatter_data[\"difficulty\"], scatter_data[\"acceptance\"], color=\"purple\", alpha=0.7)\n",
    "    plt.title(\"Acceptance Rate Analysis\")\n",
    "    plt.xlabel(\"Difficulty\")\n",
    "    plt.ylabel(\"Acceptance Rate\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2a67e7a-369b-4e01-84a1-db0a3eb74290",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Call Functions to Generate Visualizations\n",
    "questions_by_difficulty(code_questions_data)\n",
    "most_common_topics(code_questions_data)\n",
    "acceptance_rate_analysis(code_questions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ee0fcf-6868-4d36-9188-00c84f8d2edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_count_by_field(job_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bcf325c-0bb8-41ee-9d26-882602f171b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "most_in_demand_skills(job_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fcf87c0d-55eb-4c66-aaa6-e8f54005c479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skill_distribution_by_field(job_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f5129b-3f4e-49e7-a49b-952e78a1c622",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "skills_comparison_across_levels(job_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03afaa91-a9b0-4b20-beed-8ca03f77b467",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cooccurrence_of_skills(job_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ecf82a47-4324-4383-8667-ad74ff70fd99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8da8e1d5-aa92-4577-aab5-f20fbcbc1bde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "The simulation - #TAKE_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "186e9fab-66d5-471f-8109-25cecf1b6cbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from api_keys import API_KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f227db-71d6-426d-bf28-16ab9eb5f0dd",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "claude"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, explode, split\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.ml.feature import Word2VecModel, HashingTF, IDF\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import PyPDF2\n",
    "\n",
    "class InterviewSimulationApp:\n",
    "    def __init__(self, gemini_api_key):\n",
    "        # Initialize Spark Session\n",
    "        self.spark = SparkSession.builder \\\n",
    "            .appName(\"InterviewSimulationApp\") \\\n",
    "            .getOrCreate()\n",
    "        \n",
    "        # Configure Gemini\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-flash')\n",
    "        \n",
    "        # Load datasets using Spark DataFrame\n",
    "        self.job_data = self.spark.read.csv('job_data.csv', header=True)\n",
    "        self.interview_questions = self.spark.read.csv('interview_questions.csv', header=True)\n",
    "        self.coding_questions = self.spark.read.json('coding_questions.json')\n",
    "    \n",
    "    def extract_pdf_text(self, pdf_path):\n",
    "        \"\"\"PDF text extraction using Spark UDF\"\"\"\n",
    "        def extract_text(path):\n",
    "            with open(path, 'rb') as file:\n",
    "                reader = PyPDF2.PdfReader(file)\n",
    "                return ' '.join([page.extract_text() for page in reader.pages])\n",
    "        \n",
    "        extract_udf = udf(extract_text, StringType())\n",
    "        return self.spark.read.text(pdf_path).withColumn(\"extracted_text\", extract_udf(col(\"value\")))\n",
    "    \n",
    "    def match_job_questions(self, cv_text, job_title):\n",
    "        \"\"\"Job matching and question generation\"\"\"\n",
    "        # Filter job data\n",
    "        job_match = self.job_data.filter(col(\"title\").contains(job_title))\n",
    "        \n",
    "        # Generate questions using Gemini\n",
    "        questions_prompt = f\"\"\"Generate interview questions for {job_title}\n",
    "        considering CV context: {cv_text}\"\"\"\n",
    "        \n",
    "        questions = self.model.generate_content(questions_prompt).text\n",
    "        \n",
    "        # Convert questions to Spark DataFrame\n",
    "        questions_df = self.spark.createDataFrame(\n",
    "            [(q.strip(),) for q in questions.split('\\n') if q.strip()],\n",
    "            ['question']\n",
    "        )\n",
    "        \n",
    "        return questions_df\n",
    "    \n",
    "    def generate_coding_challenges(self, experience_level):\n",
    "        \"\"\"Coding challenge selection\"\"\"\n",
    "        difficulty_map = {\n",
    "            'junior': ['easy', 'medium'],\n",
    "            'senior': ['medium', 'hard']\n",
    "        }\n",
    "        \n",
    "        challenges = self.coding_questions.filter(\n",
    "            col('difficulty').isin(difficulty_map.get(experience_level, []))\n",
    "        )\n",
    "        \n",
    "        return challenges.limit(3)\n",
    "    \n",
    "    def simulate_interview(self, cv_text, job_title):\n",
    "        \"\"\"Interview simulation workflow\"\"\"\n",
    "        # Generate questions\n",
    "        questions = self.match_job_questions(cv_text, job_title)\n",
    "        \n",
    "        # Select coding challenges\n",
    "        coding_challenges = self.generate_coding_challenges('junior')\n",
    "        \n",
    "        return {\n",
    "            'behavioral_questions': questions,\n",
    "            'coding_challenges': coding_challenges\n",
    "        }\n",
    "    \n",
    "    def analyze_performance(self, questions_df, answers_df):\n",
    "        \"\"\"Performance analysis\"\"\"\n",
    "        # Join questions and answers\n",
    "        performance_df = questions_df.join(answers_df, 'question')\n",
    "        \n",
    "        # Use Gemini for feedback generation\n",
    "        def generate_feedback(questions, answers):\n",
    "            feedback_prompt = f\"\"\"Analyze interview performance:\n",
    "            Questions: {questions}\n",
    "            Answers: {answers}\n",
    "            Provide STAR method feedback\"\"\"\n",
    "            \n",
    "            return self.model.generate_content(feedback_prompt).text\n",
    "        \n",
    "        feedback_udf = udf(generate_feedback, StringType())\n",
    "        \n",
    "        feedback_df = performance_df.withColumn(\n",
    "            'feedback', \n",
    "            feedback_udf(col('question'), col('answer'))\n",
    "        )\n",
    "        \n",
    "        return feedback_df\n",
    "\n",
    "def main():\n",
    "    app = InterviewSimulationApp(\n",
    "        gemini_api_key=os.getenv(API_KEY)\n",
    "    )\n",
    "    \n",
    "    print(\"Upload CV or paste text for interview simulation\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f145f7a-6eb9-49f9-93b9-11aae87bffd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "1. **Initialization (`__init__`)**:\n",
    "   - Sets up Spark Session with configurable parameters\n",
    "   - Configures Gemini AI with API key\n",
    "   - Downloads NLTK resources\n",
    "   - Loads datasets (job data, interview questions, coding challenges)\n",
    "\n",
    "2. **Dataset Loading (`_load_dataset`)**:\n",
    "   - Supports loading CSV and JSON files\n",
    "   - Handles different file types\n",
    "   - Includes error handling for dataset loading\n",
    "\n",
    "3. **PDF Text Extraction (`extract_pdf_text`)**:\n",
    "   - Uses PyPDF2 to extract text from PDFs\n",
    "   - Converts PDF content to Spark DataFrame\n",
    "   - Includes safe extraction with error handling\n",
    "\n",
    "4. **Text Preprocessing (`preprocess_text`)**:\n",
    "   - Tokenizes text\n",
    "   - Removes stopwords\n",
    "   - Cleans and normalizes text\n",
    "   - Prepares text for further analysis\n",
    "\n",
    "5. **Semantic Job Matching (`semantic_job_matching`)**:\n",
    "   - Uses Gemini to understand CV context\n",
    "   - Filters interview questions based on job title\n",
    "   - Provides contextually relevant questions\n",
    "\n",
    "6. **Coding Challenge Generation (`generate_coding_challenges`)**:\n",
    "   - Selects coding challenges based on experience level\n",
    "   - Supports different difficulty levels (junior, mid-level, senior)\n",
    "   - Limits number of challenges\n",
    "\n",
    "7. **Performance Analysis (`interview_performance_analysis`)**:\n",
    "   - Combines questions and answers\n",
    "   - Generates feedback using STAR methodology\n",
    "   - Provides structured performance assessment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a3bfcb4-8db6-4692-98cc-4aa142e32714",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "## Technical Enhancements\n",
    "1. **Machine Learning Improvements**\n",
    "   - Implement advanced embedding techniques\n",
    "   - Use transfer learning for question generation\n",
    "   - Develop custom ML models for skill matching\n",
    "\n",
    "2. **Distributed Computing Optimizations**\n",
    "   - Implement dynamic resource allocation\n",
    "   - Add caching mechanisms\n",
    "   - Optimize Spark configurations\n",
    "\n",
    "3. **AI Integration Enhancements**\n",
    "   - Multi-model approach (Gemini + Local Models)\n",
    "   - Real-time feedback generation\n",
    "   - Contextual understanding improvements\n",
    "\n",
    "4. **Data Management**\n",
    "   - Create robust data versioning\n",
    "   - Implement data quality checks\n",
    "   - Add automated dataset refresh mechanisms\n",
    "\n",
    "5. **Scalability Features**\n",
    "   - Microservices architecture\n",
    "   - Containerization (Docker)\n",
    "   - Kubernetes orchestration\n",
    "\n",
    "## User Experience Improvements\n",
    "1. Interactive CLI/Web Interface\n",
    "2. Personalized Learning Paths\n",
    "3. Comprehensive Skill Gap Analysis\n",
    "4. Multi-language Support\n",
    "\n",
    "## Advanced Features\n",
    "1. Continuous Learning Model\n",
    "2. Industry-Specific Question Banks\n",
    "3. Mock Interview Recordings\n",
    "4. Adaptive Difficulty Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a4ab7ff-276d-4bc3-bce1-941766bbf8d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Dict, List\n",
    "\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql.functions import col, udf, explode, split, regexp_replace\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "from pyspark.ml.feature import Word2VecModel, HashingTF, IDF\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "import google.generativeai as genai\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "29cacc0a-1900-4b03-920a-99db52988d36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class InterviewSimulationApp:\n",
    "    def __init__(self, gemini_api_key: str, config: Dict = None):\n",
    "        # Logging setup\n",
    "        logging.basicConfig(level=logging.INFO)\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        # Spark Session with enhanced configurations\n",
    "        spark_config = config or {}\n",
    "        self.spark = (SparkSession.builder\n",
    "            .appName(\"InterviewSimulationApp\")\n",
    "            .config(\"spark.sql.shuffle.partitions\", spark_config.get('shuffle_partitions', 200))\n",
    "            .config(\"spark.executor.memory\", spark_config.get('executor_memory', '4g'))\n",
    "            .getOrCreate())\n",
    "        \n",
    "        # Natural Language Processing Setup\n",
    "        nltk.download('punkt')\n",
    "        nltk.download('stopwords')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        # Gemini AI Configuration\n",
    "        genai.configure(api_key=gemini_api_key)\n",
    "        self.model = genai.GenerativeModel('gemini-flash')\n",
    "        \n",
    "        # Load Datasets - TODO:\n",
    "        self.job_data = self._load_dataset('job_data.csv')\n",
    "        self.interview_questions = self._load_dataset('interview_questions.csv')\n",
    "        self.coding_questions = self._load_dataset('coding_questions.json')\n",
    "    \n",
    "    def _load_dataset(self, path: str) -> DataFrame:\n",
    "        \"\"\"Robust dataset loading with error handling\"\"\"\n",
    "        try:\n",
    "            if path.endswith('.csv'):\n",
    "                return self.spark.read.csv(path, header=True, inferSchema=True)\n",
    "            elif path.endswith('.json'):\n",
    "                return self.spark.read.json(path)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported file type: {path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to load dataset {path}: {e}\")\n",
    "            return self.spark.createDataFrame([], StructType([]))\n",
    "    \n",
    "    def extract_pdf_text(self, pdf_path: str) -> DataFrame:\n",
    "        \"\"\"Advanced PDF text extraction with error handling\"\"\"\n",
    "        def safe_extract_text(path: str) -> str:\n",
    "            try:\n",
    "                with open(path, 'rb') as file:\n",
    "                    reader = PyPDF2.PdfReader(file)\n",
    "                    return ' '.join([page.extract_text() for page in reader.pages])\n",
    "            except Exception as e:\n",
    "                self.logger.error(f\"PDF extraction error: {e}\")\n",
    "                return \"\"\n",
    "        \n",
    "        extract_udf = udf(safe_extract_text, StringType())\n",
    "        return self.spark.read.text(pdf_path).withColumn(\"extracted_text\", extract_udf(col(\"value\")))\n",
    "    \n",
    "    def preprocess_text(self, text_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Advanced text preprocessing\"\"\"\n",
    "        def tokenize_and_clean(text: str) -> List[str]:\n",
    "            tokens = word_tokenize(text.lower())\n",
    "            return [token for token in tokens if token.isalnum() and token not in self.stop_words]\n",
    "        \n",
    "        tokenize_udf = udf(tokenize_and_clean, ArrayType(StringType()))\n",
    "        \n",
    "        return text_df.withColumn(\"processed_tokens\", tokenize_udf(col(\"extracted_text\")))\n",
    "    \n",
    "    def semantic_job_matching(self, cv_text: str, job_title: str) -> DataFrame:\n",
    "        \"\"\"Advanced semantic job matching with embeddings\"\"\"\n",
    "        # Use Gemini for contextual understanding\n",
    "        job_context = self.model.generate_content(\n",
    "            f\"Extract key skills and experience relevant to {job_title} from: {cv_text}\"\n",
    "        ).text\n",
    "        \n",
    "        # Filter and rank job questions\n",
    "        matched_questions = self.interview_questions.filter(\n",
    "            col(\"job_category\").contains(job_title)\n",
    "        )\n",
    "        \n",
    "        return matched_questions\n",
    "    \n",
    "    # TODO: Add more advanced semantic matching\n",
    "    def generate_coding_challenges(self, experience_level: str, num_challenges: int = 3) -> DataFrame:\n",
    "        \"\"\"Intelligent coding challenge selection\"\"\"\n",
    "        difficulty_mapping = {\n",
    "            'junior': ['easy', 'medium'],\n",
    "            'mid-level': ['medium'],\n",
    "            'senior': ['medium', 'hard']\n",
    "        }\n",
    "        \n",
    "        difficulties = difficulty_mapping.get(experience_level, ['easy', 'medium'])\n",
    "        \n",
    "        return self.coding_questions.filter(\n",
    "            col('difficulty').isin(difficulties)\n",
    "        ).limit(num_challenges)\n",
    "    \n",
    "    def interview_performance_analysis(self, questions_df: DataFrame, answers_df: DataFrame) -> DataFrame:\n",
    "        \"\"\"Advanced performance analysis using AI and distributed computing\"\"\"\n",
    "        def generate_star_feedback(questions: List[str], answers: List[str]) -> str:\n",
    "            feedback_prompt = f\"\"\"Analyze interview performance using STAR method:\n",
    "            Questions: {questions}\n",
    "            Answers: {answers}\n",
    "            \n",
    "            Provide:\n",
    "            1. Strengths\n",
    "            2. Areas of Improvement\n",
    "            3. Overall Assessment\"\"\"\n",
    "            \n",
    "            return self.model.generate_content(feedback_prompt).text\n",
    "        \n",
    "        feedback_udf = udf(generate_star_feedback, StringType())\n",
    "        \n",
    "        return questions_df.join(answers_df, \"question_id\").withColumn(\n",
    "            \"performance_feedback\", \n",
    "            feedback_udf(col(\"questions\"), col(\"answers\"))\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "201972a4-ce38-4256-9ceb-51d97342357e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize the application\n",
    "    app = InterviewSimulationApp(\n",
    "        gemini_api_key=os.getenv(API_KEY),\n",
    "        config={\n",
    "            'shuffle_partitions': 300,\n",
    "            'executor_memory': '8g'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    print(\"Welcome to Advanced Interview Simulation!\")\n",
    "    print(\"You can upload CV or paste text directly.\")\n",
    "\n",
    "    # TODO: ADD OUR CODE HERE\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "job_simulation",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b98472b4-79a8-49ef-bc2d-2526ea0c9f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f930a11d-35d4-4e83-a812-9581bddc1110",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Code answers evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dabf723-6954-4b6d-91d3-7267dc39b1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from consts import DATA_PATH, GEMINI_SIMULATION_DATA_PATH\n",
    "\n",
    "code_questions_solutions = pd.read_csv(os.path.join(DATA_PATH, 'top_code_questions.csv'))[['question', 'solution']]\n",
    "code_questions_answers = pd.read_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'code_questions_answers.csv'))\n",
    "code_answers_evaluation = code_questions_answers[['question', 'answer']].merge(code_questions_solutions, on='question', how='left').drop_duplicates()\n",
    "\n",
    "code_answers_evaluation['evaluation'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6044053-347b-40b0-837f-f3e5aa2ebd40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from api_keys import API_KEYS\n",
    "from consts import DATA_PATH\n",
    "\n",
    "def evaluate_code_answer(question, answer, solution):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "   \n",
    "    prompt = f\"\"\"\n",
    "        The user has submitted a code solution to a programming question. Your task is to analyze and evaluate the code, considering both correctness and the quality of the approach. Even if the code has syntax errors or does not compile, partial credit should be awarded if the logic or idea is sound. \n",
    "\n",
    "        Evaluation Criteria:\n",
    "        - Correctness – Does the code produce the expected output for all cases? Is it written in the required programming language (if specified)?\n",
    "        - Functionality – Does the code correctly solve the problem?\n",
    "        - Efficiency – If there are complexity constraints, is the code optimized in terms of time and space complexity?\n",
    "        - Readability & Best Practices – Is the code well-structured, using meaningful variable names and following coding standards?\n",
    "        - Edge Case Handling – Does the solution consider different edge cases, such as empty inputs, extreme values, or invalid data?\n",
    "        - Logical Soundness (For Non-Compiling Code/Pseudocode) – Even if the code contains syntax errors or is in pseudocode, does it show a clear and correct approach to solving the problem?\n",
    "\n",
    "        Scoring Guidelines:\n",
    "        Assign a score between 0 and 100 based on the above criteria. 0 = An entirely incorrect or non-functional solution with no meaningful approach. 100 = A fully correct, efficient (if needed), and well-structured solution. Provide the feedback in the following format: The score in a 'x/100' format, where 'x' is a number between 0 and 100 that represents the score, then a newline, then the feedback message.\n",
    "\n",
    "        Example Evaluations:\n",
    "        Example 1: Fully Correct Code.\n",
    "        Question: Write a Python function that returns the factorial of a number.\n",
    "        User’s Code: \n",
    "            def factorial(n):\n",
    "                if not isinstance(n, int) or n < 0:\n",
    "                    raise ValueError('Input must be a non-negative integer')\n",
    "                result = 1\n",
    "                for i in range(2, n + 1):\n",
    "                    result *= i\n",
    "                return result\n",
    "        Output: 100/100\\nCorrect, handles all cases including invalid input. Efficient (iterative approach avoids recursion depth issues). Readable and follows best practices.\n",
    "\n",
    "        Now, evaluate the following coding solution:\n",
    "\n",
    "        Question: {question}\n",
    "        User's Code: \n",
    "        {answer}\n",
    "\n",
    "        Java Solution: {solution if solution else 'Not provided'}\n",
    "\n",
    "        If the Java solution is provided, check if the user’s solution is correct by comparing it to the Java solution. Provide feedback based on correctness, efficiency, readability, edge cases, and logical soundness. Assign a score accordingly, giving partial credit for a good idea even if the code does not compile.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       evaluation = response.text.strip()\n",
    "       print(\"response:\", evaluation)\n",
    "       return evaluation if evaluation else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 900\n",
    "empty_evaluation_rows = code_answers_evaluation[code_answers_evaluation['answer'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_evaluation_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "\n",
    "        # Filter the rows where the \"evaluation\" column is empty\n",
    "        empty_evaluation_rows = code_answers_evaluation[(code_answers_evaluation['evaluation'] == '') & (code_answers_evaluation['answer'].str.strip() != '')]\n",
    "        if (empty_evaluation_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"evaluation\"\n",
    "        indices_to_update = empty_evaluation_rows.index[:15]\n",
    "\n",
    "        # Apply the function with all required columns\n",
    "        code_answers_evaluation.loc[indices_to_update, 'evaluation'] = code_answers_evaluation.loc[indices_to_update].apply(\n",
    "            lambda row: evaluate_code_answer(row['question'], row['answer'], row['solution']), axis=1\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b966623-0f4c-4e64-b3af-1385945e16c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_answers_evaluation.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'code_answers_evaluation.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb3a50fa-c444-4724-b139-3e48b45af176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Open answers evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7716a7b-3633-4e51-a14d-ec2aa8928aac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from consts import DATA_PATH\n",
    "\n",
    "GEMINI_SIMULATION_DATA_PATH = os.path.join(DATA_PATH, 'gemini_simulation/')\n",
    "open_questions_answers = pd.read_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'open_questions_answers.csv'))\n",
    "open_answers_evaluation = open_questions_answers[['question', 'answer']]\n",
    "open_answers_evaluation['evaluation'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c02c9814-41a4-409a-a97d-0c1fb74b7636",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from api_keys import API_KEYS\n",
    "from consts import DATA_PATH\n",
    "\n",
    "def evaluate_open_answer(question, answer):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "   \n",
    "    prompt = f\"\"\"\n",
    "    The user has provided an answer to an open-ended question. Your task is to evaluate their response based on the following criteria:\n",
    "    - Relevance & Completeness – Does the answer directly address the question and cover all key aspects?\n",
    "    - Clarity & Coherence – Is the response well-structured, logically presented, and easy to understand?\n",
    "    - Accuracy (for factual questions) or Depth & Justification (for subjective questions). If the question is factual, does the answer provide correct and well-supported information?, If the question is subjective, does the response demonstrate thoughtful reasoning, realistic insights, and a well-supported argument?\n",
    "    - Balance & Perspective (for subjective questions) – Does the response consider different viewpoints or provide a well-rounded perspective?.\n",
    "    \n",
    "    Scoring: Assign a score between 0 and 100 based on the criteria above. 0 means the answer is entirely incorrect, off-topic, or lacks coherence. 100 means the response is fully relevant, well-articulated, and appropriately detailed.\n",
    "    Provide the feedback in the following format: The score in a 'x/100' format, where 'x' is a number between 0 and 100 that represents the score, then a newline, then the feedback message.\n",
    "    \n",
    "    Examples:\n",
    "    Example 1: Factual Question.\n",
    "    Question: What are the key benefits of cloud computing?.\n",
    "    User’s Answer: Cloud computing makes things faster and better.\n",
    "    Ideal Answer: Cloud computing provides scalability, cost efficiency, remote accessibility, and security improvements. Businesses benefit from reduced infrastructure costs and improved collaboration.\n",
    "    Feedback:\n",
    "    50/100\n",
    "    Strengths: The answer hints at benefits but lacks specifics.\n",
    "    Improvements: More details on specific advantages like scalability and cost savings would improve clarity.\n",
    "    \n",
    "    Example 2: Subjective Question.\n",
    "    Question: Where do you see yourself in 5 years professionally?.\n",
    "    User’s Answer: I want to have a good job and be successful.\n",
    "    Ideal Answer: In five years, I aim to become a project manager in the tech industry, leading cross-functional teams. To prepare, I plan to gain experience in team management, obtain a PMP certification, and refine my leadership skills.\n",
    "    Feedback:\n",
    "    60/100\n",
    "    Strengths: The answer shows ambition.\n",
    "    Improvements: It lacks specificity regarding career path and steps to achieve success.\n",
    "    \n",
    "    Now, evaluate the following response:\n",
    "    Question:\n",
    "    {question}\n",
    "\n",
    "    User's Answer:\n",
    "    {answer}\n",
    "    \n",
    "    Provide constructive feedback and assign a score based on clarity, completeness, reasoning, and relevance.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       evaluation = response.text.strip()\n",
    "       print(\"response:\", evaluation)\n",
    "       return evaluation if evaluation else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 900\n",
    "empty_evaluation_rows = open_answers_evaluation[open_answers_evaluation['answer'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_evaluation_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel(\"gemini-2.0-flash-exp\")\n",
    "\n",
    "        # Filter the rows where the \"evaluation\" column is empty\n",
    "        empty_evaluation_rows = open_answers_evaluation[(open_answers_evaluation['evaluation'] == '') & (open_answers_evaluation['answer'].str.strip() != '')]\n",
    "        if (empty_evaluation_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"evaluation\"\n",
    "        indices_to_update = empty_evaluation_rows.index[:15]\n",
    "\n",
    "        # Apply the function with all required columns\n",
    "        open_answers_evaluation.loc[indices_to_update, 'evaluation'] = open_answers_evaluation.loc[indices_to_update].apply(\n",
    "            lambda row: evaluate_open_answer(row['question'], row['answer']), axis=1\n",
    "        )\n",
    "    running_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f6f50968-4db4-4e78-86c4-c7130bbf0ba4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "open_answers_evaluation.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'open_answers_evaluation.csv'))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "evaluate_answers",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

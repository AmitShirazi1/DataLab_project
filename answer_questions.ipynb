{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39955b4b-3b77-455a-a73c-1cba240ba9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbe1eb8-dab3-42a8-86a4-f56cfd478cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from consts import DATA_PATH\n",
    "\n",
    "# Load datasets\n",
    "code_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_code_questions.csv\")).drop_duplicates()\n",
    "open_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_open_questions.csv\")).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dd63259-80db-48aa-9805-f7c973faa9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answering the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f586344d-a8a5-4fe1-b455-81506d88b0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_questions_and_answers = code_questions[['question']].drop_duplicates()\n",
    "code_questions_and_answers['answer'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9b76a3-91e0-4d71-b69e-fc3f3c7bfe8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import time\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "def answer_code_question(question):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    prompt = f\"You are a candidate being interviewed for a specific role related to coding. Your task is to answer the questions provided to you in a professional, clear, and concise manner, as if you are participating in a real interview. Understand the Question: Take a moment to fully grasp what is being asked. The question requires a code answer, ensure your code is clear and functional. Answer Clearly: Provide structured responses, focusing on clarity and correctness in your code. The question might contain multiple requirements, input-output examples, hints, and other specifications. \\nFor example: Question: 'Implement a function to reverse a linked list in Python.'. Your Answer: 'class ListNode: \\n def __init__(self, val=0, next=None): \\n\\t self.val = val \\n\\t self.next = next \\n def reverse_linked_list(head): \\n\\t prev = None \\n\\t current = head \\n\\t while current: \\n\\t\\t next_node = current.next \\n\\t\\t current.next = prev \\n\\t\\t prev = current \\n\\t\\t current = next_node \\n\\t return prev \\n'. Here is the question for you: {question}\"\n",
    "   \n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       answer = response.text.strip()\n",
    "       print(\"response:\", answer)\n",
    "       return answer if answer else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 3000\n",
    "empty_answer_rows = code_questions_and_answers[code_questions_and_answers['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_answer_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"answer\" column is empty\n",
    "        empty_answer_rows = code_questions_and_answers[(code_questions_and_answers['answer'] == '') & (code_questions_and_answers['question'].str.strip() != '')]\n",
    "        if (empty_answer_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"answers\"\n",
    "        indices_to_update = empty_answer_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        code_questions_and_answers.loc[indices_to_update, 'answer'] = (\n",
    "            code_questions_and_answers.loc[indices_to_update, 'question']\n",
    "                .apply(answer_code_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a4b097-012b-467a-a7bc-7b04cbcd1e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import GEMINI_SIMULATION_DATA_PATH\n",
    "\n",
    "code_questions_and_answers.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, \"code_questions_answers.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67adb448-127a-43de-ac26-6122f3f94b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "open_questions_and_answers = open_questions[['question']].drop_duplicates()\n",
    "open_questions_and_answers['answer'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc662369-5bb6-4b76-b40c-b497239cc240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from api_keys import API_KEYS\n",
    "from consts import DATA_PATH\n",
    "\n",
    "def answer_open_question(question):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "   \n",
    "    prompt = f\"You are a candidate being interviewed for a specific role. Your task is to answer the open questions provided to you in a professional, clear, and concise manner, as if you are participating in a real interview. Understand the Question: Take a moment to fully grasp what is being asked. The question may be a professional one or a question about you. If the question requires an example or explanation, make sure to provide one. Answer Clearly: Provide structured responses. Use full sentences, avoid unnecessary jargon, and ensure your answer directly addresses the question. Be Thoughtful: Demonstrate your expertise by including relevant concepts, examples, or real-world applications in your answers. Stay Brief: Avoid over-explaining but ensure your answer is complete. Strike a balance between depth and brevity. For example: Question: 'Can you explain the concept of overfitting in machine learning and provide a solution to address it?'. Your Answer: 'Overfitting occurs when a model learns the details and noise in the training data, leading to poor generalization on unseen data. To address it, we can use techniques like regularization (e.g., L1 or L2), reduce the complexity of the model, or employ cross-validation to ensure robust performance'.\\nThis is the questions for you: {question}\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       answer = response.text.strip()\n",
    "       print(\"response:\", answer)\n",
    "       return answer if answer else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 900\n",
    "empty_answer_rows = open_questions_and_answers[open_questions['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_answer_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"answer\" column is empty\n",
    "        empty_answer_rows = open_questions_and_answers[(open_questions_and_answers['answer'] == '') & (open_questions_and_answers['question'].str.strip() != '')]\n",
    "        if (empty_answer_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"answers\"\n",
    "        indices_to_update = empty_answer_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        open_questions_and_answers.loc[indices_to_update, 'answer'] = (\n",
    "            open_questions_and_answers.loc[indices_to_update, 'question']\n",
    "                .apply(answer_open_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b20a79-544e-4c70-b983-9d48f38874d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GEMINI_SIMULATION_DATA_PATH = os.path.join(DATA_PATH, 'gemini_simulation/')\n",
    "os.makedirs(GEMINI_SIMULATION_DATA_PATH, exist_ok=True)\n",
    "open_questions_and_answers.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'open_questions_answers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a78ceeaa-f1fb-4b95-a7f1-b8c01c92c1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Giving feedback on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2431b46f-b11d-421a-84cb-a6aca79b6d40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_open_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_open_questions.csv\")).drop_duplicates()\n",
    "jobs_with_open_questions['feedback'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efb16dc-4ae7-4a93-8dbe-f59ec0892a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_code_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_code_questions.csv\")).drop_duplicates()\n",
    "jobs_with_open_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_open_questions.csv\")).drop_duplicates()\n",
    "\n",
    "jobs_with_code_questions['feedback'] = ''\n",
    "jobs_with_open_questions['feedback'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15af5d33-9af3-4b83-91a8-5e514ebb2923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from api_keys import API_KEYS\n",
    "from consts import DATA_PATH\n",
    "\n",
    "seniority_map = [\"Internship\", \"Entry level/Associate\", \"Mid-Senior level/Manager and above\"]\n",
    "\n",
    "def question_feedback(question, job, level, job_summary):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    # Handle NaN values in 'level'\n",
    "    if pd.isna(level) or not str(level).isdigit():\n",
    "        seniority = \"Not specified\"\n",
    "    else:\n",
    "        seniority = seniority_map[int(level)]\n",
    "\n",
    "    prompt = f\"We need to evaluate how well a given interview question aligns with the requirements of a specific job position. Your task is to assign two scores between 1 and 10 based on the following criteria: \\nRelevance Score (1-10): How well does the question relate to the job's details? \\nSuitability Score (1-10): How appropriate is the question for assessing a candidate’s fitness for this role in a real interview setting?.\\n Provide the two scores as a comma-separated string. Evaluation Process: Consider the core skills and competencies needed for the job. Analyze whether the question effectively tests these skills or knowledge areas. \\nInput Example: Job: 'Data Analyst'. seniority: 'junior'. Skills: 'SQL, RDBMS, Non-RDBMS, MySQL, PostgreSQL, MongoDB, Python, Jupyter Notebook, Inferential Statistics, Probability, ETL, Data Pipeline, Automated Reporting, Data Analysis, Statistical Modeling, Machine Learning, Git, Hive, Spark, Presto, Diagnostic Analytics, Forecasting, Big Data, problem-solving.' \\nQuestion: 'How would you handle missing values in a dataset?'. \\nAnalysis: Relevance Score: 9 (The question directly relates to data preprocessing, which is crucial for data analysts). Suitability Score: 8 (A good question for assessing problem-solving skills, though it could be more role-specific by considering the industry context). \\nOutput: 9,8. \\nProvide the Relevance and Suitability scores to the following job details and question: Job: '{job}', seniority: '{seniority}', Job Summary: '{job_summary}', \\nQuestion: '{question}'.\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       feedback = response.text.strip()\n",
    "       print(\"response:\", feedback)\n",
    "       return feedback if feedback else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 240\n",
    "empty_feedback_rows = jobs_with_code_questions[jobs_with_code_questions['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_feedback_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"feedback\" column is empty\n",
    "        empty_feedback_rows = jobs_with_code_questions[(jobs_with_code_questions['feedback'] == '') & (jobs_with_code_questions['question'].str.strip() != '')]\n",
    "        if (empty_feedback_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"feedback\"\n",
    "        indices_to_update = empty_feedback_rows.index[:15]\n",
    "\n",
    "        # Apply the function with all required columns\n",
    "        jobs_with_code_questions.loc[indices_to_update, 'feedback'] = jobs_with_code_questions.loc[indices_to_update].apply(\n",
    "            lambda row: question_feedback(row['question'], row['job_title'], row['level'], row['job_summary']), axis=1\n",
    "        )\n",
    "    running_time = time.time() - start_time\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79da2645-35b3-4978-bed7-bdcc2b9c93d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_code_questions.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'code_questions_feedback.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50c2c4d3-eed6-459d-b4f0-305a5b9ba79a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "seniority_map = [\"Internship\", \"Entry level/Associate\", \"Mid-Senior level/Manager and above\"]\n",
    "\n",
    "def question_feedback(question, job, level, job_summary):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    # Handle NaN values in 'level'\n",
    "    if pd.isna(level) or not str(level).isdigit():\n",
    "        seniority = \"Not specified\"\n",
    "    else:\n",
    "        seniority = seniority_map[int(level)]\n",
    "\n",
    "    prompt = f\"We need to evaluate how well a given interview question aligns with the requirements of a specific job position. Your task is to assign two scores between 1 and 10 based on the following criteria: \\nRelevance Score (1-10): How well does the question relate to the job's details? \\nSuitability Score (1-10): How appropriate is the question for assessing a candidate’s fitness for this role in a real interview setting?.\\n Provide the two scores as a comma-separated string. Evaluation Process: Consider the core skills and competencies needed for the job. Analyze whether the question effectively tests these skills or knowledge areas. \\nInput Example: Job: 'Data Analyst'. seniority: 'junior'. Skills: 'SQL, RDBMS, Non-RDBMS, MySQL, PostgreSQL, MongoDB, Python, Jupyter Notebook, Inferential Statistics, Probability, ETL, Data Pipeline, Automated Reporting, Data Analysis, Statistical Modeling, Machine Learning, Git, Hive, Spark, Presto, Diagnostic Analytics, Forecasting, Big Data, problem-solving.' \\nQuestion: 'How would you handle missing values in a dataset?'. \\nAnalysis: Relevance Score: 9 (The question directly relates to data preprocessing, which is crucial for data analysts). Suitability Score: 8 (A good question for assessing problem-solving skills, though it could be more role-specific by considering the industry context). \\nOutput: 9,8. \\nProvide the Relevance and Suitability scores to the following job details and question: Job: '{job}', seniority: '{seniority}', Job Summary: '{job_summary}', \\nQuestion: '{question}'.\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       feedback = response.text.strip()\n",
    "       print(\"response:\", feedback)\n",
    "       return feedback if feedback else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13471601-8dd5-4ebe-a2fb-dec40d745915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import google.generativeai as genai\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 900\n",
    "empty_feedback_rows = jobs_with_open_questions[jobs_with_open_questions['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_feedback_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"feedback\" column is empty\n",
    "        empty_feedback_rows = jobs_with_open_questions[(jobs_with_open_questions['feedback'] == '') & (jobs_with_open_questions['question'].str.strip() != '')]\n",
    "        if (empty_feedback_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"feedback\"\n",
    "        indices_to_update = empty_feedback_rows.index[:15]\n",
    "\n",
    "        # Apply the function with all required columns\n",
    "        jobs_with_open_questions.loc[indices_to_update, 'feedback'] = jobs_with_open_questions.loc[indices_to_update].apply(\n",
    "            lambda row: question_feedback(row['question'], row['job_title'], row['level'], row['job_summary']), axis=1\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a6dcd5-4b15-4b34-8ffd-4548478dfa6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_open_questions.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'open_questions_feedback.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "066ba46a-a265-4dd8-b262-ad4a4a1fd35e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(jobs_with_open_questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "128074b4-06e8-469c-9bae-facad83f0580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Visualising results\n",
    "Visualisations and metric calculations of code questions' true vs. predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef457240-eefe-4f17-a770-1904c3c535ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from consts import GEMINI_SIMULATION_DATA_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MatchingEvaluation\").getOrCreate()\n",
    "jobs_with_code_questions = open_csv_file(spark, GEMINI_SIMULATION_DATA_PATH, 'code_questions_feedback.csv') \\\n",
    "    .select('heuristic_score', 'feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34eff786-a006-449f-85d2-5fa044fdc022",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "# Extract only the first two numeric values from feedback and discard extra data\n",
    "predicted_and_true_scores = jobs_with_code_questions.withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Load the data\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Logistic Transformation\n",
    "predicted_and_true_scores_df[\"transformed_pred\"] = 1 / (1 + np.exp(-predicted_and_true_scores_df[\"heuristic_score\"]))\n",
    "\n",
    "# Fit Linear Regression\n",
    "reg = LinearRegression()\n",
    "reg.fit(predicted_and_true_scores_df[[\"transformed_pred\"]], predicted_and_true_scores_df[\"true_score\"] / 10)  # Normalize true score\n",
    "predicted_and_true_scores_df[\"adjusted_pred\"] = reg.predict(predicted_and_true_scores_df[[\"transformed_pred\"]])\n",
    "\n",
    "# Optional: Power Transformation (Box-Cox / Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "predicted_and_true_scores_df[\"power_transformed_pred\"] = pt.fit_transform(predicted_and_true_scores_df[[\"adjusted_pred\"]])\n",
    "\n",
    "# Evaluate Improvements\n",
    "mse = ((predicted_and_true_scores_df[\"true_score\"] / 10 - predicted_and_true_scores_df[\"adjusted_pred\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score\"] / 10 - predicted_and_true_scores_df[\"adjusted_pred\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"adjusted_pred\"], y=predicted_and_true_scores_df[\"true_score\"] / 10, alpha=0.6)\n",
    "plt.xlabel(\"Transformed Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Scatter Plot of Transformed Predicted vs True Scores (Code Questions)\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(predicted_and_true_scores_df[\"adjusted_pred\"], kde=True, color='blue', label='Adjusted Predicted Scores', alpha=0.6)\n",
    "sns.histplot(predicted_and_true_scores_df[\"true_score\"] / 10, kde=True, color='red', label='Normalized True Scores', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Distribution of Transformed Predicted and True Scores (Code Questions)\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals Distribution\n",
    "residuals = predicted_and_true_scores_df[\"true_score\"] / 10 - predicted_and_true_scores_df[\"adjusted_pred\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"purple\", alpha=0.6)\n",
    "plt.axvline(0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution (Code Questions)\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "spearman_corr, spearman_p = spearmanr(predicted_and_true_scores_df[\"adjusted_pred\"], predicted_and_true_scores_df[\"true_score\"] / 10)\n",
    "pearson_corr, pearson_p = pearsonr(predicted_and_true_scores_df[\"adjusted_pred\"], predicted_and_true_scores_df[\"true_score\"] / 10)\n",
    "kendall_corr, kendall_p = kendalltau(predicted_and_true_scores_df[\"adjusted_pred\"], predicted_and_true_scores_df[\"true_score\"] / 10)\n",
    "\n",
    "print(f\"Spearman Correlation: {spearman_corr:.4f} (p-value: {spearman_p:.4f})\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.4f} (p-value: {pearson_p:.4f})\")\n",
    "print(f\"Kendall's Tau: {kendall_corr:.4f} (p-value: {kendall_p:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0ae96f8-c23a-4f51-bec0-9f31b9e9c475",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "\n",
    "def compare_model_to_baseline(predicted_and_true_scores_df):\n",
    "    # Extract the true normalized scores for comparison\n",
    "    true_scores = predicted_and_true_scores_df[\"true_score\"] / 10\n",
    "\n",
    "    # Baseline 1: Mean Prediction\n",
    "    mean_score = true_scores.mean()\n",
    "    mean_predictions = np.full_like(true_scores, mean_score)\n",
    "\n",
    "    # Metrics for Mean Prediction\n",
    "    spearman_mean, _ = spearmanr(mean_predictions, true_scores)\n",
    "    pearson_mean, _ = pearsonr(mean_predictions, true_scores)\n",
    "    mse_mean = mean_squared_error(true_scores, mean_predictions)\n",
    "    mae_mean = mean_absolute_error(true_scores, mean_predictions)\n",
    "    rmse_mean = np.sqrt(mse_mean)\n",
    "\n",
    "    # Baseline 2: Random Prediction\n",
    "    random_predictions = np.random.uniform(low=0.0, high=1.0, size=true_scores.shape)\n",
    "\n",
    "    # Metrics for Random Prediction\n",
    "    spearman_random, _ = spearmanr(random_predictions, true_scores)\n",
    "    pearson_random, _ = pearsonr(random_predictions, true_scores)\n",
    "    mse_random = mean_squared_error(true_scores, random_predictions)\n",
    "    mae_random = mean_absolute_error(true_scores, random_predictions)\n",
    "    rmse_random = np.sqrt(mse_random)\n",
    "\n",
    "    # Store Baseline Metrics\n",
    "    baseline_metrics = {\n",
    "        \"Mean Prediction\": {\"MSE\": mse_mean, \"MAE\": mae_mean, \"RMSE\": rmse_mean, \"Spearman\": spearman_mean, \"Pearson\": pearson_mean},\n",
    "        \"Random Prediction\": {\"MSE\": mse_random, \"MAE\": mae_random, \"RMSE\": rmse_random, \"Spearman\": spearman_random, \"Pearson\": pearson_random}\n",
    "    }\n",
    "\n",
    "    # Model's Metrics (Assuming these are computed earlier in the workflow)\n",
    "    model_metrics = {\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"RMSE\": rmse,\n",
    "        \"Spearman\": spearman_corr,\n",
    "        \"Pearson\": pearson_corr\n",
    "    }\n",
    "\n",
    "    # Print Results\n",
    "    print(f\"{'Metric':<15} {'Model':<15} {'Mean Baseline':<15} {'Random Baseline':<15}\")\n",
    "    for metric, model_value in model_metrics.items():\n",
    "        mean_baseline_value = baseline_metrics[\"Mean Prediction\"].get(metric, \"N/A\")\n",
    "        random_baseline_value = baseline_metrics[\"Random Prediction\"].get(metric, \"N/A\")\n",
    "        print(f\"{metric:<15} {model_value:<15.4f} {mean_baseline_value:<15.4f} {random_baseline_value:<15.4f}\")\n",
    "\n",
    "compare_model_to_baseline(predicted_and_true_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36832f77-7ea4-47a5-ac69-fdbf46033316",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Visualisations and metric calculations of open questions' true vs. predicted results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff75ef88-2dd8-417c-9cf0-c43e8836702d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from consts import GEMINI_SIMULATION_DATA_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MatchingEvaluation\").getOrCreate()\n",
    "jobs_with_open_questions = open_csv_file(spark, GEMINI_SIMULATION_DATA_PATH, 'open_questions_feedback.csv') \\\n",
    "    .select('heuristic_score', 'feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee2361d-affd-4e11-9634-e54b79d909ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, count, split, regexp_extract\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, ttest_rel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract only the first two numeric values from feedback\n",
    "predicted_and_true_scores = jobs_with_open_questions.select('heuristic_score', 'feedback') \\\n",
    "    .withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Convert to Pandas for visualization and analysis\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Normalize true scores (range [0,1])\n",
    "predicted_and_true_scores_df[\"true_score_normalized\"] = predicted_and_true_scores_df[\"true_score\"] / 10\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = predicted_and_true_scores_df[\"heuristic_score\"]\n",
    "\n",
    "# Error Metrics\n",
    "mse = ((predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"final_predicted_score\"], y=predicted_and_true_scores_df[\"true_score_normalized\"], alpha=0.6)\n",
    "plt.xlabel(\"Transformed Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Scatter Plot of Transformed Predicted vs True Scores (Open Questions)\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(predicted_and_true_scores_df[\"final_predicted_score\"], kde=True, color='blue', label='Final Predicted Scores', alpha=0.6)\n",
    "sns.histplot(predicted_and_true_scores_df[\"true_score_normalized\"], kde=True, color='red', label='Normalized True Scores', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Distribution of Transformed Predicted and True Scores (Open Questions)\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Distribution\n",
    "residuals = predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"purple\", alpha=0.6)\n",
    "plt.axvline(0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution (Open Questions)\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "spearman_corr, _ = spearmanr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "pearson_corr, _ = pearsonr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "kendall_corr, _ = kendalltau(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "\n",
    "print(f\"Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Kendall's Tau: {kendall_corr:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25a8f103-72ad-455f-a17e-9d2e3a11cb32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "compare_model_to_baseline(predicted_and_true_scores_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0a4b89-5afb-49d7-9a1e-99105852643d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "answer_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

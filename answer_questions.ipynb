{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b7a94b3-0134-4751-ac3c-d466a37f1b3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# check if the candidate user answered correctly and gives feedback to the user and a score\n",
    "\n",
    "feedback_prompt = f\"The user provides an answer to a specific question, you need to evaluate their solution. Your task is to: Give detailed feedback on the solution, highlighting strengths, identifying areas for improvement, and pointing out any missing or incorrect aspects.Assign a score between 0 and 100 based on how well the user's solution aligns with the ideal answer. The score should reflect the correctness, completeness, and clarity of the solution.Hereâ€™s how to proceed: Read the Question:{question}.\\n Understand the requirements of the question. Evaluate the following answer: {answer}.\\n Compare the user's solution to the expected answer or standard criteria for that question. Consider whether the solution addresses all key points, uses appropriate methods, and avoids critical mistakes.Provide Feedback: Write constructive comments explaining what the user did well and what could be improved. Assign a Score: The score should reflect the degree of correctness, with 0 meaning the solution is entirely incorrect and 100 meaning it is perfect. For example: Question: Explain the concept of overfitting in machine learning and how to address it. User's Answer: Overfitting happens when a model performs very well on the training data but fails to generalize to new data. It can be solved using dropout or by adding more layers to the model. Ideal Answer: Overfitting occurs when a model learns the noise and details of the training data instead of the underlying patterns, causing poor performance on unseen data. It can be mitigated using techniques like cross-validation, regularization (e.g., L1, L2), reducing model complexity, or increasing training data. Feedback: Strengths: The user correctly identified overfitting as an issue with generalization and mentioned dropout as a potential solution. Improvements: Adding more layers may worsen overfitting, not fix it. Other key solutions like regularization, reducing model complexity, and cross-validation were missing. Score: 70/100.Based on the user's input, follow this framework to provide feedback and a score for their answer to the given question.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b5a8067-6afa-4357-8341-9503da612244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# answering questions and gives feedback as a user of how well the simulation was\n",
    "\n",
    "candidate_Gemini_prompt = f\"You are a candidate being interviewed for a specific role. Your task is to answer the questions provided to you in a professional, clear, and concise manner, as if you are participating in a real interview. Understand the Question: Take a moment to fully grasp what is being asked. If the question requires an example or explanation, make sure to provide one. Answer Clearly: Provide structured responses. Use full sentences, avoid unnecessary jargon, and ensure your answer directly addresses the question. Be Thoughtful: Demonstrate your expertise by including relevant concepts, examples, or real-world applications in your answers. Stay Brief: Avoid over-explaining but ensure your answer is complete. Strike a balance between depth and brevity. For example: Question: Can you explain the concept of overfitting in machine learning and provide a solution to address it?. Your Answer: Overfitting occurs when a model learns the details and noise in the training data, leading to poor generalization on unseen data. To address it, we can use techniques like regularization (e.g., L1 or L2), reduce the complexity of the model, or employ cross-validation to ensure robust performance. After answering all the questions, provide feedback on the interview simulation: How well did the questions align with the job role you are applying for? How well do you think you performed in answering the questions? Did you enjoy the simulation? Were there any questions you felt were missing that would have helped you be better prepared for the actual interview? Here are the questions for you: {questions}\"\n",
    "\n",
    "\"For example: Question: 'Can you explain the concept of overfitting in machine learning and provide a solution to address it?'. Your Answer: 'Overfitting occurs when a model learns the details and noise in the training data, leading to poor generalization on unseen data. To address it, we can use techniques like regularization (e.g., L1 or L2), reduce the complexity of the model, or employ cross-validation to ensure robust performance'.\"\n",
    "\n",
    "\"After answering all the questions, provide feedback on the interview simulation: How well did the questions align with the job role you are applying for? How well do you think you performed in answering the questions? Did you enjoy the simulation? Were there any questions you felt were missing that would have helped you be better prepared for the actual interview?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6256fedf-14bd-4dec-b10b-67694ad5baaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_candidate_Gemini_prompt = f\"You are a candidate being interviewed for a specific role related to coding. Your task is to answer the questions provided to you in a professional, clear, and concise manner, as if you are participating in a real interview. Understand the Question: Take a moment to fully grasp what is being asked. If the question requires a code example or explanation, ensure your example is clear, functional, and concise.Answer Clearly: Provide structured responses, focusing on clarity and correctness in your code. Use full sentences to explain your approach, avoid unnecessary jargon, and ensure your answer directly addresses the question. Be Thoughtful: Demonstrate your expertise by explaining key concepts, showing your reasoning, and, when appropriate, providing code snippets that solve the problem. Highlight relevant tools or libraries and discuss how they can be applied. Stay Brief: Avoid over-explaining but ensure your answer is complete. Strike a balance between depth and brevity. For example: Question: How would you implement a function to reverse a linked list in Python? Your Answer: To reverse a linked list, we can iterate through the list, changing the next pointer of each node to point to the previous node. Here's a Python implementation: class ListNode: \\n def __init__(self, val=0, next=None): \\n\\t self.val = val \\n\\t self.next = next \\n def reverse_linked_list(head): \\n\\t prev = None \\n\\t current = head \\n\\t while current: \\n\\t\\t next_node = current.next \\n\\t\\t current.next = prev \\n\\t\\t prev = current \\n\\t\\t current = next_node \\n\\t return prev \\n After answering all the questions, provide feedback on the interview simulation: How well did the questions align with the job role you are applying for?, How well do you think you performed in answering the questions?, Did you enjoy the simulation?, Were there any questions you felt were missing that would have helped you be better prepared for the actual interview?, Here are the questions for you: {questions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf754541-b0bc-4a66-a7ed-0cb3889954bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# \n",
    "question_job_compatibility _prompt = f\"We need to evaluate how well a given interview question aligns with the requirements of a specific job position. Your task is to assign two scores between 1 and 10 based on the following criteria: Relevance Score (1-10): How well does the question relate to the job's details? Suitability Score (1-10): How appropriate is the question for assessing a candidateâ€™s fitness for this role in a real interview setting?. Evaluation Process: Consider the core skills and competencies needed for the job. Analyze whether the question effectively tests these skills or knowledge areas. Input Example: Job: Data Analyst. seniority: junior. Skills: SQL, RDBMS, Non-RDBMS, MySQL, PostgreSQL, MongoDB, Python, Jupyter Notebook, Inferential Statistics, Probability, ETL, Data Pipeline, Automated Reporting, Data Analysis, Statistical Modeling, Machine Learning, Git, Hive, Spark, Presto, Diagnostic Analytics, Forecasting, Big Data, problem-solving. Question: How would you handle missing values in a dataset?. Example Output: Relevance Score: 9/10 (The question directly relates to data preprocessing, which is crucial for data analysts). Suitability Score: 8/10 (A good question for assessing problem-solving skills, though it could be more role-specific by considering the industry context). Letâ€™s analyze the following job details and question: Job: {job}, seniority:{seniority}, Job Summary: {job_summary}, Question: {question}. Provide a brief justification for each score.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "659b0075-12f3-4ccb-acda-afce4463869d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# checks how good is the answer of a user to a given open question\n",
    "\n",
    "open_questions_prompt = \"The user has provided an answer to an open-ended question. Your task is to evaluate their response based on the following criteria: Relevance & Completeness â€“ Does the answer directly address the question and cover all key aspects?, Clarity & Coherence â€“ Is the response well-structured, logically presented, and easy to understand?, Accuracy (for factual questions) or Depth & Justification (for subjective questions). If the question is factual, does the answer provide correct and well-supported information?, If the question is subjective, does the response demonstrate thoughtful reasoning, realistic insights, and a well-supported argument?, and Balance & Perspective (for subjective questions) â€“ Does the response consider different viewpoints or provide a well-rounded perspective?. Scoring: Assign a score between 0 and 100 based on the criteria above. 0 means the answer is entirely incorrect, off-topic, or lacks coherence. 100 means the response is fully relevant, well-articulated, and appropriately detailed. Examples: Example 1: Factual Question. Question: What are the key benefits of cloud computing?. Userâ€™s Answer: Cloud computing makes things faster and better. Ideal Answer: Cloud computing provides scalability, cost efficiency, remote accessibility, and security improvements. Businesses benefit from reduced infrastructure costs and improved collaboration.Feedback: Strengths: The answer hints at benefits but lacks specifics. Improvements: More details on specific advantages like scalability and cost savings would improve clarity. Score: 50/100. Example 2: Subjective Question. Question: Where do you see yourself in 5 years professionally?. Userâ€™s Answer: I want to have a good job and be successful. Ideal Answer: In five years, I aim to become a project manager in the tech industry, leading cross-functional teams. To prepare, I plan to gain experience in team management, obtain a PMP certification, and refine my leadership skills.Feedback: Strengths: The answer shows ambition. Improvements: It lacks specificity regarding career path and steps to achieve success. Score: 60/100. Now, evaluate the following response: Question: {question}. User's Answer: {answer}. Provide constructive feedback and assign a score based on clarity, completeness, reasoning, and relevance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "703a6efb-e5a2-4927-9d24-d69df04462a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_questions_prompt = f\"The user has submitted a code solution to a programming question. Your task is to analyze and evaluate the code, considering both correctness and the quality of the approach. Even if the code has syntax errors or does not compile, partial credit should be awarded if the logic or idea is sound. Evaluation Criteria: Correctness â€“ Does the code produce the expected output for all cases? Is it written in the required programming language (if specified)?. Functionality â€“ Does the code correctly solve the problem?. Efficiency â€“ If there are complexity constraints, is the code optimized in terms of time and space complexity?. Readability & Best Practices â€“ Is the code well-structured, using meaningful variable names and following coding standards?. Edge Case Handling â€“ Does the solution consider different edge cases, such as empty inputs, extreme values, or invalid data?. Logical Soundness (For Non-Compiling Code/Pseudocode) â€“ Even if the code contains syntax errors or is in pseudocode, does it show a clear and correct approach to solving the problem?. Scoring Guidelines:\n",
    "Assign a score between 0 and 100 based on the above criteria. 0 = An entirely incorrect or non-functional solution with no meaningful approach. 100 = A fully correct, efficient (if needed), and well-structured solution. Example Evaluations: Example 1: Fully Correct Code. Question: Write a Python function that returns the factorial of a number. Userâ€™s Code: \\n def factorial(n): \\n\\t if not isinstance(n, int) or n < 0: \\n\\t\\t raise ValueError('Input must be a non-negative integer') \\n\\t result = 1 \\n\\t for i in range(2, n + 1): \\n\\t\\t result *= i \\n\\t return result \\n Feedback: Correct,handles all cases including invalid input. Efficient (iterative approach avoids recursion depth issues). Readable and follows best practices. Score: 100/100. Now, evaluate the following coding solution: Question: {question} \\n User's Code: \\n {answer} \\n Provide feedback based on correctness, efficiency, readability, edge cases, and logical soundness. Assign a score accordingly, giving partial credit for a good idea even if the code does not compile.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c08200e0-750b-4a43-9a95-7548dd4b252b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "question_answer_solution_prompt = code_questions_prompt = f\"\"\"\n",
    "The user has submitted a code solution to a programming question. Your task is to analyze and evaluate the code, considering both correctness and the quality of the approach. Even if the code has syntax errors or does not compile, partial credit should be awarded if the logic or idea is sound. \n",
    "\n",
    "Evaluation Criteria:\n",
    "- Correctness â€“ Does the code produce the expected output for all cases? Is it written in the required programming language (if specified)?\n",
    "- Functionality â€“ Does the code correctly solve the problem?\n",
    "- Efficiency â€“ If there are complexity constraints, is the code optimized in terms of time and space complexity?\n",
    "- Readability & Best Practices â€“ Is the code well-structured, using meaningful variable names and following coding standards?\n",
    "- Edge Case Handling â€“ Does the solution consider different edge cases, such as empty inputs, extreme values, or invalid data?\n",
    "- Logical Soundness (For Non-Compiling Code/Pseudocode) â€“ Even if the code contains syntax errors or is in pseudocode, does it show a clear and correct approach to solving the problem?\n",
    "\n",
    "Scoring Guidelines:\n",
    "Assign a score between 0 and 100 based on the above criteria. 0 = An entirely incorrect or non-functional solution with no meaningful approach. 100 = A fully correct, efficient (if needed), and well-structured solution.\n",
    "\n",
    "Example Evaluations:\n",
    "Example 1: Fully Correct Code.\n",
    "Question: Write a Python function that returns the factorial of a number.\n",
    "Userâ€™s Code: \n",
    "    def factorial(n):\n",
    "        if not isinstance(n, int) or n < 0:\n",
    "            raise ValueError('Input must be a non-negative integer')\n",
    "        result = 1\n",
    "        for i in range(2, n + 1):\n",
    "            result *= i\n",
    "        return result\n",
    "Feedback: Correct, handles all cases including invalid input. Efficient (iterative approach avoids recursion depth issues). Readable and follows best practices. Score: 100/100.\n",
    "\n",
    "Now, evaluate the following coding solution:\n",
    "\n",
    "Question: {question}\n",
    "User's Code: \n",
    "{answer}\n",
    "\n",
    "Java Solution (if available): {java_solution}\n",
    "\n",
    "If the Java solution is provided, check if the userâ€™s solution is correct by comparing it to the Java solution. Provide feedback based on correctness, efficiency, readability, edge cases, and logical soundness. Assign a score accordingly, giving partial credit for a good idea even if the code does not compile.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39955b4b-3b77-455a-a73c-1cba240ba9f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dbe1eb8-dab3-42a8-86a4-f56cfd478cea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from consts import DATA_PATH\n",
    "\n",
    "# Load datasets\n",
    "code_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_code_questions.csv\")).drop_duplicates()\n",
    "open_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_open_questions.csv\")).drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dd63259-80db-48aa-9805-f7c973faa9bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Answering the questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f586344d-a8a5-4fe1-b455-81506d88b0f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_questions_and_answers = code_questions[['question']].drop_duplicates()\n",
    "code_questions_and_answers['answer'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d9b76a3-91e0-4d71-b69e-fc3f3c7bfe8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import time\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "def answer_code_question(question):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    prompt = f\"You are a candidate being interviewed for a specific role related to coding. Your task is to answer the questions provided to you in a professional, clear, and concise manner, as if you are participating in a real interview. Understand the Question: Take a moment to fully grasp what is being asked. The question requires a code answer, ensure your code is clear and functional. Answer Clearly: Provide structured responses, focusing on clarity and correctness in your code. The question might contain multiple requirements, input-output examples, hints, and other specifications. \\nFor example: Question: 'Implement a function to reverse a linked list in Python.'. Your Answer: 'class ListNode: \\n def __init__(self, val=0, next=None): \\n\\t self.val = val \\n\\t self.next = next \\n def reverse_linked_list(head): \\n\\t prev = None \\n\\t current = head \\n\\t while current: \\n\\t\\t next_node = current.next \\n\\t\\t current.next = prev \\n\\t\\t prev = current \\n\\t\\t current = next_node \\n\\t return prev \\n'. Here is the question for you: {question}\"\n",
    "   \n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       answer = response.text.strip()\n",
    "       print(\"response:\", answer)\n",
    "       return answer if answer else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "    \n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 3000\n",
    "empty_answer_rows = code_questions_and_answers[code_questions_and_answers['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_answer_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"answer\" column is empty\n",
    "        empty_answer_rows = code_questions_and_answers[(code_questions_and_answers['answer'] == '') & (code_questions_and_answers['question'].str.strip() != '')]\n",
    "        if (empty_answer_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"answers\"\n",
    "        indices_to_update = empty_answer_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        code_questions_and_answers.loc[indices_to_update, 'answer'] = (\n",
    "            code_questions_and_answers.loc[indices_to_update, 'question']\n",
    "                .apply(answer_code_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66a4b097-012b-467a-a7bc-7b04cbcd1e6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(code_questions_and_answers)\n",
    "code_questions_and_answers.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, \"code_questions_answers.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67adb448-127a-43de-ac26-6122f3f94b7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "open_questions_and_answers = open_questions[['question']].drop_duplicates()\n",
    "open_questions_and_answers['answer'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc662369-5bb6-4b76-b40c-b497239cc240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from api_keys import API_KEYS\n",
    "from consts import DATA_PATH\n",
    "\n",
    "def answer_open_question(question):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "   \n",
    "    prompt = f\"You are a candidate being interviewed for a specific role. Your task is to answer the open questions provided to you in a professional, clear, and concise manner, as if you are participating in a real interview. Understand the Question: Take a moment to fully grasp what is being asked. The question may be a professional one or a question about you. If the question requires an example or explanation, make sure to provide one. Answer Clearly: Provide structured responses. Use full sentences, avoid unnecessary jargon, and ensure your answer directly addresses the question. Be Thoughtful: Demonstrate your expertise by including relevant concepts, examples, or real-world applications in your answers. Stay Brief: Avoid over-explaining but ensure your answer is complete. Strike a balance between depth and brevity. For example: Question: 'Can you explain the concept of overfitting in machine learning and provide a solution to address it?'. Your Answer: 'Overfitting occurs when a model learns the details and noise in the training data, leading to poor generalization on unseen data. To address it, we can use techniques like regularization (e.g., L1 or L2), reduce the complexity of the model, or employ cross-validation to ensure robust performance'.\\nThis is the questions for you: {question}\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       answer = response.text.strip()\n",
    "       print(\"response:\", answer)\n",
    "       return answer if answer else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 900\n",
    "empty_answer_rows = open_questions_and_answers[open_questions['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_answer_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"answer\" column is empty\n",
    "        empty_answer_rows = open_questions_and_answers[(open_questions_and_answers['answer'] == '') & (open_questions_and_answers['question'].str.strip() != '')]\n",
    "        if (empty_answer_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"answers\"\n",
    "        indices_to_update = empty_answer_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        open_questions_and_answers.loc[indices_to_update, 'answer'] = (\n",
    "            open_questions_and_answers.loc[indices_to_update, 'question']\n",
    "                .apply(answer_open_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80b20a79-544e-4c70-b983-9d48f38874d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "GEMINI_SIMULATION_DATA_PATH = os.path.join(DATA_PATH, 'gemini_simulation/')\n",
    "os.makedirs(GEMINI_SIMULATION_DATA_PATH, exist_ok=True)\n",
    "open_questions_and_answers.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'open_questions_answers.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a78ceeaa-f1fb-4b95-a7f1-b8c01c92c1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Giving feedback on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efb16dc-4ae7-4a93-8dbe-f59ec0892a14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_code_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_code_questions.csv\")).drop_duplicates()\n",
    "jobs_with_open_questions = pd.read_csv(os.path.join(DATA_PATH, \"top_open_questions.csv\")).drop_duplicates()\n",
    "\n",
    "jobs_with_code_questions['feedback'] = ''\n",
    "jobs_with_open_questions['feedback'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15af5d33-9af3-4b83-91a8-5e514ebb2923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "from api_keys import API_KEYS\n",
    "from consts import DATA_PATH\n",
    "\n",
    "seniority_map = [\"Internship\", \"Entry level/Associate\", \"Mid-Senior level/Manager and above\"]\n",
    "\n",
    "def question_feedback(question, job, level, job_summary):\n",
    "    if pd.isna(question) or question.strip() == '':\n",
    "        return ''\n",
    "    \n",
    "    # Handle NaN values in 'level'\n",
    "    if pd.isna(level) or not str(level).isdigit():\n",
    "        seniority = \"Not specified\"\n",
    "    else:\n",
    "        seniority = seniority_map[int(level)]\n",
    "\n",
    "    prompt = f\"We need to evaluate how well a given interview question aligns with the requirements of a specific job position. Your task is to assign two scores between 1 and 10 based on the following criteria: \\nRelevance Score (1-10): How well does the question relate to the job's details? \\nSuitability Score (1-10): How appropriate is the question for assessing a candidateâ€™s fitness for this role in a real interview setting?.\\n Provide the two scores as a comma-separated string. Evaluation Process: Consider the core skills and competencies needed for the job. Analyze whether the question effectively tests these skills or knowledge areas. \\nInput Example: Job: 'Data Analyst'. seniority: 'junior'. Skills: 'SQL, RDBMS, Non-RDBMS, MySQL, PostgreSQL, MongoDB, Python, Jupyter Notebook, Inferential Statistics, Probability, ETL, Data Pipeline, Automated Reporting, Data Analysis, Statistical Modeling, Machine Learning, Git, Hive, Spark, Presto, Diagnostic Analytics, Forecasting, Big Data, problem-solving.' \\nQuestion: 'How would you handle missing values in a dataset?'. \\nAnalysis: Relevance Score: 9 (The question directly relates to data preprocessing, which is crucial for data analysts). Suitability Score: 8 (A good question for assessing problem-solving skills, though it could be more role-specific by considering the industry context). \\nOutput: 9,8. \\nProvide the Relevance and Suitability scores to the following job details and question: Job: '{job}', seniority: '{seniority}', Job Summary: '{job_summary}', \\nQuestion: '{question}'.\"\n",
    "\n",
    "    try:\n",
    "       response = model.generate_content(prompt)\n",
    "       feedback = response.text.strip()\n",
    "       print(\"response:\", feedback)\n",
    "       return feedback if feedback else ''\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return ''\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 240\n",
    "empty_feedback_rows = jobs_with_code_questions[jobs_with_code_questions['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_feedback_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"feedback\" column is empty\n",
    "        empty_feedback_rows = jobs_with_code_questions[(jobs_with_code_questions['feedback'] == '') & (jobs_with_code_questions['question'].str.strip() != '')]\n",
    "        if (empty_feedback_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"feedback\"\n",
    "        indices_to_update = empty_feedback_rows.index[:15]\n",
    "\n",
    "        # Apply the function with all required columns\n",
    "        jobs_with_code_questions.loc[indices_to_update, 'feedback'] = jobs_with_code_questions.loc[indices_to_update].apply(\n",
    "            lambda row: question_feedback(row['question'], row['job_title'], row['level'], row['job_summary']), axis=1\n",
    "        )\n",
    "    running_time = time.time() - start_time\n",
    "    time.sleep(30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79da2645-35b3-4978-bed7-bdcc2b9c93d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_code_questions.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'code_questions_feedback.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13471601-8dd5-4ebe-a2fb-dec40d745915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "running_time = 0\n",
    "max_time = 900\n",
    "empty_feedback_rows = jobs_with_open_questions[jobs_with_open_questions['question'].str.strip() != '']\n",
    "\n",
    "while (running_time < max_time) and (empty_feedback_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"feedback\" column is empty\n",
    "        empty_feedback_rows = jobs_with_open_questions[(jobs_with_open_questions['feedback'] == '') & (jobs_with_open_questions['question'].str.strip() != '')]\n",
    "        if (empty_feedback_rows.shape[0] == 0) or (time.time() - start_time > max_time):\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"feedback\"\n",
    "        indices_to_update = empty_feedback_rows.index[:15]\n",
    "\n",
    "        # Apply the function with all required columns\n",
    "        jobs_with_open_questions.loc[indices_to_update, 'feedback'] = jobs_with_open_questions.loc[indices_to_update].apply(\n",
    "            lambda row: question_feedback(row['question'], row['job_title'], row['level'], row['job_summary']), axis=1\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95a6dcd5-4b15-4b34-8ffd-4548478dfa6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jobs_with_open_questions.to_csv(os.path.join(GEMINI_SIMULATION_DATA_PATH, 'open_questions_feedback.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a7d60a-df44-454f-8391-c46f2b4caed0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from statsmodels.graphics.regressionplots import influence_plot\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Convert 'feedback' from string to numeric average\n",
    "jobs_with_open_questions['feedback_avg'] = (\n",
    "    jobs_with_open_questions['feedback']\n",
    "    .astype(str)\n",
    "    .apply(lambda x: np.mean([float(num) for num in x.split(',')]))\n",
    ")\n",
    "\n",
    "# Rescale heuristic_score to match feedback scale (1-10)\n",
    "jobs_with_open_questions['heuristic_score_scaled'] = 1 + 9 * jobs_with_open_questions['heuristic_score']\n",
    "\n",
    "# Extract true and predicted values\n",
    "y_true = jobs_with_open_questions['feedback_avg']\n",
    "y_pred = jobs_with_open_questions['heuristic_score_scaled']\n",
    "\n",
    "# 1. Regression Metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# 2. Correlation Analysis\n",
    "pearson_r, pearson_p = stats.pearsonr(y_true, y_pred)\n",
    "spearman_rho, spearman_p = stats.spearmanr(y_true, y_pred)\n",
    "kendall_tau, kendall_p = stats.kendalltau(y_true, y_pred)\n",
    "\n",
    "print(f\"Pearson Correlation: {pearson_r:.4f} (p={pearson_p:.4f})\")\n",
    "print(f\"Spearman Rank Correlation: {spearman_rho:.4f} (p={spearman_p:.4f})\")\n",
    "print(f\"Kendallâ€™s Tau: {kendall_tau:.4f} (p={kendall_p:.4f})\")\n",
    "\n",
    "# 3. Statistical Tests\n",
    "t_stat, t_p = stats.ttest_rel(y_true, y_pred)\n",
    "wilcoxon_stat, wilcoxon_p = stats.wilcoxon(y_true, y_pred)\n",
    "\n",
    "print(f\"Paired t-test: t={t_stat:.4f}, p={t_p:.4f}\")\n",
    "print(f\"Wilcoxon Signed-Rank Test: W={wilcoxon_stat:.4f}, p={wilcoxon_p:.4f}\")\n",
    "\n",
    "# 4. Error Distribution Analysis\n",
    "errors = y_true - y_pred\n",
    "\n",
    "# Histogram of errors\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(errors, bins=20, kde=True)\n",
    "plt.axvline(0, color='red', linestyle='--', label=\"Zero Error Line\")\n",
    "plt.title(\"Error Distribution (True - Predicted)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Q-Q Plot for errors\n",
    "plt.figure(figsize=(6, 6))\n",
    "stats.probplot(errors, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Errors\")\n",
    "plt.show()\n",
    "\n",
    "# Skewness & Kurtosis\n",
    "skewness = stats.skew(errors)\n",
    "kurtosis = stats.kurtosis(errors)\n",
    "\n",
    "print(f\"Skewness of Errors: {skewness:.4f}\")\n",
    "print(f\"Kurtosis of Errors: {kurtosis:.4f}\")\n",
    "\n",
    "# 5. Residual Plot (Residual vs Fitted Plot)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_pred, y=errors, lowess=True, color=\"blue\", line_kws={'color': 'red'})\n",
    "plt.title(\"Residuals vs Fitted (Predicted) Values\")\n",
    "plt.xlabel(\"Fitted Values (Predicted)\")\n",
    "plt.ylabel(\"Residuals (Errors)\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Add constant to the independent variables for intercept\n",
    "X = sm.add_constant(y_pred)\n",
    "model = sm.OLS(y_true, X).fit()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "influence_plot(model)\n",
    "plt.title(\"Leverage vs Residuals Plot (Influence Plot)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "492b52d3-2c0a-4998-8b21-6ba911a4056b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### **Analysis of the Evaluation Results**  \n",
    "\n",
    "---\n",
    "\n",
    "### **1. Regression Metrics**  \n",
    "#### ðŸ”¹ **Mean Absolute Error (MAE):** **2.2530**  \n",
    "- The average absolute difference between the predicted and true values is **2.25**.\n",
    "- Lower values indicate better predictions.  \n",
    "\n",
    "#### ðŸ”¹ **Mean Squared Error (MSE):** **6.2556**  \n",
    "- Measures squared differences, penalizing larger errors more than MAE.  \n",
    "\n",
    "#### ðŸ”¹ **Root Mean Squared Error (RMSE):** **2.5011**  \n",
    "- Shows that typical prediction errors are around **2.50** units.  \n",
    "\n",
    "#### ðŸ”¹ **RÂ² Score (Coefficient of Determination):** **-0.4725**  \n",
    "- **Negative RÂ²** suggests that the model performs **worse than simply predicting the mean** of `feedback`.  \n",
    "- Ideally, RÂ² should be closer to **1**, meaning the model explains most of the variance.\n",
    "\n",
    "ðŸ’¡ **Conclusion:**  \n",
    "The model has significant prediction errors, and its predictions do not explain much of the variance in the actual values.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Correlation Analysis**  \n",
    "#### ðŸ”¹ **Pearson Correlation:** **0.1369** (p=0.0001)  \n",
    "- Weak **linear** correlation between predictions and true values.  \n",
    "- **p-value < 0.05** suggests statistical significance but not practical significance.  \n",
    "\n",
    "#### ðŸ”¹ **Spearman Rank Correlation:** **0.0939** (p=0.0073)  \n",
    "- Measures **monotonic relationships**; a small positive value suggests a weak association.  \n",
    "\n",
    "#### ðŸ”¹ **Kendallâ€™s Tau:** **0.0750** (p=0.0032)  \n",
    "- Also indicates a very weak correlation.  \n",
    "\n",
    "ðŸ’¡ **Conclusion:**  \n",
    "The predicted `heuristic_score` values are **weakly correlated** with the true `feedback` values, meaning the model is **not capturing meaningful patterns**.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Statistical Tests**  \n",
    "#### ðŸ”¹ **Paired t-test:** **t=-20.1937, p=0.0000**  \n",
    "- The t-test checks whether predictions and actual values have significantly different means.  \n",
    "- Since **p = 0.0000** (very small), the difference is **statistically significant**, meaning the model is **systematically biased**.  \n",
    "\n",
    "#### ðŸ”¹ **Wilcoxon Signed-Rank Test:** **W=52274.0000, p=0.0000**  \n",
    "- A non-parametric test that confirms significant differences between actual and predicted values.  \n",
    "\n",
    "ðŸ’¡ **Conclusion:**  \n",
    "The predicted values **systematically differ** from the true values, suggesting **model bias**.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Error Distribution Analysis**  \n",
    "#### ðŸ”¹ **Histogram of Errors** (True - Predicted)  \n",
    "- Errors are **not normally distributed**.  \n",
    "- There is a clear bias with more negative errors, suggesting **underprediction** (i.e., predictions are lower than actual values).  \n",
    "- Bimodal distribution suggests **systematic prediction errors** for different groups of values.\n",
    "\n",
    "#### ðŸ”¹ **Q-Q Plot of Errors**  \n",
    "- The points **deviate from the red diagonal**, especially in the tails, meaning the errors **are not normally distributed**.  \n",
    "- There is an excess of extreme errors.\n",
    "\n",
    "#### ðŸ”¹ **Skewness = 0.5419**  \n",
    "- **Positive skew** means more predictions **underestimate** feedback values.  \n",
    "\n",
    "#### ðŸ”¹ **Kurtosis = -0.8881**  \n",
    "- Negative kurtosis suggests a **flatter distribution**, meaning fewer extreme values than a normal distribution.  \n",
    "\n",
    "ðŸ’¡ **Conclusion:**  \n",
    "- **Errors are biased** (predictions are too low).  \n",
    "- **Non-normal error distribution** suggests the need for **model improvement** (e.g., different transformation techniques or feature engineering).  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Takeaways & Recommendations**  \n",
    "âŒ **Current Issues:**  \n",
    "- **High error values** (MAE = 2.25, RMSE = 2.50).  \n",
    "- **Negative RÂ²** suggests the model is **worse than random guessing**.  \n",
    "- **Weak correlation** (Pearson, Spearman, Kendall all close to 0).  \n",
    "- **Systematic underprediction** (positive skew).  \n",
    "- **Significant difference** between actual and predicted values (paired t-test, Wilcoxon test).  \n",
    "\n",
    "âœ… **Next Steps to Improve the Model:**  \n",
    "1. **Feature Engineering:**  \n",
    "   - Include additional relevant features that may improve prediction accuracy.  \n",
    "   - Consider **non-linear transformations** of input variables.  \n",
    "\n",
    "2. **Try a Different Model:**  \n",
    "   - If using a **linear model**, consider a **non-linear regression model** (e.g., Random Forest, Neural Networks).  \n",
    "   - If using a **simple heuristic**, explore **ML-based models**.  \n",
    "\n",
    "3. **Address Scaling Issues:**  \n",
    "   - Try different normalization strategies for **heuristic_score** before rescaling.  \n",
    "\n",
    "4. **Handle Systematic Bias:**  \n",
    "   - If predictions are systematically **lower**, adjust model predictions using bias correction.  \n",
    "   - Try quantile regression to model different parts of the feedback distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8062d7a-1c3e-4700-af0f-24820acefce4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import re\n",
    "from statsmodels.graphics.regressionplots import influence_plot\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# # Convert 'feedback' from string to numeric average\n",
    "# jobs_with_code_questions['feedback_avg'] = (\n",
    "#     jobs_with_code_questions['feedback']\n",
    "#     .astype(str)\n",
    "#     .apply(lambda x: np.mean([float(num) for num in x.split(',') if num.strip()]) if x.strip() else np.nan)\n",
    "# )\n",
    "\n",
    "jobs_with_code_questions['feedback_avg'] = (\n",
    "    jobs_with_code_questions['feedback']\n",
    "    .astype(str)\n",
    "    .apply(lambda x: np.mean([float(num) for num in re.findall(r'\\d+', x)]))\n",
    ")\n",
    "\n",
    "# Rescale heuristic_score to match feedback scale (1-10)\n",
    "jobs_with_code_questions['heuristic_score_scaled'] = 1 + 9 * jobs_with_code_questions['heuristic_score']\n",
    "\n",
    "# Extract true and predicted values\n",
    "y_true = jobs_with_code_questions['feedback_avg']\n",
    "y_pred = jobs_with_code_questions['heuristic_score_scaled']\n",
    "\n",
    "# 1. Regression Metrics\n",
    "mae = mean_absolute_error(y_true, y_pred)\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "print(f\"Mean Squared Error (MSE): {mse:.4f}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n",
    "\n",
    "# 2. Correlation Analysis\n",
    "pearson_r, pearson_p = stats.pearsonr(y_true, y_pred)\n",
    "spearman_rho, spearman_p = stats.spearmanr(y_true, y_pred)\n",
    "kendall_tau, kendall_p = stats.kendalltau(y_true, y_pred)\n",
    "\n",
    "print(f\"Pearson Correlation: {pearson_r:.4f} (p={pearson_p:.4f})\")\n",
    "print(f\"Spearman Rank Correlation: {spearman_rho:.4f} (p={spearman_p:.4f})\")\n",
    "print(f\"Kendallâ€™s Tau: {kendall_tau:.4f} (p={kendall_p:.4f})\")\n",
    "\n",
    "# 3. Statistical Tests\n",
    "t_stat, t_p = stats.ttest_rel(y_true, y_pred)\n",
    "wilcoxon_stat, wilcoxon_p = stats.wilcoxon(y_true, y_pred)\n",
    "\n",
    "print(f\"Paired t-test: t={t_stat:.4f}, p={t_p:.4f}\")\n",
    "print(f\"Wilcoxon Signed-Rank Test: W={wilcoxon_stat:.4f}, p={wilcoxon_p:.4f}\")\n",
    "\n",
    "# 4. Error Distribution Analysis\n",
    "errors = y_true - y_pred\n",
    "\n",
    "# Histogram of errors\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.histplot(errors, bins=20, kde=True)\n",
    "plt.axvline(0, color='red', linestyle='--', label=\"Zero Error Line\")\n",
    "plt.title(\"Error Distribution (True - Predicted)\")\n",
    "plt.xlabel(\"Error\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Q-Q Plot for errors\n",
    "plt.figure(figsize=(6, 6))\n",
    "stats.probplot(errors, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot of Errors\")\n",
    "plt.show()\n",
    "\n",
    "# Skewness & Kurtosis\n",
    "skewness = stats.skew(errors)\n",
    "kurtosis = stats.kurtosis(errors)\n",
    "\n",
    "print(f\"Skewness of Errors: {skewness:.4f}\")\n",
    "print(f\"Kurtosis of Errors: {kurtosis:.4f}\")\n",
    "\n",
    "# 5. Residual Plot (Residual vs Fitted Plot)\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.residplot(x=y_pred, y=errors, lowess=True, color=\"blue\", line_kws={'color': 'red'})\n",
    "plt.title(\"Residuals vs Fitted (Predicted) Values\")\n",
    "plt.xlabel(\"Fitted Values (Predicted)\")\n",
    "plt.ylabel(\"Residuals (Errors)\")\n",
    "plt.show()\n",
    "\n",
    "# 6. Add constant to the independent variables for intercept\n",
    "X = sm.add_constant(y_pred)\n",
    "model = sm.OLS(y_true, X).fit()\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "influence_plot(model)\n",
    "plt.title(\"Leverage vs Residuals Plot (Influence Plot)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef457240-eefe-4f17-a770-1904c3c535ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from consts import GEMINI_SIMULATION_DATA_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MatchingEvaluation\").getOrCreate()\n",
    "jobs_with_code_questions = open_csv_file(spark, GEMINI_SIMULATION_DATA_PATH, 'code_questions_feedback.csv') \\\n",
    "    .select('heuristic_score', 'feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a38906-901d-456b-bb51-328cee9b65bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, count, split, regexp_extract\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, ttest_rel\n",
    "\n",
    "# Extract only the first two numeric values from feedback and discard extra data\n",
    "predicted_and_true_scores = jobs_with_code_questions.withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Convert to Pandas for visualization and correlation analysis\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Scatter Plot\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"heuristic_score\"], y=predicted_and_true_scores_df[\"true_score\"], alpha=0.6)\n",
    "plt.xlabel(\"Predicted Score (Heuristic)\")\n",
    "plt.ylabel(\"True Score\")\n",
    "plt.title(\"Scatter Plot of Predicted vs True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(predicted_and_true_scores_df[\"heuristic_score\"], kde=True, color='blue', label='Predicted Scores', alpha=0.6)\n",
    "sns.histplot(predicted_and_true_scores_df[\"true_score\"] / 10, kde=True, color='red', label='Normalized True Scores', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Distribution of Predicted and True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis\n",
    "spearman_corr, _ = spearmanr(predicted_and_true_scores_df[\"heuristic_score\"], predicted_and_true_scores_df[\"true_score\"])\n",
    "pearson_corr, _ = pearsonr(predicted_and_true_scores_df[\"heuristic_score\"], predicted_and_true_scores_df[\"true_score\"])\n",
    "kendall_corr, _ = kendalltau(predicted_and_true_scores_df[\"heuristic_score\"], predicted_and_true_scores_df[\"true_score\"])\n",
    "\n",
    "print(f\"Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Kendall's Tau: {kendall_corr:.3f}\")\n",
    "\n",
    "# Error Metrics\n",
    "mse = ((predicted_and_true_scores_df[\"true_score\"] / 10 - predicted_and_true_scores_df[\"heuristic_score\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score\"] / 10 - predicted_and_true_scores_df[\"heuristic_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Paired t-test (if necessary)\n",
    "t_stat, p_value = ttest_rel(predicted_and_true_scores_df[\"heuristic_score\"], predicted_and_true_scores_df[\"true_score\"] / 10)\n",
    "print(f\"Paired t-test p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50f5eea6-03f0-4884-85f9-24235ade9d03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, min, max\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, ttest_rel\n",
    "\n",
    "# Rescale true scores using Min-Max Scaling\n",
    "min_true = predicted_and_true_scores.agg(min(col(\"true_score\"))).collect()[0][0]\n",
    "max_true = predicted_and_true_scores.agg(max(col(\"true_score\"))).collect()[0][0]\n",
    "\n",
    "predicted_and_true_scores = predicted_and_true_scores.withColumn(\"normalized_true_score\", \n",
    "    (col(\"true_score\") - min_true) / (max_true - min_true))\n",
    "\n",
    "# Convert to Pandas\n",
    "df = predicted_and_true_scores.select(\"heuristic_score\", \"normalized_true_score\").toPandas()\n",
    "\n",
    "# Boxplot for distribution comparison\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.boxplot(data=df, orient=\"h\")\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Box Plot of Predicted vs True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Residual plot (error distribution)\n",
    "df[\"residual\"] = df[\"normalized_true_score\"] - df[\"heuristic_score\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(df[\"residual\"], kde=True, bins=30, color=\"purple\")\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Recompute Correlation & Error Metrics\n",
    "spearman_corr, _ = spearmanr(df[\"heuristic_score\"], df[\"normalized_true_score\"])\n",
    "pearson_corr, _ = pearsonr(df[\"heuristic_score\"], df[\"normalized_true_score\"])\n",
    "kendall_corr, _ = kendalltau(df[\"heuristic_score\"], df[\"normalized_true_score\"])\n",
    "\n",
    "mse = ((df[\"normalized_true_score\"] - df[\"heuristic_score\"]) ** 2).mean()\n",
    "mae = abs(df[\"normalized_true_score\"] - df[\"heuristic_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "mbd = df[\"residual\"].mean()\n",
    "\n",
    "t_stat, p_value = ttest_rel(df[\"heuristic_score\"], df[\"normalized_true_score\"])\n",
    "\n",
    "# Print revised results\n",
    "print(f\"Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Kendall's Tau: {kendall_corr:.3f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"Mean Bias Deviation (MBD): {mbd:.4f}\")\n",
    "print(f\"Paired t-test p-value: {p_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "172cf5d9-204b-40f2-aa6b-039e88b22849",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import curve_fit\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "# Load the data\n",
    "df = predicted_and_true_scores_df.copy()\n",
    "\n",
    "# Logistic Transformation\n",
    "df[\"transformed_pred\"] = 1 / (1 + np.exp(-df[\"heuristic_score\"]))\n",
    "\n",
    "# Fit Linear Regression (Adjust bias and scaling)\n",
    "reg = LinearRegression()\n",
    "reg.fit(df[[\"transformed_pred\"]], df[\"true_score\"] / 10)  # Normalize true score\n",
    "df[\"adjusted_pred\"] = reg.predict(df[[\"transformed_pred\"]])\n",
    "\n",
    "# Optional: Power Transformation (Box-Cox / Yeo-Johnson)\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "df[\"power_transformed_pred\"] = pt.fit_transform(df[[\"adjusted_pred\"]])\n",
    "\n",
    "# Evaluate Improvements\n",
    "new_mse = ((df[\"true_score\"] / 10 - df[\"adjusted_pred\"]) ** 2).mean()\n",
    "new_mae = abs(df[\"true_score\"] / 10 - df[\"adjusted_pred\"]).mean()\n",
    "new_rmse = new_mse ** 0.5\n",
    "\n",
    "print(f\"New MSE: {new_mse:.4f}\")\n",
    "print(f\"New MAE: {new_mae:.4f}\")\n",
    "print(f\"New RMSE: {new_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9005c5b3-d816-46e1-ab6c-42ad6a176263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, count, split, regexp_extract\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, ttest_rel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract only the first two numeric values from feedback\n",
    "predicted_and_true_scores = jobs_with_code_questions.withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Convert to Pandas for visualization and analysis\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Normalize true scores (range [0,1])\n",
    "predicted_and_true_scores_df[\"true_score_normalized\"] = predicted_and_true_scores_df[\"true_score\"] / 10\n",
    "\n",
    "# Apply a logistic transformation to predicted scores\n",
    "predicted_and_true_scores_df[\"transformed_predicted_score\"] = 1 / (1 + np.exp(-5 * (predicted_and_true_scores_df[\"heuristic_score\"] - 0.5)))\n",
    "\n",
    "# Fit a linear regression to further adjust predictions\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(predicted_and_true_scores_df[[\"transformed_predicted_score\"]])\n",
    "y = predicted_and_true_scores_df[\"true_score_normalized\"]\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = regressor.predict(X)\n",
    "\n",
    "# Ensure predictions are within [0,1]\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = predicted_and_true_scores_df[\"final_predicted_score\"].clip(0,1)\n",
    "\n",
    "# Scatter Plot (Updated)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"final_predicted_score\"], y=predicted_and_true_scores_df[\"true_score_normalized\"], alpha=0.6)\n",
    "plt.xlabel(\"Transformed Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Scatter Plot of Transformed Predicted vs True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram (Updated)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(predicted_and_true_scores_df[\"final_predicted_score\"], kde=True, color='blue', label='Final Predicted Scores', alpha=0.6)\n",
    "sns.histplot(predicted_and_true_scores_df[\"true_score_normalized\"], kde=True, color='red', label='Normalized True Scores', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Distribution of Transformed Predicted and True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Distribution\n",
    "residuals = predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"purple\", alpha=0.6)\n",
    "plt.axvline(0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis (Updated)\n",
    "spearman_corr, _ = spearmanr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "pearson_corr, _ = pearsonr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "kendall_corr, _ = kendalltau(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "\n",
    "print(f\"Updated Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Updated Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Updated Kendall's Tau: {kendall_corr:.3f}\")\n",
    "\n",
    "# Error Metrics (Updated)\n",
    "mse = ((predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"New MSE: {mse:.4f}\")\n",
    "print(f\"New MAE: {mae:.4f}\")\n",
    "print(f\"New RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cfb17b-692c-454b-8a20-b905fc28ec7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, count, split, regexp_extract\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, wilcoxon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract only the first two numeric values from feedback\n",
    "predicted_and_true_scores = jobs_with_code_questions.withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Convert to Pandas for visualization and analysis\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Normalize true scores (range [0,1])\n",
    "predicted_and_true_scores_df[\"true_score_normalized\"] = predicted_and_true_scores_df[\"true_score\"] / 10\n",
    "\n",
    "# Apply a logistic transformation to predicted scores\n",
    "predicted_and_true_scores_df[\"transformed_predicted_score\"] = 1 / (1 + np.exp(-5 * (predicted_and_true_scores_df[\"heuristic_score\"] - 0.5)))\n",
    "\n",
    "# Fit a linear regression to further adjust predictions\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(predicted_and_true_scores_df[[\"transformed_predicted_score\"]])\n",
    "y = predicted_and_true_scores_df[\"true_score_normalized\"]\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = regressor.predict(X)\n",
    "\n",
    "# Ensure predictions are within [0,1]\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = predicted_and_true_scores_df[\"final_predicted_score\"].clip(0,1)\n",
    "\n",
    "# Scatter Plot (Enhanced)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"final_predicted_score\"], y=predicted_and_true_scores_df[\"true_score_normalized\"], alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Perfect Agreement')\n",
    "plt.xlabel(\"Final Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Scatter Plot of Final Predicted vs True Scores\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2D Density Plot (Added)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.kdeplot(\n",
    "    x=predicted_and_true_scores_df[\"final_predicted_score\"], \n",
    "    y=predicted_and_true_scores_df[\"true_score_normalized\"], \n",
    "    fill=True, cmap=\"Blues\", alpha=0.7)\n",
    "plt.xlabel(\"Final Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Density Plot of Final Predicted vs True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Distribution\n",
    "residuals = predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"purple\", alpha=0.6)\n",
    "plt.axvline(0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Statistical Testing\n",
    "spearman_corr, _ = spearmanr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "pearson_corr, _ = pearsonr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "kendall_corr, _ = kendalltau(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "w_stat, w_pvalue = wilcoxon(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "\n",
    "# Error Metrics\n",
    "mse = ((predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Print Results\n",
    "print(f\"Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Kendall's Tau: {kendall_corr:.3f}\")\n",
    "print(f\"Wilcoxon Test Statistic: {w_stat:.3f}, p-value: {w_pvalue:.3f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ef94b53-e7fa-42db-bb65-a7af6a78671b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import numpy as np\n",
    "\n",
    "# Extract the true normalized scores for comparison\n",
    "true_scores = predicted_and_true_scores_df[\"true_score_normalized\"]\n",
    "\n",
    "# Baseline 1: Mean Prediction\n",
    "mean_score = true_scores.mean()\n",
    "mean_predictions = np.full_like(true_scores, mean_score)\n",
    "\n",
    "# Metrics for Mean Prediction\n",
    "spearman_mean, _ = spearmanr(mean_predictions, true_scores)\n",
    "pearson_mean, _ = pearsonr(mean_predictions, true_scores)\n",
    "mse_mean = mean_squared_error(true_scores, mean_predictions)\n",
    "mae_mean = mean_absolute_error(true_scores, mean_predictions)\n",
    "rmse_mean = np.sqrt(mse_mean)\n",
    "\n",
    "# Baseline 2: Random Prediction\n",
    "random_predictions = np.random.uniform(low=0.0, high=1.0, size=true_scores.shape)\n",
    "\n",
    "# Metrics for Random Prediction\n",
    "spearman_random, _ = spearmanr(random_predictions, true_scores)\n",
    "pearson_random, _ = pearsonr(random_predictions, true_scores)\n",
    "mse_random = mean_squared_error(true_scores, random_predictions)\n",
    "mae_random = mean_absolute_error(true_scores, random_predictions)\n",
    "rmse_random = np.sqrt(mse_random)\n",
    "\n",
    "# Print Results\n",
    "print(\"Baseline: Mean Prediction\")\n",
    "print(f\"Spearman Correlation: {spearman_mean:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_mean:.3f}\")\n",
    "print(f\"MSE: {mse_mean:.4f}\")\n",
    "print(f\"MAE: {mae_mean:.4f}\")\n",
    "print(f\"RMSE: {rmse_mean:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"Baseline: Random Prediction\")\n",
    "print(f\"Spearman Correlation: {spearman_random:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_random:.3f}\")\n",
    "print(f\"MSE: {mse_random:.4f}\")\n",
    "print(f\"MAE: {mae_random:.4f}\")\n",
    "print(f\"RMSE: {rmse_random:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c10528f-b221-4ff3-b6c3-b994aeee0fb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Baseline metrics\n",
    "baseline_metrics = {\n",
    "    \"Mean Prediction\": {\"MSE\": 0.0283, \"MAE\": 0.1209, \"RMSE\": 0.1681, \"Spearman\": None, \"Pearson\": None},\n",
    "    \"Random Prediction\": {\"MSE\": 0.1651, \"MAE\": 0.3377, \"RMSE\": 0.4063, \"Spearman\": -0.058, \"Pearson\": -0.061}\n",
    "}\n",
    "\n",
    "# Model's metrics\n",
    "model_metrics = {\n",
    "    \"MSE\": mse,\n",
    "    \"MAE\": mae,\n",
    "    \"RMSE\": rmse,\n",
    "    \"Spearman\": spearman_corr,\n",
    "    \"Pearson\": pearson_corr\n",
    "}\n",
    "\n",
    "# Combine and print metrics for comparison\n",
    "print(f\"{'Metric':<15} {'Model':<15} {'Mean Baseline':<15} {'Random Baseline':<15}\")\n",
    "for metric, model_value in model_metrics.items():\n",
    "    mean_baseline_value = baseline_metrics[\"Mean Prediction\"].get(metric)\n",
    "    random_baseline_value = baseline_metrics[\"Random Prediction\"].get(metric)\n",
    "    print(f\"{metric:<15} {model_value:<15.4f} {mean_baseline_value or 'N/A':<15} {random_baseline_value:<15.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff75ef88-2dd8-417c-9cf0-c43e8836702d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import pandas as pd\n",
    "from consts import GEMINI_SIMULATION_DATA_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"MatchingEvaluation\").getOrCreate()\n",
    "jobs_with_open_questions = open_csv_file(spark, GEMINI_SIMULATION_DATA_PATH, 'open_questions_feedback.csv') \\\n",
    "    .select('heuristic_score', 'feedback')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cee2361d-affd-4e11-9634-e54b79d909ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, count, split, regexp_extract\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, ttest_rel\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract only the first two numeric values from feedback\n",
    "predicted_and_true_scores = jobs_with_open_questions.withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Convert to Pandas for visualization and analysis\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Normalize true scores (range [0,1])\n",
    "predicted_and_true_scores_df[\"true_score_normalized\"] = predicted_and_true_scores_df[\"true_score\"] / 10\n",
    "\n",
    "# # Apply a logistic transformation to predicted scores\n",
    "# predicted_and_true_scores_df[\"transformed_predicted_score\"] = 1 / (1 + np.exp(-5 * (predicted_and_true_scores_df[\"heuristic_score\"] - 0.5)))\n",
    "\n",
    "# Fit a linear regression to further adjust predictions\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(predicted_and_true_scores_df[[\"heuristic_score\"]])\n",
    "y = predicted_and_true_scores_df[\"true_score_normalized\"]\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = regressor.predict(X)\n",
    "\n",
    "# Ensure predictions are within [0,1]\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = predicted_and_true_scores_df[\"final_predicted_score\"].clip(0,1)\n",
    "\n",
    "# Scatter Plot (Updated)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"final_predicted_score\"], y=predicted_and_true_scores_df[\"true_score_normalized\"], alpha=0.6)\n",
    "plt.xlabel(\"Transformed Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Scatter Plot of Transformed Predicted vs True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Histogram (Updated)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(predicted_and_true_scores_df[\"final_predicted_score\"], kde=True, color='blue', label='Final Predicted Scores', alpha=0.6)\n",
    "sns.histplot(predicted_and_true_scores_df[\"true_score_normalized\"], kde=True, color='red', label='Normalized True Scores', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlabel(\"Score\")\n",
    "plt.title(\"Distribution of Transformed Predicted and True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Distribution\n",
    "residuals = predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"purple\", alpha=0.6)\n",
    "plt.axvline(0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Analysis (Updated)\n",
    "spearman_corr, _ = spearmanr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "pearson_corr, _ = pearsonr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "kendall_corr, _ = kendalltau(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "\n",
    "print(f\"Updated Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Updated Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Updated Kendall's Tau: {kendall_corr:.3f}\")\n",
    "\n",
    "# Error Metrics (Updated)\n",
    "mse = ((predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "print(f\"New MSE: {mse:.4f}\")\n",
    "print(f\"New MAE: {mae:.4f}\")\n",
    "print(f\"New RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6c7ed04-fed5-4070-be48-726f940ee4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, stddev, count, split, regexp_extract\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr, kendalltau, wilcoxon\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Extract only the first two numeric values from feedback\n",
    "predicted_and_true_scores = jobs_with_code_questions.withColumn(\"relevance_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+)\", 1).cast(\"float\")) \\\n",
    "    .withColumn(\"suitability_score\", regexp_extract(col(\"feedback\"), \"(\\\\d+),\\\\s*(\\\\d+)\", 2).cast(\"float\")) \\\n",
    "    .withColumn(\"true_score\", (col(\"relevance_score\") + col(\"suitability_score\")) / 2)\n",
    "\n",
    "# Convert to Pandas for visualization and analysis\n",
    "predicted_and_true_scores_df = predicted_and_true_scores.toPandas()\n",
    "\n",
    "# Normalize true scores (range [0,1])\n",
    "predicted_and_true_scores_df[\"true_score_normalized\"] = predicted_and_true_scores_df[\"true_score\"] / 10\n",
    "\n",
    "# Apply a logistic transformation to predicted scores\n",
    "predicted_and_true_scores_df[\"transformed_predicted_score\"] = 1 / (1 + np.exp(-5 * (predicted_and_true_scores_df[\"heuristic_score\"] - 0.5)))\n",
    "\n",
    "# Fit a linear regression to further adjust predictions\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(predicted_and_true_scores_df[[\"transformed_predicted_score\"]])\n",
    "y = predicted_and_true_scores_df[\"true_score_normalized\"]\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = regressor.predict(X)\n",
    "\n",
    "# Ensure predictions are within [0,1]\n",
    "predicted_and_true_scores_df[\"final_predicted_score\"] = predicted_and_true_scores_df[\"final_predicted_score\"].clip(0,1)\n",
    "\n",
    "# Scatter Plot (Enhanced)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=predicted_and_true_scores_df[\"final_predicted_score\"], y=predicted_and_true_scores_df[\"true_score_normalized\"], alpha=0.6)\n",
    "plt.plot([0, 1], [0, 1], color='red', linestyle='--', label='Perfect Agreement')\n",
    "plt.xlabel(\"Final Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Scatter Plot of Final Predicted vs True Scores\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2D Density Plot (Added)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.kdeplot(\n",
    "    x=predicted_and_true_scores_df[\"final_predicted_score\"], \n",
    "    y=predicted_and_true_scores_df[\"true_score_normalized\"], \n",
    "    fill=True, cmap=\"Blues\", alpha=0.7)\n",
    "plt.xlabel(\"Final Predicted Score\")\n",
    "plt.ylabel(\"Normalized True Score\")\n",
    "plt.title(\"Density Plot of Final Predicted vs True Scores\")\n",
    "plt.show()\n",
    "\n",
    "# Residual Distribution\n",
    "residuals = predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(residuals, kde=True, bins=30, color=\"purple\", alpha=0.6)\n",
    "plt.axvline(0, color='red', linestyle='dashed')\n",
    "plt.xlabel(\"Residual (True - Predicted)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Residual Distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Statistical Testing\n",
    "spearman_corr, _ = spearmanr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "pearson_corr, _ = pearsonr(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "kendall_corr, _ = kendalltau(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "w_stat, w_pvalue = wilcoxon(predicted_and_true_scores_df[\"final_predicted_score\"], predicted_and_true_scores_df[\"true_score_normalized\"])\n",
    "\n",
    "# Error Metrics\n",
    "mse = ((predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]) ** 2).mean()\n",
    "mae = abs(predicted_and_true_scores_df[\"true_score_normalized\"] - predicted_and_true_scores_df[\"final_predicted_score\"]).mean()\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Print Results\n",
    "print(f\"Spearman Correlation: {spearman_corr:.3f}\")\n",
    "print(f\"Pearson Correlation: {pearson_corr:.3f}\")\n",
    "print(f\"Kendall's Tau: {kendall_corr:.3f}\")\n",
    "print(f\"Wilcoxon Test Statistic: {w_stat:.3f}, p-value: {w_pvalue:.3f}\")\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9b0a4b89-5afb-49d7-9a1e-99105852643d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "answer_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

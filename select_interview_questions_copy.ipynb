{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6629c4-2385-43b1-b94b-d4b8f046d49a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "from consts import JOBS_PATH, QUESTIONS_PATH, open_csv_file\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"JobQuestionMatching\").getOrCreate()\n",
    "\n",
    "# Load datasets into Spark DataFrames\n",
    "job_postings_df = open_csv_file(spark, JOBS_PATH, \"all_jobpostings_with_skills.csv\")\n",
    "job_postings_df = job_postings_df.fillna({'skills': ''})\n",
    "code_questions_df = open_csv_file(spark, QUESTIONS_PATH, \"all_code_questions_with_topics.csv\")\n",
    "open_questions_df = open_csv_file(spark, QUESTIONS_PATH, \"all_open_questions_with_topics.csv\")\n",
    "print(\"num_rows:\", job_postings_df.count(), code_questions_df.count(), open_questions_df.count())\n",
    "\n",
    "# Explode the skills and topics columns\n",
    "job_postings_exploded = job_postings_df.withColumn(\"skill\", explode(split(\"skills\", \",\")))\n",
    "code_questions_exploded = code_questions_df.withColumn(\"topic\", explode(split(\"topics\", \",\")))\n",
    "open_questions_exploded = open_questions_df.withColumn(\"topic\", explode(split(\"topics\", \",\")))\n",
    "\n",
    "# Cartesian product between job postings and questions\n",
    "cartesian_code = job_postings_exploded.crossJoin(code_questions_exploded)\n",
    "cartesian_open = job_postings_exploded.crossJoin(open_questions_exploded)\n",
    "print(\"num_rows:\", cartesian_code.count(), cartesian_open.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33bc528c-3638-437f-8ad3-9d425a4b04f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from consts import PROJECT_PATH\n",
    "\n",
    "display(cartesian_code)\n",
    "display(cartesian_open)\n",
    "\n",
    "checkpoints_path = os.path.join(PROJECT_PATH, \"tmp/spark-checkpoints/\")\n",
    "spark.sparkContext.setCheckpointDir(checkpoints_path)\n",
    "cartesian_code = cartesian_code.checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ab7ffb4-19bf-423c-91dc-da67a4f57f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4168f5e-68fa-408a-a586-50538153f820",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, udf, array\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the model globally\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# UDF to generate embeddings\n",
    "@udf(ArrayType(FloatType()))\n",
    "def generate_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "\n",
    "# UDF to calculate cosine similarity\n",
    "@udf(DoubleType())\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    return float(cosine_similarity([embedding1], [embedding2])[0][0])\n",
    "\n",
    "# Extract unique skills and topics\n",
    "unique_skills = job_postings_exploded.select(\"skill\").distinct()\n",
    "unique_topics = code_questions_exploded.select(\"topic\").distinct()\n",
    "\n",
    "# Cartesian product of unique skills and topics\n",
    "unique_pairs = unique_skills.crossJoin(unique_topics)\n",
    "\n",
    "# Generate embeddings for skills and topics\n",
    "unique_pairs = unique_pairs.withColumn(\"skill_embedding\", generate_embedding(col(\"skill\")))\n",
    "unique_pairs = unique_pairs.withColumn(\"topic_embedding\", generate_embedding(col(\"topic\")))\n",
    "\n",
    "# Compute similarity for unique pairs\n",
    "unique_pairs = unique_pairs.withColumn(\n",
    "    \"similarity\", calculate_similarity(col(\"skill_embedding\"), col(\"topic_embedding\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42375a8d-25d1-4166-b339-c76408d7f9f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_pairs.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0db18b2e-b911-4954-8621-b3f6f16a3003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "unique_pairs_pandas = unique_pairs.toPandas()\n",
    "unique_pairs_pandas.to_csv(\"unique_pairs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c932be62-2bff-4e34-bf6f-be15ea26791a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Join similarity back to Cartesian product\n",
    "cartesian_code_with_similarity = cartesian_code.join(\n",
    "    broadcast(unique_pairs.select(\"skill\", \"topic\", \"similarity\")),\n",
    "    on=[\"skill\", \"topic\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "cartesian_open_with_similarity = cartesian_open.join(\n",
    "    broadcast(unique_pairs.select(\"skill\", \"topic\", \"similarity\")),\n",
    "    on=[\"skill\", \"topic\"],\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "# Aggregate similarity scores for each job-question pair\n",
    "columns_to_group_by = [col for col in cartesian_code.columns if col not in [\"skill\", \"topic\"]]\n",
    "aggregated_code_scores = cartesian_code_with_similarity.groupBy(*columns_to_group_by).agg(\n",
    "    {\"similarity\": \"avg\"}\n",
    ")\n",
    "\n",
    "columns_to_group_by = [col for col in cartesian_open.columns if col not in [\"skill\", \"topic\"]]\n",
    "aggregated_open_scores = cartesian_open_with_similarity.groupBy(*columns_to_group_by).agg(\n",
    "    {\"similarity\": \"avg\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "128c2c73-3688-46b9-b83f-b8aceec5a470",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aggregated_code_scores.display()\n",
    "aggregated_open_scores.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08d47776-6808-44fe-a3ab-5b96bf329681",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from consts import DATA_PATH\n",
    "\n",
    "aggregated_code_scores_pandas = aggregated_code_scores.toPandas()\n",
    "aggregated_open_scores_pandas = aggregated_open_scores.toPandas()\n",
    "\n",
    "aggregated_code_scores_pandas.to_csv(os.path.join(DATA_PATH, \"code_questions_topis_skills_scores.csv\"), index=False)\n",
    "aggregated_open_scores_pandas.to_csv(os.path.join(DATA_PATH, \"open_questions_topis_skills_scores.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "select_interview_questions_copy",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

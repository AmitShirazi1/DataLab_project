{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394de329-f692-4305-bd5a-3b9699e37a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, StringType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import os\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InterviewQuestionSelector\").getOrCreate()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d44fb8d-1589-45c0-91aa-f6b4f938aa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets into Spark DataFrames\n",
    "job_postings = open_csv_file(spark, JOBS_PATH, 'all_jobpostings.csv')\n",
    "code_questions = open_csv_file(spark, QUESTIONS_PATH, 'all_code_problems_with_solutions.csv')\n",
    "open_questions = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions.csv')\n",
    "\n",
    "# Preprocessing function to handle missing values and ensure string type\n",
    "def preprocess_column_spark(df, column):\n",
    "    df = df.withColumn(column, col(column).cast(\"string\"))\n",
    "    df = df.fillna({column: \"\"})\n",
    "    return df\n",
    "\n",
    "# Preprocess columns in the datasets\n",
    "job_postings = preprocess_column_spark(job_postings, 'job_summary')\n",
    "code_questions = preprocess_column_spark(code_questions, 'topics')\n",
    "open_questions = preprocess_column_spark(open_questions, 'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc13445-76e1-4bcc-afc5-493325c68c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdff48ed-0e98-448a-874f-5e93b383e52d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import google.generativeai as genai\n",
    "\n",
    "# # Set the API key securely\n",
    "# os.environ['GOOGLE_API_KEY'] = 'AIzaSyDRhW3zsC_9C6JfevPqLb88QGWjy21Zf4c'\n",
    "\n",
    "# genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "\n",
    "# # List available models\n",
    "# for m in genai.list_models():\n",
    "#   if 'generateContent' in m.supported_generation_methods:\n",
    "#     print(m.name)\n",
    "\n",
    "# # Create model and generate content\n",
    "# model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "# response = model.generate_content(\"What is the meaning of life?\")\n",
    "# print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2d6fd1-63dd-4be6-a6c3-29e493bbb833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, concat_ws\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import ast\n",
    "\n",
    "# Define a UDF to safely parse the string to a list\n",
    "def parse_skills(skills_str):\n",
    "    try:\n",
    "        return ast.literal_eval(skills_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "parse_skills_udf = udf(parse_skills, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to create a proper list column\n",
    "job_postings = job_postings.withColumn(\"skills_list\", parse_skills_udf(\"skills\"))\n",
    "\n",
    "# Convert the skills list to a single string\n",
    "job_postings = job_postings.withColumn(\"skills_string\", concat_ws(\", \", \"skills_list\")) \\\n",
    "    .drop(\"skills\", \"skills_list\").withColumnRenamed(\"skills_string\", \"skills\")\n",
    "job_postings.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2be925-b4c9-416f-be09-2b0cc399a896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "from pyspark.sql.types import StringType\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "\n",
    "# Configure Gemini API\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyDRhW3zsC_9C6JfevPqLb88QGWjy21Zf4c'\n",
    "\n",
    "def init_genai():\n",
    "    \"\"\"Initialize the Gemini API client.\"\"\"\n",
    "    genai.configure(api_key='AIzaSyDRhW3zsC_9C6JfevPqLb88QGWjy21Zf4c')\n",
    "    return genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "def infer_skills_partition(rows):\n",
    "    print(\"Starting partition processing...\")\n",
    "    model = init_genai()\n",
    "    results = []\n",
    "    \n",
    "    for row in rows:\n",
    "        print(f\"Processing row: {row}\")  # Debug input row\n",
    "        job_summary = row.job_summary if row.job_summary else \"\"\n",
    "        if not job_summary.strip():\n",
    "            results.append((row.job_summary, row.skills))\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            prompt = f\"Extract a comma-separated list of technical, professional, and soft skills required for this job description: {job_summary}\"\n",
    "            response = model.generate_content(prompt)\n",
    "            # Ensure response content exists\n",
    "            extracted_skills = response.candidates[0].content.parts[0].text.strip()\n",
    "            print(f\"Extracted skills: {extracted_skills}\")  # Debug response\n",
    "            results.append((row.job_summary, extracted_skills))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row: {e}\")\n",
    "            results.append((row.job_summary, row.skills))  # Return existing skills on error\n",
    "    \n",
    "    return iter(results)\n",
    "\n",
    "\n",
    "# Apply to DataFrame\n",
    "job_postings_sample = job_postings.limit(10)\n",
    "job_postings_sample_rdd = job_postings_sample.rdd.mapPartitions(infer_skills_partition)\n",
    "job_postings_with_skills_sample = spark.createDataFrame(\n",
    "    job_postings_sample_rdd, schema=['job_summary', 'skills']\n",
    ")\n",
    "job_postings_with_skills_sample.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61677e9-008e-451e-8ed0-fdab1fa772ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_postings_with_skills.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e619a9cf-76bb-496b-ac3d-9720cf577a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.environ.get('GOOGLE_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b84699e-41ae-4efc-b148-db4bb728b3ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Generate embeddings and merge with the DataFrame\n",
    "def generate_embeddings(df, column, model):\n",
    "    # Collect the column data as a list\n",
    "    rows = df.select(column).rdd.map(lambda row: row[column]).collect()\n",
    "    \n",
    "    # Generate embeddings using the model\n",
    "    embeddings = model.encode(rows, batch_size=32, show_progress_bar=True)\n",
    "    \n",
    "    # Add embeddings as a new column in the DataFrame\n",
    "    embeddings_df = spark.createDataFrame(\n",
    "        [(row, embedding.tolist()) for row, embedding in zip(rows, embeddings)],\n",
    "        schema=StructType([\n",
    "            StructField(column, StringType(), True),\n",
    "            StructField(f\"{column}_embedding\", ArrayType(FloatType()), True)\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Join the embeddings DataFrame back with the original DataFrame\n",
    "    return df.join(embeddings_df, column)\n",
    "\n",
    "# Generate embeddings for job summaries, code questions, and open questions\n",
    "job_postings = generate_embeddings(job_postings, 'job_summary', model)\n",
    "job_postings = generate_embeddings(job_postings, 'skills', model)\n",
    "code_questions = generate_embeddings(code_questions, 'topics', model)\n",
    "open_questions = generate_embeddings(open_questions, 'question', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e65bd6-2137-4972-b88b-3702cca59a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(job_postings)\n",
    "display(code_questions)\n",
    "display(open_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6abaa5-9a67-4ef5-84e7-ae3947756ca3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Combine job embeddings (job_summary and skills) for topic relevance\n",
    "job_postings = job_postings.withColumn(\n",
    "    \"jobposting_embedding\",\n",
    "    col(\"job_summary_embedding\") + col(\"skills_embedding\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fafbe473-5181-4e79-9f42-170755a5059d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from scipy.spatial.distance import cosine\n",
    "from pyspark.sql.types import FloatType, ArrayType\n",
    "\n",
    "# UDF for cosine similarity\n",
    "cosine_similarity_udf = udf(\n",
    "    lambda e1, e2: 1 - cosine(e1, e2) if e1 and e2 else 0,  # Handle null cases\n",
    "    FloatType()\n",
    ")\n",
    "\n",
    "# Compute similarity-based weights\n",
    "job_postings = job_postings.withColumn(\n",
    "    \"attention_weight\",\n",
    "    cosine_similarity_udf(col(\"job_summary_embeddings\"), col(\"skills_embeddings\"))\n",
    ")\n",
    "\n",
    "# UDF for weighted combination using attention weights\n",
    "def attention_weighted_average(e1, e2, weight):\n",
    "    return [\n",
    "        (1 - weight) * e1_val + weight * e2_val\n",
    "        for e1_val, e2_val in zip(e1, e2)\n",
    "    ] if e1 and e2 else None  # Handle null cases\n",
    "\n",
    "attention_weighted_avg_udf = udf(attention_weighted_average, ArrayType(FloatType()))\n",
    "\n",
    "# Apply weighted averaging\n",
    "job_postings = job_postings.withColumn(\n",
    "    \"job_embedding\",\n",
    "    attention_weighted_avg_udf(\n",
    "        col(\"job_summary_embeddings\"),\n",
    "        col(\"skills_embeddings\"),\n",
    "        col(\"attention_weight\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop the intermediate attention_weight column if no longer needed\n",
    "job_postings = job_postings.drop(\"attention_weight\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf5d192-60ce-4109-85d7-6a44a53d7bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Each question will be initially considered for each job posting.\n",
    "jobs_with_code_questions = job_postings.crossJoin(code_questions)\n",
    "jobs_with_open_questions = job_postings.crossJoin(open_questions)\n",
    "\n",
    "# Map difficulty levels to numeric values\n",
    "difficulty_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "\n",
    "# Change difficulty column to numeric\n",
    "code_questions = code_questions.withColumn(\n",
    "    \"difficulty\", col(\"difficulty\").map(difficulty_map)\n",
    ")\n",
    "\n",
    "# Match question's difficulty to job posting's level\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"difficulty_match\",\n",
    "    1 - abs(col(\"difficulty\") - col(\"level\")) / 2\n",
    ")\n",
    "\n",
    "# Calculate cosine similarity between embeddings\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    if embedding1 is None or embedding2 is None:\n",
    "        return 0.0\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "similarity_udf = udf(calculate_similarity, FloatType())\n",
    "\n",
    "# Add topic similarity scores\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"emb_similarity\",\n",
    "    similarity_udf(col(\"jobposting_embedding\", \"topics_embedding\"))\n",
    ")\n",
    "jobs_with_open_questions = jobs_with_open_questions.withColumn(\n",
    "    \"emb_similarity\",\n",
    "    similarity_udf(col(\"jobposting_embedding\", \"question_embedding\"))\n",
    ")\n",
    "\n",
    "# Normalize Acceptance for code questions\n",
    "max_acceptance = code_questions.agg({\"acceptance\": \"max\"}).collect()[0][0]\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"normalized_acceptance\", col(\"acceptance\") / max_acceptance\n",
    ")\n",
    "\n",
    "# Calculate Heuristic Score\n",
    "def calculate_score(difficulty, similarity, acceptance):\n",
    "    return 0.3 * difficulty + 0.5 * similarity + 0.2 * acceptance\n",
    "calculate_score_udf = udf(calculate_score, FloatType())\n",
    "\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"heuristic_score\",\n",
    "    calculate_score_udf(\n",
    "        col(\"difficulty_match\"),\n",
    "        col(\"emb_similarity\"),\n",
    "        col(\"normalized_acceptance\"),\n",
    "    ),\n",
    ")\n",
    "display(jobs_with_code_questions)\n",
    "\n",
    "jobs_with_open_questions = jobs_with_open_questions.withColumn(\n",
    "    \"heuristic_score\",\n",
    "    calculate_score_udf(\n",
    "        col(\"difficulty_match\"),  # This is 0 for open questions\n",
    "        col(\"emb_similarity\"),\n",
    "        lit(0)  # No acceptance column in open questions\n",
    "    ),\n",
    ")\n",
    "display(jobs_with_open_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5413a107-3774-46df-9ee5-cc1586e9ab28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select Top Questions for Each Job\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"job_id\").orderBy(col(\"heuristic_score\").desc())\n",
    "\n",
    "top_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 10)\n",
    "\n",
    "top_open_questions = jobs_with_open_questions.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 10)\n",
    "\n",
    "display(top_code_questions)\n",
    "display(top_open_questions)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "select_interview_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

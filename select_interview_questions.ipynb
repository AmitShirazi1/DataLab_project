{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394de329-f692-4305-bd5a-3b9699e37a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, StringType\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import os\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InterviewQuestionSelector\").getOrCreate()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d44fb8d-1589-45c0-91aa-f6b4f938aa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets into Spark DataFrames\n",
    "job_postings = open_csv_file(spark, JOBS_PATH, 'all_jobpostings.csv')\n",
    "code_questions = open_csv_file(spark, QUESTIONS_PATH, 'all_code_problems_with_solutions.csv')\n",
    "open_questions = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions.csv')\n",
    "\n",
    "# Preprocessing function to handle missing values and ensure string type\n",
    "def preprocess_column_spark(df, column):\n",
    "    df = df.withColumn(column, col(column).cast(\"string\"))\n",
    "    df = df.fillna({column: \"\"})\n",
    "    return df\n",
    "\n",
    "# Preprocess columns in the datasets\n",
    "job_postings = preprocess_column_spark(job_postings, 'job_summary')\n",
    "code_questions = preprocess_column_spark(code_questions, 'topics')\n",
    "open_questions = preprocess_column_spark(open_questions, 'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc13445-76e1-4bcc-afc5-493325c68c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf8e5b7-73d4-4954-b0bb-3e71f267e372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Job postings: filling in missing skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2d6fd1-63dd-4be6-a6c3-29e493bbb833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, concat_ws\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import ast\n",
    "\n",
    "# Define a UDF to safely parse the string to a list\n",
    "def parse_skills(skills_str):\n",
    "    try:\n",
    "        return ast.literal_eval(skills_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "parse_skills_udf = udf(parse_skills, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to create a proper list column\n",
    "job_postings = job_postings.withColumn(\"skills_list\", parse_skills_udf(\"skills\"))\n",
    "\n",
    "# Convert the skills list to a single string\n",
    "job_postings = job_postings.withColumn(\"skills_string\", concat_ws(\", \", \"skills_list\")) \\\n",
    "    .drop(\"skills\", \"skills_list\").withColumnRenamed(\"skills_string\", \"skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca912ba-e9dc-437b-b98f-8dda92e45c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dataset to pandas\n",
    "job_postings_pandas = job_postings.toPandas()\n",
    "empty_skills_count = job_postings_pandas[job_postings_pandas['skills'] == ''].shape[0]\n",
    "print(empty_skills_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2be925-b4c9-416f-be09-2b0cc399a896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "def infer_skills(job_summary):\n",
    "    \"\"\"\n",
    "    Extracts skills from a job description using the Gemini model.\n",
    "\n",
    "    Args:\n",
    "        job_summary: The job description text.\n",
    "\n",
    "    Returns:\n",
    "        A comma-separated string of skills extracted from the job description.\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(job_summary) or str(job_summary).strip() == '':\n",
    "        return ''\n",
    "\n",
    "    prompt = f\"Infer a comma-separated list of skills required for the following job description:\\n{job_summary}\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        return ''\n",
    "    \n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "empty_skills_rows = job_postings_pandas[(job_postings_pandas['skills'] == '') & (job_postings_pandas['job_summary'].str.strip() != '')]\n",
    "\n",
    "while (running_time < 3600) and (empty_skills_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"skills\" column is empty\n",
    "        empty_skills_rows = job_postings_pandas[(job_postings_pandas['skills'] == '') & (job_postings_pandas['job_summary'].str.strip() != '')]\n",
    "        if empty_skills_rows.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"skills\"\n",
    "        indices_to_update = empty_skills_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        job_postings_pandas.loc[indices_to_update, 'skills'] = (\n",
    "            job_postings_pandas.loc[indices_to_update, 'job_summary']\n",
    "                .apply(infer_skills)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755bb41f-5557-4f25-b216-8fc716df3036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_postings_pandas['skills'] = job_postings_pandas['skills'].fillna('')\n",
    "empty_skills_count = job_postings_pandas[job_postings_pandas['skills'] == ''].shape[0]\n",
    "job_postings_pandas['job_summary'] = job_postings_pandas['job_summary'].fillna('')\n",
    "empty_job_summaries = job_postings_pandas[job_postings_pandas['job_summary'].str.strip() == ''].shape[0]\n",
    "print(\"empty strings:\", empty_skills_count)\n",
    "print(\"empty job summaries:\", empty_job_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61677e9-008e-451e-8ed0-fdab1fa772ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import JOBS_PATH\n",
    "import os\n",
    "\n",
    "job_postings_with_skills = spark.createDataFrame(job_postings_pandas)\n",
    "job_postings_pandas.to_csv(os.path.join(JOBS_PATH, 'all_jobpostings_with_skills.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e11bee5d-82b6-424f-8737-c74d0dcf6ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Code questions: filling in missing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe35cd7-4c71-4f9d-8a6c-d8797b288514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, concat_ws, split, col, expr\n",
    "\n",
    "# Format the topics column to be a string containing comma-separated topics.\n",
    "code_questions = code_questions.withColumn(\"topics_array\", split(col(\"topics\"), \", \")) \\\n",
    "    .withColumn(\"topics_array_cleaned\", expr(\"transform(topics_array, x -> regexp_replace(x, \\\"'\\\", \\\"\\\"))\")) \\\n",
    "    .withColumn(\"topics_formatted\", concat_ws(\", \", col(\"topics_array_cleaned\"))) \\\n",
    "    .drop(\"topics_array\", \"topics_array_cleaned\", \"topics\").withColumnRenamed(\"topics_formatted\", \"topics\")\n",
    "\n",
    "code_questions_pandas = code_questions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48438f18-9fd5-4e2c-9cf9-7961b837135b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "def extract_topics_from_question(question):\n",
    "   if pd.isna(question) or question.strip() == '':\n",
    "       return ''\n",
    "   \n",
    "   prompt = f\"Analyze the following question and identify the specific skills being tested or evaluated. Return the skills as a comma-separated list of skills. If the question does not test any skills, return an empty string. Question: {question}\"\n",
    "   \n",
    "   try:\n",
    "       response = model.generate_content(prompt)\n",
    "       skills = response.text.strip()\n",
    "       return skills if skills else ''\n",
    "   except Exception as e:\n",
    "       return ''\n",
    "   \n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "empty_topics_rows = code_questions_pandas[(code_questions_pandas['topics'] == '') & (code_questions_pandas['question'].str.strip() != '')]\n",
    "\n",
    "while (running_time < 900) and (empty_topics_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"topics\" column is empty\n",
    "        empty_topics_rows = code_questions_pandas[(code_questions_pandas['topics'] == '') & (code_questions_pandas['question'].str.strip() != '')]\n",
    "        if empty_topics_rows.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"topics\"\n",
    "        indices_to_update = empty_topics_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        code_questions_pandas.loc[indices_to_update, 'topics'] = (\n",
    "            code_questions_pandas.loc[indices_to_update, 'question']\n",
    "                .apply(extract_topics_from_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cb0e2e-8da8-43db-8407-531143935770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty_topics_count = code_questions_pandas[code_questions_pandas['topics'].isna()].shape[0]\n",
    "print(\"nulls:\", empty_topics_count)\n",
    "code_questions_pandas['topics'] = code_questions_pandas['topics'].fillna('')\n",
    "empty_topics_count = code_questions_pandas[code_questions_pandas['topics'] == ''].shape[0]\n",
    "code_questions_pandas['question'] = code_questions_pandas['question'].fillna('')\n",
    "empty_questions = code_questions_pandas[code_questions_pandas['question'].str.strip() == ''].shape[0]\n",
    "print(\"empty strings:\", empty_topics_count)\n",
    "print(\"empty questions:\", empty_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376651f-53ec-42dc-ac16-71e23cefb3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import QUESTIONS_PATH\n",
    "import os\n",
    "\n",
    "code_questions_with_topics = spark.createDataFrame(code_questions_pandas)\n",
    "code_questions_pandas.to_csv(os.path.join(QUESTIONS_PATH, 'all_code_questions_with_topics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8085bc-a827-4b4a-bd7c-14c6b2a86f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_questions_with_topics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be977699-545b-46f4-9770-5846f8884a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Open questions: filling in missing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce33961-7366-49b8-ad1c-aaca6187fef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dataset to pandas\n",
    "open_questions_pandas = open_questions.toPandas()\n",
    "open_questions_pandas['topics'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51902921-91e4-4b22-91f3-8e9518526738",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wait 1 minute before executing this code"
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from api_keys import API_KEYS\n",
    "   \n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "empty_topics_rows = open_questions_pandas[(open_questions_pandas['topics'] == '') & (open_questions_pandas['question'].str.strip() != '')]\n",
    "\n",
    "while (running_time < 900) and (empty_topics_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        empty_topics_rows = open_questions_pandas[(open_questions_pandas['topics'] == '') & (open_questions_pandas['question'].str.strip() != '')]\n",
    "        if empty_topics_rows.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"topics\"\n",
    "        indices_to_update = empty_topics_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        open_questions_pandas.loc[indices_to_update, 'topics'] = (\n",
    "            open_questions_pandas.loc[indices_to_update, 'question']\n",
    "                .apply(extract_topics_from_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ed7f94-9a5b-4ce1-b31d-bda46b441b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import QUESTIONS_PATH\n",
    "import os\n",
    "\n",
    "open_questions_with_topics = spark.createDataFrame(open_questions_pandas)\n",
    "open_questions_pandas.to_csv(os.path.join(QUESTIONS_PATH, 'all_open_questions_with_topics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987c6eb2-9188-4593-a66b-8d33a469e0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "open_questions_with_topics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fca2da7-7d30-457f-a931-0e343f47e921",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Demonstrating the model on 50 random jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ebb6881-50f5-4a5b-8832-e993c5e9781e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from consts import JOBS_PATH, DATA_PATH\n",
    "\n",
    "# Load the CSV file\n",
    "jobs_data = pd.read_csv(os.path.join(JOBS_PATH, \"all_jobpostings_with_skills.csv\"))\n",
    "\n",
    "# Perform train-test split (e.g., 80-20 split)\n",
    "jobs_sample = jobs_data.sample(n=50, random_state=42)\n",
    "\n",
    "# Save the train and test sets to separate files\n",
    "jobs_sample.to_csv(os.path.join(JOBS_PATH, \"jobs_sample.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0aaeee6-2bdf-4bba-8fe9-33572e9beb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Topics & skills embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9407d24-c9ed-42e0-a418-5a846c6b99f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8dbde8e-7863-48d2-ada6-34f11c497249",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col, lit, udf, array, broadcast\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from calculate_heuristic_score import calculate_score\n",
    "from consts import DATA_PATH, QUESTIONS_PATH, MID_CALC_PATH, open_csv_file\n",
    "\n",
    "jobs_sample = open_csv_file(spark, JOBS_PATH, \"jobs_sample.csv\")\n",
    "code_questions = open_csv_file(spark, QUESTIONS_PATH, \"all_code_questions_with_topics.csv\")\n",
    "open_questions = open_csv_file(spark, QUESTIONS_PATH, \"all_open_questions_with_topics.csv\")\n",
    "\n",
    "code_questions_exploded = code_questions.withColumn(\"topic\", explode(split(\"topics\", \",\")))\n",
    "open_questions_exploded = open_questions.withColumn(\"topic\", explode(split(\"topics\", \",\")))\n",
    "\n",
    "# Load the model globally\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# UDF to generate embeddings\n",
    "@udf(ArrayType(FloatType()))\n",
    "def generate_embedding(text):\n",
    "    return model.encode(text).tolist()\n",
    "unique_topics_code = code_questions_exploded.select(\"topic\").distinct()\n",
    "unique_topics_open = open_questions_exploded.select(\"topic\").distinct()\n",
    "unique_topics_code = unique_topics_code.withColumn(\"topic_embedding\", generate_embedding(col(\"topic\")))\n",
    "unique_topics_open = unique_topics_open.withColumn(\"topic_embedding\", generate_embedding(col(\"topic\")))\n",
    "\n",
    "code_questions_exploded_df = code_questions_exploded.toPandas()\n",
    "open_questions_exploded_df = open_questions_exploded.toPandas()\n",
    "unique_topics_code_df = unique_topics_code.toPandas()\n",
    "unique_topics_open_df = unique_topics_open.toPandas()\n",
    "\n",
    "os.makedirs(MID_CALC_PATH, exist_ok=True)\n",
    "code_questions_exploded_df.to_csv(os.path.join(MID_CALC_PATH, \"code_questions_exploded.csv\"), index=False)\n",
    "open_questions_exploded_df.to_csv(os.path.join(MID_CALC_PATH, \"open_questions_exploded.csv\"), index=False)\n",
    "unique_topics_code_df.to_csv(os.path.join(MID_CALC_PATH, \"unique_topics_code.csv\"), index=False)\n",
    "unique_topics_open_df.to_csv(os.path.join(MID_CALC_PATH, \"unique_topics_open.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48fd7542-b0b8-4e3e-a9f8-108c7dd6a25b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import rand\n",
    "from calculate_heuristic_score import calculate_score\n",
    "\n",
    "jobs_sample = open_csv_file(spark, JOBS_PATH, \"jobs_sample.csv\")\n",
    "code_scores, open_scores = calculate_score(jobs_sample, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7bd7b53c-8742-4037-b707-4c68da8f5465",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_scores.select(\"topics\", \"skills\", \"similarity\").limit(70).display()\n",
    "open_scores.select(\"topics\", \"skills\", \"similarity\").limit(70).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b3b94f5-cd98-44d3-a296-60066b9d62ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import MID_CALC_PATH\n",
    "\n",
    "code_scores_df = code_scores.select(\"similarity\").toPandas()\n",
    "code_scores_df.to_csv(os.path.join(MID_CALC_PATH, \"code_questions_similarity.csv\"), index=False)\n",
    "open_scores_df = open_scores.select(\"similarity\").toPandas()\n",
    "open_scores_df.to_csv(os.path.join(MID_CALC_PATH, \"open_questions_similarity.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "487bd303-5b39-42d2-801a-c45536bdfb8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, when, expr\n",
    "\n",
    "\"\"\" Distributing the scores, so they'd be further apart \"\"\"\n",
    "\n",
    "# Define the transformation function\n",
    "def apply_root_transform(df, col_name):\n",
    "    return df.withColumn(\n",
    "        col_name,\n",
    "        when(col(col_name) >= 0, col(col_name) ** 0.5)  # Apply x^0.5 for positive values\n",
    "        .otherwise(-(-col(col_name)) ** 0.5)           # Apply -(-x)^0.5 for negative values\n",
    "    )\n",
    "\n",
    "# Apply the transformation on the \"similarity\" column for both datasets\n",
    "code_scores_after_transformation = apply_root_transform(code_scores, \"similarity\")\n",
    "open_scores_after_transformation = apply_root_transform(open_scores, \"similarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b65ed870-3708-4ea3-9d4b-85df0a7f923f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Show transformed datasets (optional)\n",
    "code_scores_after_transformation.select(\"topics\", \"skills\", \"similarity\").limit(70).display()\n",
    "open_scores_after_transformation.select(\"topics\", \"skills\", \"similarity\").limit(70).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edcd0e78-580d-4add-88a4-a47f947673ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_scores_after_transformation_df = code_scores_after_transformation.select(\"similarity\").toPandas()\n",
    "code_scores_after_transformation_df.to_csv(os.path.join(MID_CALC_PATH, \"code_questions_transformed_similarity.csv\"), index=False)\n",
    "open_scores_after_transformation_df = open_scores_after_transformation.select(\"similarity\").toPandas()\n",
    "open_scores_after_transformation_df.to_csv(os.path.join(MID_CALC_PATH, \"open_questions_transformed_similarity.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330b82b4-bb7f-4610-b8ab-ce2220d40899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Heuristic to match questions to jobs\n",
    "Questions with the highest hueristic grades wil be the most likely to appear in the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "caf5d192-60ce-4109-85d7-6a44a53d7bc6",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1738113499083}",
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, udf, abs, coalesce, when\n",
    "from pyspark.sql.types import FloatType\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Map difficulty levels to numeric values\n",
    "difficulty_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "\n",
    "# Change difficulty column to numeric\n",
    "code_with_heuristic = code_scores_after_transformation.withColumn(\n",
    "    \"difficulty\",\n",
    "    when(col(\"difficulty\") == \"Easy\", difficulty_map[\"Easy\"])\n",
    "    .when(col(\"difficulty\") == \"Medium\", difficulty_map[\"Medium\"])\n",
    "    .when(col(\"difficulty\") == \"Hard\", difficulty_map[\"Hard\"])\n",
    ").cache()\n",
    "\n",
    "# Match question's difficulty to job posting's level\n",
    "code_with_heuristic = code_with_heuristic.withColumn(\n",
    "    \"difficulty_match\",\n",
    "    1 - abs(col(\"difficulty\") - col(\"level\")) / 2) \\\n",
    "    .withColumn(\"difficulty_match\", coalesce(col(\"difficulty_match\"), lit(0.5))) \\\n",
    "    .drop(\"difficulty\").cache()\n",
    "    \n",
    "open_with_heuristic = open_scores_after_transformation.withColumn(\n",
    "    \"difficulty_match\",\n",
    "    lit(0.5)\n",
    ").drop(\"difficulty\").cache()\n",
    "\n",
    "# Normalize Acceptance for code questions\n",
    "max_acceptance = code_with_heuristic.agg({\"acceptance\": \"max\"}).collect()[0][0]\n",
    "code_with_heuristic = code_with_heuristic.withColumn(\n",
    "    \"normalized_acceptance\", col(\"acceptance\") / max_acceptance\n",
    ").drop(\"acceptance\").cache()\n",
    "open_with_heuristic = open_with_heuristic.withColumn(\"normalized_acceptance\", lit(0.5)).cache()\n",
    "\n",
    "# Calculate Heuristic Score\n",
    "def calculate_score(difficulty, similarity, acceptance):\n",
    "    return 0.3 * difficulty + 0.5 * similarity + 0.2 * acceptance\n",
    "calculate_score_udf = udf(calculate_score, FloatType())\n",
    "\n",
    "mean_difficulty_match = code_with_heuristic.agg({\"difficulty_match\": \"mean\"}).collect()[0][0]\n",
    "mean_similarity = open_with_heuristic.agg({\"similarity\": \"mean\"}).collect()[0][0]\n",
    "mean_acceptance = code_with_heuristic.agg({\"normalized_acceptance\": \"mean\"}).collect()[0][0]\n",
    "\n",
    "code_with_heuristic = code_with_heuristic.withColumn(\"difficulty_match\", coalesce(col(\"difficulty_match\"), lit(mean_difficulty_match))) \\\n",
    "    .withColumn(\"similarity\", coalesce(col(\"similarity\"), lit(mean_similarity))) \\\n",
    "    .withColumn(\"normalized_acceptance\", coalesce(col(\"normalized_acceptance\"), lit(mean_acceptance))) \\\n",
    "    .withColumn(\"heuristic_score\",\n",
    "    calculate_score_udf(\n",
    "        col(\"difficulty_match\"),\n",
    "        col(\"similarity\"),\n",
    "        col(\"normalized_acceptance\"),\n",
    "    ),\n",
    ").drop(\"difficulty_match\", \"similarity\", \"normalized_acceptance\").cache()\n",
    "display(code_with_heuristic.head(70))\n",
    "\n",
    "mean_similarity = open_with_heuristic.agg({\"similarity\": \"mean\"}).collect()[0][0]\n",
    "open_with_heuristic = open_with_heuristic.withColumn(\"similarity\", coalesce(col(\"similarity\"), lit(mean_similarity))) \\\n",
    "    .withColumn(\"heuristic_score\",\n",
    "    calculate_score_udf(\n",
    "        col(\"difficulty_match\"),\n",
    "        col(\"similarity\"),\n",
    "        col(\"normalized_acceptance\"),\n",
    "    ),\n",
    ").drop(\"difficulty_match\", \"similarity\", \"normalized_acceptance\").cache()\n",
    "display(open_with_heuristic.head(70))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5413a107-3774-46df-9ee5-cc1586e9ab28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select Top Questions for Each Job\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "import pandas as pd\n",
    "\n",
    "job_cols = pd.read_csv(os.path.join(JOBS_PATH, \"jobs_sample.csv\"), header=0).columns\n",
    "window_spec = Window.partitionBy(*job_cols).orderBy(col(\"heuristic_score\").desc())\n",
    "\n",
    "top_code_questions = code_with_heuristic.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 20)\n",
    "\n",
    "top_open_questions = open_with_heuristic.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d0ab9a95-27bb-471d-bb20-9ad9faa8596b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import DATA_PATH\n",
    "top_code_questions_df = top_code_questions.toPandas()\n",
    "top_code_questions_df.to_csv(os.path.join(DATA_PATH, \"top_code_questions.csv\"), index=False)\n",
    "top_open_questions_df = top_open_questions.toPandas()\n",
    "top_open_questions_df.to_csv(os.path.join(DATA_PATH, \"top_open_questions.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "select_interview_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

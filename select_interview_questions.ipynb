{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "394de329-f692-4305-bd5a-3b9699e37a23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, StructType, StructField, StringType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "import os\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"InterviewQuestionSelector\").getOrCreate()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d44fb8d-1589-45c0-91aa-f6b4f938aa76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load datasets into Spark DataFrames\n",
    "job_postings = open_csv_file(spark, JOBS_PATH, 'all_jobpostings.csv')\n",
    "code_questions = open_csv_file(spark, QUESTIONS_PATH, 'all_code_problems_with_solutions.csv')\n",
    "open_questions = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions.csv')\n",
    "\n",
    "# Preprocessing function to handle missing values and ensure string type\n",
    "def preprocess_column_spark(df, column):\n",
    "    df = df.withColumn(column, col(column).cast(\"string\"))\n",
    "    df = df.fillna({column: \"\"})\n",
    "    return df\n",
    "\n",
    "# Preprocess columns in the datasets\n",
    "job_postings = preprocess_column_spark(job_postings, 'job_summary')\n",
    "code_questions = preprocess_column_spark(code_questions, 'topics')\n",
    "open_questions = preprocess_column_spark(open_questions, 'question')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfc13445-76e1-4bcc-afc5-493325c68c2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abf8e5b7-73d4-4954-b0bb-3e71f267e372",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Job postings: filling in missing skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f2d6fd1-63dd-4be6-a6c3-29e493bbb833",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, concat_ws\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "import ast\n",
    "\n",
    "# Define a UDF to safely parse the string to a list\n",
    "def parse_skills(skills_str):\n",
    "    try:\n",
    "        return ast.literal_eval(skills_str)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "parse_skills_udf = udf(parse_skills, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to create a proper list column\n",
    "job_postings = job_postings.withColumn(\"skills_list\", parse_skills_udf(\"skills\"))\n",
    "\n",
    "# Convert the skills list to a single string\n",
    "job_postings = job_postings.withColumn(\"skills_string\", concat_ws(\", \", \"skills_list\")) \\\n",
    "    .drop(\"skills\", \"skills_list\").withColumnRenamed(\"skills_string\", \"skills\")\n",
    "job_postings.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bca912ba-e9dc-437b-b98f-8dda92e45c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dataset to pandas\n",
    "job_postings_pandas = job_postings.toPandas()\n",
    "empty_skills_count = job_postings_pandas[job_postings_pandas['skills'] == ''].shape[0]\n",
    "print(empty_skills_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab2be925-b4c9-416f-be09-2b0cc399a896",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "def infer_skills(job_summary):\n",
    "    \"\"\"\n",
    "    Extracts skills from a job description using the Gemini model.\n",
    "\n",
    "    Args:\n",
    "        job_summary: The job description text.\n",
    "\n",
    "    Returns:\n",
    "        A comma-separated string of skills extracted from the job description.\n",
    "    \"\"\"\n",
    "\n",
    "    if pd.isna(job_summary) or str(job_summary).strip() == '':\n",
    "        return ''\n",
    "\n",
    "    prompt = f\"Infer a comma-separated list of skills required for the following job description:\\n{job_summary}\"\n",
    "\n",
    "    try:\n",
    "        response = model.generate_content(prompt)\n",
    "        print(\"response:\", response.text)\n",
    "        return response.text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\"Error inferring skills: {e}\")\n",
    "        return ''\n",
    "    \n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "empty_skills_rows = job_postings_pandas[(job_postings_pandas['skills'] == '') & (job_postings_pandas['job_summary'].str.strip() != '')]\n",
    "\n",
    "while (running_time < 3600) and (empty_skills_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"skills\" column is empty\n",
    "        empty_skills_rows = job_postings_pandas[(job_postings_pandas['skills'] == '') & (job_postings_pandas['job_summary'].str.strip() != '')]\n",
    "        if empty_skills_rows.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"skills\"\n",
    "        indices_to_update = empty_skills_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        job_postings_pandas.loc[indices_to_update, 'skills'] = (\n",
    "            job_postings_pandas.loc[indices_to_update, 'job_summary']\n",
    "                .apply(infer_skills)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "755bb41f-5557-4f25-b216-8fc716df3036",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "job_postings_pandas['skills'] = job_postings_pandas['skills'].fillna('')\n",
    "empty_skills_count = job_postings_pandas[job_postings_pandas['skills'] == ''].shape[0]\n",
    "job_postings_pandas['job_summary'] = job_postings_pandas['job_summary'].fillna('')\n",
    "empty_job_summaries = job_postings_pandas[job_postings_pandas['job_summary'].str.strip() == ''].shape[0]\n",
    "print(\"empty strings:\", empty_skills_count)\n",
    "print(\"empty job summaries:\", empty_job_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a61677e9-008e-451e-8ed0-fdab1fa772ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import JOBS_PATH\n",
    "import os\n",
    "\n",
    "job_postings_with_skills = spark.createDataFrame(job_postings_pandas)\n",
    "job_postings_pandas.to_csv(os.path.join(JOBS_PATH, 'all_jobpostings_with_skills.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e11bee5d-82b6-424f-8737-c74d0dcf6ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Code questions: filling in missing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ebe35cd7-4c71-4f9d-8a6c-d8797b288514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, concat_ws, split, col, expr\n",
    "\n",
    "# Format the topics column to be a string containing comma-separated topics.\n",
    "code_questions = code_questions.withColumn(\"topics_array\", split(col(\"topics\"), \", \")) \\\n",
    "    .withColumn(\"topics_array_cleaned\", expr(\"transform(topics_array, x -> regexp_replace(x, \\\"'\\\", \\\"\\\"))\")) \\\n",
    "    .withColumn(\"topics_formatted\", concat_ws(\", \", col(\"topics_array_cleaned\"))) \\\n",
    "    .drop(\"topics_array\", \"topics_array_cleaned\", \"topics\").withColumnRenamed(\"topics_formatted\", \"topics\")\n",
    "\n",
    "code_questions_pandas = code_questions.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48438f18-9fd5-4e2c-9cf9-7961b837135b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "import time\n",
    "from api_keys import API_KEYS\n",
    "\n",
    "def extract_topics_from_question(question):\n",
    "   if pd.isna(question) or question.strip() == '':\n",
    "       return ''\n",
    "   \n",
    "   prompt = f\"Analyze the following question and identify the specific skills being tested or evaluated. Return the skills as a comma-separated list of skills. If the question does not test any skills, return an empty string. Question: {question}\"\n",
    "   \n",
    "   try:\n",
    "       response = model.generate_content(prompt)\n",
    "       skills = response.text.strip()\n",
    "       print(\"response:\", skills)\n",
    "       return skills if skills else ''\n",
    "   except Exception as e:\n",
    "       print(f\"Error extracting topics: {e}\")\n",
    "       return ''\n",
    "   \n",
    "\n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "empty_topics_rows = code_questions_pandas[(code_questions_pandas['topics'] == '') & (code_questions_pandas['question'].str.strip() != '')]\n",
    "\n",
    "while (running_time < 900) and (empty_topics_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        # Filter the rows where the \"topics\" column is empty\n",
    "        empty_topics_rows = code_questions_pandas[(code_questions_pandas['topics'] == '') & (code_questions_pandas['question'].str.strip() != '')]\n",
    "        if empty_topics_rows.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"topics\"\n",
    "        indices_to_update = empty_topics_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        code_questions_pandas.loc[indices_to_update, 'topics'] = (\n",
    "            code_questions_pandas.loc[indices_to_update, 'question']\n",
    "                .apply(extract_topics_from_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04cb0e2e-8da8-43db-8407-531143935770",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "empty_topics_count = code_questions_pandas[code_questions_pandas['topics'].isna()].shape[0]\n",
    "print(\"nulls:\", empty_topics_count)\n",
    "code_questions_pandas['topics'] = code_questions_pandas['topics'].fillna('')\n",
    "empty_topics_count = code_questions_pandas[code_questions_pandas['topics'] == ''].shape[0]\n",
    "code_questions_pandas['question'] = code_questions_pandas['question'].fillna('')\n",
    "empty_questions = code_questions_pandas[code_questions_pandas['question'].str.strip() == ''].shape[0]\n",
    "print(\"empty strings:\", empty_topics_count)\n",
    "print(\"empty questions:\", empty_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d376651f-53ec-42dc-ac16-71e23cefb3da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import QUESTIONS_PATH\n",
    "import os\n",
    "\n",
    "code_questions_with_topics = spark.createDataFrame(code_questions_pandas)\n",
    "code_questions_pandas.to_csv(os.path.join(QUESTIONS_PATH, 'all_code_questions_with_topics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8085bc-a827-4b4a-bd7c-14c6b2a86f1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "code_questions_with_topics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be977699-545b-46f4-9770-5846f8884a40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Open questions: filling in missing topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cce33961-7366-49b8-ad1c-aaca6187fef3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert dataset to pandas\n",
    "open_questions_pandas = open_questions.toPandas()\n",
    "open_questions_pandas['topics'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51902921-91e4-4b22-91f3-8e9518526738",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Wait 1 minute before executing this code"
    }
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "import os\n",
    "from api_keys import API_KEYS\n",
    "   \n",
    "start_time = time.time()\n",
    "running_time = 0\n",
    "empty_topics_rows = open_questions_pandas[(open_questions_pandas['topics'] == '') & (open_questions_pandas['question'].str.strip() != '')]\n",
    "\n",
    "while (running_time < 900) and (empty_topics_rows.shape[0] > 0):\n",
    "    for api_key in API_KEYS.values():\n",
    "        # Configure Gemini API\n",
    "        os.environ['GOOGLE_API_KEY'] = api_key\n",
    "        genai.configure(api_key=os.environ['GOOGLE_API_KEY'])\n",
    "        model = genai.GenerativeModel('gemini-1.5-flash')\n",
    "\n",
    "        empty_topics_rows = open_questions_pandas[(open_questions_pandas['topics'] == '') & (open_questions_pandas['question'].str.strip() != '')]\n",
    "        if empty_topics_rows.shape[0] == 0:\n",
    "            break\n",
    "\n",
    "        # Get the indices of the first 15 rows with empty \"topics\"\n",
    "        indices_to_update = empty_topics_rows.index[:15]\n",
    "\n",
    "        # Apply the UDF only to the selected rows\n",
    "        open_questions_pandas.loc[indices_to_update, 'topics'] = (\n",
    "            open_questions_pandas.loc[indices_to_update, 'question']\n",
    "                .apply(extract_topics_from_question)\n",
    "        )\n",
    "    running_time = time.time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31ed7f94-9a5b-4ce1-b31d-bda46b441b80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from consts import QUESTIONS_PATH\n",
    "import os\n",
    "\n",
    "open_questions_with_topics = spark.createDataFrame(open_questions_pandas)\n",
    "open_questions_pandas.to_csv(os.path.join(QUESTIONS_PATH, 'all_open_questions_with_topics.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987c6eb2-9188-4593-a66b-8d33a469e0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "open_questions_with_topics.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c0aaeee6-2bdf-4bba-8fe9-33572e9beb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Topics & skills embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed3c55f9-6d4a-4210-bad3-7c018a06dc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "job_postings_with_skills = pd.read_csv(os.path.join(JOBS_PATH, \"all_jobpostings_with_skills.csv\"))\n",
    "job_postings_with_skills['skills'] = job_postings_with_skills['skills'].fillna('')\n",
    "code_questions_with_topics = pd.read_csv(os.path.join(QUESTIONS_PATH, \"all_code_questions_with_topics.csv\"))\n",
    "open_questions_with_topics = pd.read_csv(os.path.join(QUESTIONS_PATH, \"all_open_questions_with_topics.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b84699e-41ae-4efc-b148-db4bb728b3ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load the model for embedding generation\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # Lightweight model for semantic similarity\n",
    "\n",
    "# Tokenization and Embedding Generation\n",
    "def preprocess_and_embed_list(text_column):\n",
    "    \"\"\"Tokenizes the list and generates embeddings for individual items.\"\"\"\n",
    "    # Split the list into individual items\n",
    "    tokens = text_column.str.split(',').apply(lambda x: [t.strip().lower() for t in x])\n",
    "    \n",
    "    # Embed each item and aggregate embeddings\n",
    "    embeddings = tokens.apply(lambda x: model.encode(x))\n",
    "    aggregated_embeddings = embeddings.apply(lambda x: x.mean(axis=0))  # Mean pooling\n",
    "    \n",
    "    return tokens, aggregated_embeddings\n",
    "\n",
    "job_postings_with_skills['skills_tokens'], job_postings_with_skills['skills_embeddings'] = preprocess_and_embed_list(job_postings_with_skills['skills'])\n",
    "code_questions_with_topics['topics_tokens'], code_questions_with_topics['topics_embeddings'] = preprocess_and_embed_list(code_questions_with_topics['topics'])\n",
    "open_questions_with_topics['topics_tokens'], open_questions_with_topics['topics_embeddings'] = preprocess_and_embed_list(open_questions_with_topics['topics'])\n",
    "\n",
    "# Cartesian Product\n",
    "cartesian_pairs_code = pd.DataFrame(\n",
    "    itertools.product(job_postings_with_skills.index, code_questions_with_topics.index),\n",
    "    columns=['job_idx', 'question_idx']\n",
    ")\n",
    "cartesian_pairs_open = pd.DataFrame(\n",
    "    itertools.product(job_postings_with_skills.index, open_questions_with_topics.index),\n",
    "    columns=['job_idx', 'question_idx']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3e65bd6-2137-4972-b88b-3702cca59a84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cartesian_pairs_code)\n",
    "display(cartesian_pairs_open)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5460dcb-4bbc-416a-acd5-dfb68e881ae4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def compute_match_score(row, questions_df, skills_column, topics_column):\n",
    "    \"\"\"\n",
    "    Calculates the match score for a job-question pair.\n",
    "    \n",
    "    Parameters:\n",
    "        - row: The row from the Cartesian product DataFrame.\n",
    "        - questions_df: The DataFrame containing the questions (either 'code_questions' or 'open_questions').\n",
    "        - skills_column: The column in the job postings DataFrame containing skill embeddings.\n",
    "        - topics_column: The column in the questions DataFrame containing topic embeddings.\n",
    "    \n",
    "    Returns:\n",
    "        - similarity: Cosine similarity between job skills and question topics.\n",
    "    \"\"\"\n",
    "    # Get job skills and question topics embeddings\n",
    "    job_skills = job_postings_with_skills.loc[row['job_idx'], skills_column]\n",
    "    question_topics = questions_df.loc[row['question_idx'], topics_column]\n",
    "    \n",
    "    # Ensure embeddings are reshaped to 2D arrays\n",
    "    job_skills = job_skills.reshape(1, -1) if len(job_skills.shape) == 1 else job_skills\n",
    "    question_topics = question_topics.reshape(1, -1) if len(question_topics.shape) == 1 else question_topics\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    similarity = cosine_similarity(job_skills, question_topics)\n",
    "    return similarity[0][0]\n",
    "\n",
    "# Apply the function to compute scores for code_questions\n",
    "cartesian_pairs_code['skills_topics_score'] = cartesian_pairs_code.apply(\n",
    "    compute_match_score, axis=1, questions_df=code_questions_with_topics, \n",
    "    skills_column='skills_embeddings', topics_column='topics_embeddings'\n",
    ")\n",
    "\n",
    "# Apply the function to compute scores for open_questions\n",
    "cartesian_pairs_open['skills_topics_score'] = cartesian_pairs_open.apply(\n",
    "    compute_match_score, axis=1, questions_df=open_questions_with_topics, \n",
    "    skills_column='skills_embeddings', topics_column='topics_embeddings'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7321b15b-9e52-455e-bbbf-968791837d20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Another try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a9811dbf-a07d-4394-8328-9afe3a7e27e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder.appName(\"JobQuestionMatching\").getOrCreate()\n",
    "\n",
    "# Load datasets into Spark DataFrames\n",
    "job_postings_df = open_csv_file(spark, JOBS_PATH, \"all_jobpostings_with_skills.csv\")\n",
    "job_postings_with_skills['skills'] = job_postings_with_skills['skills'].fillna('')\n",
    "code_questions_df = open_csv_file(spark, QUESTIONS_PATH, \"all_code_questions_with_topics.csv\")\n",
    "open_questions_df = open_csv_file(spark, QUESTIONS_PATH, \"all_open_questions_with_topics.csv\")\n",
    "\n",
    "# Explode the skills and topics columns\n",
    "job_postings_exploded = job_postings_df.withColumn(\"skill\", explode(split(\"skills\", \",\")))\n",
    "code_questions_exploded = code_questions_df.withColumn(\"topic\", explode(split(\"topics\", \",\")))\n",
    "open_questions_exploded = open_questions_df.withColumn(\"topic\", explode(split(\"topics\", \",\")))\n",
    "\n",
    "# Cartesian product between job postings and questions\n",
    "cartesian_code = job_postings_exploded.crossJoin(code_questions_exploded)\n",
    "cartesian_open = job_postings_exploded.crossJoin(open_questions_exploded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04abee34-a875-4921-bc2f-e18275da087c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas for embedding and similarity calculation\n",
    "cartesian_code_pandas = cartesian_code.toPandas()\n",
    "cartesian_open_pandas = cartesian_open.toPandas()\n",
    "\n",
    "# Generate embeddings and compute similarity in Pandas\n",
    "cartesian_code_pandas['skill_embedding'] = cartesian_code_pandas['skill'].apply(model.encode)\n",
    "cartesian_code_pandas['topic_embedding'] = cartesian_code_pandas['topic'].apply(model.encode)\n",
    "\n",
    "cartesian_code_pandas['similarity'] = cartesian_code_pandas.apply(\n",
    "    lambda row: cosine_similarity([row['skill_embedding']], [row['topic_embedding']])[0][0], axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4386595b-971f-4f5b-a399-6d606693930b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert back to Spark DataFrame for aggregation\n",
    "similarity_df = spark.createDataFrame(cartesian_code_pandas)\n",
    "\n",
    "# Aggregate similarity scores\n",
    "aggregated_scores = similarity_df.groupBy(\"job_idx\", \"question_idx\").sum(\"similarity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "330b82b4-bb7f-4610-b8ab-ce2220d40899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Heuristic to match questions to jobs\n",
    "Questions with the highest hueristic grades wil be the most likely to appear in the interview."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "caf5d192-60ce-4109-85d7-6a44a53d7bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import FloatType\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Each question will be initially considered for each job posting.\n",
    "jobs_with_code_questions = job_postings.crossJoin(code_questions)\n",
    "jobs_with_open_questions = job_postings.crossJoin(open_questions)\n",
    "\n",
    "# Map difficulty levels to numeric values\n",
    "difficulty_map = {\"Easy\": 0, \"Medium\": 1, \"Hard\": 2}\n",
    "\n",
    "# Change difficulty column to numeric\n",
    "code_questions = code_questions.withColumn(\n",
    "    \"difficulty\", col(\"difficulty\").map(difficulty_map)\n",
    ")\n",
    "\n",
    "# Match question's difficulty to job posting's level\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"difficulty_match\",\n",
    "    1 - abs(col(\"difficulty\") - col(\"level\")) / 2\n",
    ")\n",
    "\n",
    "# Calculate cosine similarity between embeddings\n",
    "def calculate_similarity(embedding1, embedding2):\n",
    "    if embedding1 is None or embedding2 is None:\n",
    "        return 0.0\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "similarity_udf = udf(calculate_similarity, FloatType())\n",
    "\n",
    "# Add topic similarity scores\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"emb_similarity\",\n",
    "    similarity_udf(col(\"jobposting_embedding\", \"topics_embedding\"))\n",
    ")\n",
    "jobs_with_open_questions = jobs_with_open_questions.withColumn(\n",
    "    \"emb_similarity\",\n",
    "    similarity_udf(col(\"jobposting_embedding\", \"question_embedding\"))\n",
    ")\n",
    "\n",
    "# Normalize Acceptance for code questions\n",
    "max_acceptance = code_questions.agg({\"acceptance\": \"max\"}).collect()[0][0]\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"normalized_acceptance\", col(\"acceptance\") / max_acceptance\n",
    ")\n",
    "\n",
    "# Calculate Heuristic Score\n",
    "def calculate_score(difficulty, similarity, acceptance):\n",
    "    return 0.3 * difficulty + 0.5 * similarity + 0.2 * acceptance\n",
    "calculate_score_udf = udf(calculate_score, FloatType())\n",
    "\n",
    "jobs_with_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"heuristic_score\",\n",
    "    calculate_score_udf(\n",
    "        col(\"difficulty_match\"),\n",
    "        col(\"emb_similarity\"),\n",
    "        col(\"normalized_acceptance\"),\n",
    "    ),\n",
    ")\n",
    "display(jobs_with_code_questions)\n",
    "\n",
    "jobs_with_open_questions = jobs_with_open_questions.withColumn(\n",
    "    \"heuristic_score\",\n",
    "    calculate_score_udf(\n",
    "        col(\"difficulty_match\"),  # This is 0 for open questions\n",
    "        col(\"emb_similarity\"),\n",
    "        lit(0)  # No acceptance column in open questions\n",
    "    ),\n",
    ")\n",
    "display(jobs_with_open_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5413a107-3774-46df-9ee5-cc1586e9ab28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Select Top Questions for Each Job\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "\n",
    "window_spec = Window.partitionBy(\"job_id\").orderBy(col(\"heuristic_score\").desc())\n",
    "\n",
    "top_code_questions = jobs_with_code_questions.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 10)\n",
    "\n",
    "top_open_questions = jobs_with_open_questions.withColumn(\n",
    "    \"rank\", row_number().over(window_spec)\n",
    ").filter(col(\"rank\") <= 10)\n",
    "\n",
    "display(top_code_questions)\n",
    "display(top_open_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7414d29b-013e-4306-afb8-3ebeec400ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Join scores back to original DataFrames for easy analysis\n",
    "result = cartesian_pairs.merge(job_postings_with_skills, left_on='job_idx', right_index=True)\n",
    "result = result.merge(code_questions_with_topics, left_on='question_idx', right_index=True)\n",
    "\n",
    "# Sort by match score\n",
    "result = result.sort_values(by='heuristic_score', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "select_interview_questions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

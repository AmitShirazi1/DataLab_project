{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12f6c0c3-6f8b-47d0-8fd8-0deea17d70c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualizations and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8382d6-2c2b-4ded-83fd-2f5611fb0bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Job and Interview Analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "job_skills_spark = open_csv_file(spark, JOBS_PATH, 'all_jobpostings_with_skills.csv')\n",
    "open_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions_with_topics.csv')\n",
    "code_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_code_questions_with_topics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ce12ce-5be8-4cd5-aff6-91d7331d1ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### check the most common seniority level for each job field and job title and present the top 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425a83eb-1efa-4f0b-926d-60af70ccc5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import seaborn as sns\n",
    "\n",
    "# Map levels back to seniority level\n",
    "seniority_map = {\n",
    "    \"0\": \"Internship\",\n",
    "    \"1\": \"Entry level/Associate\",\n",
    "    \"2\": \"Mid-Senior level/Manager and above\"\n",
    "}\n",
    "\n",
    "# Function to normalize job titles (remove seniority/level indicators like \"Senior\", \"I\", etc.)\n",
    "def normalize_job_title(title):\n",
    "    # Define common seniority indicators and level indicators\n",
    "    seniority_terms = ['senior', 'junior', 'lead', 'principal', 'assistant', 'associate']\n",
    "    level_terms = ['i', 'ii', 'iii', 'iv', 'v']  # Handles \"I\", \"II\", etc.\n",
    "\n",
    "    # Normalize case and remove special characters except spaces\n",
    "    title = re.sub(r\"[^a-zA-Z\\s]\", \"\", title.lower())\n",
    "\n",
    "    # Remove seniority and level terms\n",
    "    words = title.split()\n",
    "    filtered_words = [word for word in words if word not in seniority_terms and word not in level_terms]\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Register the function as a UDF for Spark\n",
    "normalize_job_title_udf = udf(normalize_job_title, StringType())\n",
    "\n",
    "# Clean Data (drop NaN and lowercase)\n",
    "job_skills_cleaned = job_skills_spark \\\n",
    "    .dropna(subset=[\"skills\", \"job_summary\", \"company_industry\", \"field\", \"job_title\"]) \\\n",
    "    .withColumn(\"job_title_normalized\", normalize_job_title_udf(col(\"job_title\"))) \\\n",
    "    .withColumn(\"field\", F.lower(col(\"field\"))) \\\n",
    "    .withColumn(\"seniority_level\",\n",
    "        when(col(\"level\") == \"0\", seniority_map[\"0\"])\n",
    "        .when(col(\"level\") == \"1\", seniority_map[\"1\"])\n",
    "        .when(col(\"level\") == \"2\", seniority_map[\"2\"])\n",
    "    ).drop(\"level\").cache()\n",
    "\n",
    "# Group by normalized job title and field, then count the most common seniority levels\n",
    "level_by_title_field = job_skills_cleaned.groupBy(\"field\", \"job_title_normalized\", \"seniority_level\") \\\n",
    "    .count().toPandas()\n",
    "\n",
    "# Define the correct order for seniority levels\n",
    "seniority_order = [\"Internship\", \"Entry level/Associate\", \"Mid-Senior level/Manager and above\"]\n",
    "\n",
    "# Convert the seniority_level column into a categorical type with the defined order\n",
    "level_by_title_field[\"seniority_level\"] = pd.Categorical(\n",
    "    level_by_title_field[\"seniority_level\"], \n",
    "    categories=seniority_order, \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Sort by count and select the top 50 most common job titles and fields\n",
    "top_50 = level_by_title_field.sort_values(by=\"count\", ascending=False).head(50)\n",
    "\n",
    "# Create separate plots for each seniority level\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 15), sharex=True)\n",
    "\n",
    "for ax, level in zip(axes, seniority_order):\n",
    "    data = top_50[top_50[\"seniority_level\"] == level]\n",
    "    \n",
    "    sns.barplot(\n",
    "        x='job_title_normalized', y='count', data=data, \n",
    "        palette=\"Set2\", ax=ax, edgecolor=None  # Remove black lines\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"{level} Job Titles\", fontsize=16)\n",
    "    ax.set_ylabel(\"Count\", fontsize=14)\n",
    "    ax.set_xlabel(\"Job Title\", fontsize=14)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=12)\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00568f3a-099b-43cd-9ac4-721b9daebacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### word cloud on the topics for the different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c294f7fe-c414-4f26-99ae-1b0b69c4a55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, split, explode, trim, collect_list, lit, array_except, array\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WordCloudVisualization\").getOrCreate()\n",
    "\n",
    "# Define stop words, including \"skill\" and \"skills\"\n",
    "stop_words = [\"and\", \"of\", \"the\", \"a\", \"in\", \"to\", \"skill\", \"skills\"]\n",
    "\n",
    "# Process topics column (handle nulls, lowercase, split, explode, trim, and remove stop words)\n",
    "processed_df = (\n",
    "    open_questions_spark\n",
    "    .filter(col(\"topics\").isNotNull())  # Exclude rows with null topics\n",
    "    .withColumn(\"topics\", lower(col(\"topics\")))  # Convert to lowercase\n",
    "    .withColumn(\"topics\", split(col(\"topics\"), \",\\\\s*\"))  # Split by commas\n",
    "    .withColumn(\"topics\", explode(col(\"topics\")))  # Explode into individual sub-topics\n",
    "    .withColumn(\"topics\", trim(col(\"topics\")))  # Trim whitespace\n",
    "    .withColumn(\"topics\", split(col(\"topics\"), \"\\\\s+\"))  # Split multi-word phrases into words\n",
    "    .withColumn(\"topics\", explode(col(\"topics\")))  # Explode words into separate rows\n",
    "    .withColumn(\"topics\", array_except(array(col(\"topics\")), lit(stop_words)))  # Remove stop words\n",
    ")\n",
    "\n",
    "# Aggregate topics for each category\n",
    "category_topics_df = (\n",
    "    processed_df\n",
    "    .groupBy(\"category\")\n",
    "    .agg(collect_list(\"topics\").alias(\"all_topics\"))  # Aggregate topics into a list\n",
    ")\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas for visualization\n",
    "category_topics_pd = category_topics_df.toPandas()\n",
    "\n",
    "# Generate and visualize word clouds\n",
    "for index, row in category_topics_pd.iterrows():\n",
    "    category = row[\"category\"]\n",
    "    topics = \" \".join([str(item) for sublist in row[\"all_topics\"] for item in sublist])\n",
    "\n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"viridis\",\n",
    "        max_words=100,\n",
    "        contour_color=\"black\",\n",
    "        contour_width=1\n",
    "    ).generate(topics)\n",
    "    \n",
    "    # Plot the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for Category: {category}\", fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c6b642-ba4c-433f-b64c-e3901998fcaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### difficulty vs. acceptance + difficulty vs. num of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b65ec77-cec7-4643-8230-9679fc9e2a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, size\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Add a column for the number of topics\n",
    "code_questions_spark = code_questions_spark.withColumn(\"num_topics\", size(split(col(\"topics\"), \",\")))\n",
    "\n",
    "# Step 2: Group by difficulty and calculate the average acceptance and average number of topics\n",
    "df_grouped = code_questions_spark.groupBy(\"difficulty\").agg(\n",
    "    {\"acceptance\": \"avg\", \"num_topics\": \"avg\"}\n",
    ").withColumnRenamed(\"avg(acceptance)\", \"avg_acceptance\").withColumnRenamed(\"avg(num_topics)\", \"avg_num_topics\")\n",
    "\n",
    "# Step 3: Convert the result to Pandas for plotting\n",
    "df_grouped_pd = df_grouped.toPandas()\n",
    "\n",
    "# Step 4: Plot the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot difficulty vs average acceptance\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=\"difficulty\", y=\"avg_acceptance\", data=df_grouped_pd, palette = sns.color_palette(\"Set2\"))\n",
    "plt.title(\"Difficulty vs Average Acceptance\")\n",
    "plt.xlabel(\"Difficulty\")\n",
    "plt.ylabel(\"Average Acceptance\")\n",
    "\n",
    "# Plot difficulty vs average number of topics\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=\"difficulty\", y=\"avg_num_topics\", data=df_grouped_pd, palette = sns.color_palette(\"Set2\"))\n",
    "plt.title(\"Difficulty vs Average Number of Topics\")\n",
    "plt.xlabel(\"Difficulty\")\n",
    "plt.ylabel(\"Average Number of Topics\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be214e6-ecc1-4d70-b980-547c158c3c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### word cloud for skills or job summary, see the relationship between field and company_industry\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a40a3ef5-0d0d-4646-a78d-27288a2866f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "import plotly.express as px\n",
    "\n",
    "# Assuming 'job_skills_spark' is a PySpark DataFrame\n",
    "\n",
    "# Step 1: Clean Data (drop NaN and lowercase)\n",
    "job_skills_cleaned = job_skills_spark \\\n",
    "    .dropna(subset=[\"skills\", \"job_summary\", \"company_industry\", \"field\"]) \\\n",
    "    .withColumn(\"skills\", F.lower(F.col(\"skills\"))) \\\n",
    "    .withColumn(\"job_summary\", F.lower(F.col(\"job_summary\"))) \\\n",
    "    .withColumn(\"company_industry\", F.lower(F.col(\"company_industry\"))) \\\n",
    "    .withColumn(\"field\", F.lower(F.col(\"field\")))\n",
    "\n",
    "# Step 2: Process Skills Column for Word Cloud\n",
    "# Split skills by comma, then join all to create a single string\n",
    "skills_df = job_skills_cleaned.select(F.explode(F.split(F.col(\"skills\"), \",\")).alias(\"skill\"))\n",
    "skills_list = skills_df.rdd.map(lambda row: row.skill).collect()\n",
    "\n",
    "# Step 3: Create Word Cloud for Skills\n",
    "skills_wordcloud = WordCloud(stopwords=ENGLISH_STOP_WORDS, background_color=\"white\", width=800, height=400).generate(\" \".join(skills_list))\n",
    "\n",
    "# Step 4: Process Job Summary for Word Cloud\n",
    "# Remove stop words and generate word cloud for job summary\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in text.split() if word not in ENGLISH_STOP_WORDS])\n",
    "\n",
    "job_summary_list = job_skills_cleaned.rdd.map(lambda row: remove_stopwords(row.job_summary)).collect()\n",
    "\n",
    "job_summary_wordcloud = WordCloud(stopwords=ENGLISH_STOP_WORDS, background_color=\"white\", width=800, height=400).generate(\" \".join(job_summary_list))\n",
    "\n",
    "# Step 5: Visualize the Relationship between 'field' and 'company_industry' (e.g., bar plot)\n",
    "# Create a DataFrame to count occurrences of combinations of 'field' and 'company_industry'\n",
    "field_industry_counts = job_skills_cleaned.groupBy(\"field\", \"company_industry\").count().toPandas()\n",
    "\n",
    "# Step 6: Plot the Word Clouds\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.imshow(skills_wordcloud, interpolation=\"bilinear\")\n",
    "plt.title(\"Word Cloud for Skills\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.imshow(job_summary_wordcloud, interpolation=\"bilinear\")\n",
    "plt.title(\"Word Cloud for Job Summary\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d080393-02d1-43a5-8b01-db77c804061b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 7: Plot the Relationship between 'field' and 'company_industry' using Plotly\n",
    "fig = px.bar(field_industry_counts, x='field', y='count', color='company_industry',\n",
    "             labels={\"field\": \"Field\", \"count\": \"Count\", \"company_industry\": \"Company Industry\"},\n",
    "             title=\"Relationship Between Field and Company Industry\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "057a3dba-eedf-47c6-bfd0-62acb943bbac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lower, explode, split\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process skills\n",
    "# Convert to lowercase and split by comma\n",
    "skills_df = job_skills_spark.select(\n",
    "    explode(split(lower(\"skills\"), \",\")).alias(\"skill\")\n",
    ").dropna()\n",
    "\n",
    "# Convert to pandas for visualization\n",
    "skills_pd = skills_df.toPandas()\n",
    "skills_text = \" \".join(skills_pd[\"skill\"].str.strip())\n",
    "\n",
    "# Create word cloud for skills\n",
    "plt.figure(figsize=(12, 8))\n",
    "wordcloud_skills = WordCloud(width=800, height=400,\n",
    "                           background_color='white',\n",
    "                           min_font_size=10).generate(skills_text)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(wordcloud_skills, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Skills Word Cloud')\n",
    "plt.show()\n",
    "\n",
    "# Process job summary\n",
    "# Convert to lowercase and remove stop words\n",
    "job_summary_df = job_skills_spark.select(lower(\"job_summary\").alias(\"summary\")).dropna()\n",
    "job_summary_pd = job_summary_df.toPandas()\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    if isinstance(text, str):\n",
    "        words = text.lower().split()\n",
    "        return \" \".join([word for word in words if word not in stop_words])\n",
    "    return \"\"\n",
    "\n",
    "job_summary_text = \" \".join(job_summary_pd[\"summary\"].apply(clean_text))\n",
    "\n",
    "# Create word cloud for job summary\n",
    "plt.figure(figsize=(12, 8))\n",
    "wordcloud_summary = WordCloud(width=800, height=400,\n",
    "                            background_color='white',\n",
    "                            min_font_size=10).generate(job_summary_text)\n",
    "\n",
    "plt.imshow(wordcloud_summary, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Job Summary Word Cloud')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "790251b4-4f18-4970-b332-6982084833f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Process field and company_industry relationship\n",
    "# Split and explode both columns\n",
    "relationship_df = job_skills_spark.select(\n",
    "    explode(split(lower(\"field\"), \",\")).alias(\"field\"),\n",
    "    explode(split(lower(\"company_industry\"), \",\")).alias(\"industry\")\n",
    ").dropna()\n",
    "\n",
    "# Convert to pandas and count relationships\n",
    "relationship_pd = relationship_df.toPandas()\n",
    "relationship_counts = relationship_pd.groupby(['field', 'industry']).size().reset_index(name='count')\n",
    "\n",
    "# Create node lists and mapping\n",
    "nodes = list(set(relationship_counts['field'].unique()) | set(relationship_counts['industry'].unique()))\n",
    "node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "\n",
    "# Create source, target, and value lists for Sankey diagram\n",
    "sources = [node_to_idx[field] for field in relationship_counts['field']]\n",
    "targets = [node_to_idx[industry] for industry in relationship_counts['industry']]\n",
    "values = relationship_counts['count']\n",
    "\n",
    "# Create Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=nodes,\n",
    "        color=\"blue\"\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=sources,\n",
    "        target=targets,\n",
    "        value=values\n",
    "    )\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Relationship between Fields and Company Industries\",\n",
    "    font_size=12,\n",
    "    height=800\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c5485f9-46b2-4692-b5ab-0d18fbadd06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import lower, explode, split\n",
    "\n",
    "# Process field and company_industry relationship\n",
    "relationship_df = job_skills_spark.select(\n",
    "    explode(split(lower(\"field\"), \",\")).alias(\"field\"),\n",
    "    explode(split(lower(\"company_industry\"), \",\")).alias(\"industry\")\n",
    ").dropna()\n",
    "\n",
    "# Convert to pandas and count relationships\n",
    "relationship_pd = relationship_df.toPandas()\n",
    "relationship_counts = relationship_pd.groupby(['field', 'industry']).size().reset_index(name='count')\n",
    "\n",
    "# Create a graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add edges with weights\n",
    "for _, row in relationship_counts.iterrows():\n",
    "    G.add_edge(row['field'], row['industry'], weight=row['count'])\n",
    "\n",
    "# Get maximum weight for normalization\n",
    "max_weight = max(dict(G.edges()).values(), key=lambda x: x['weight'])['weight']\n",
    "\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# Create layout\n",
    "pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "\n",
    "# Draw the network\n",
    "# Edges with fixed width calculation\n",
    "edge_weights = [G[u][v]['weight']/max_weight * 5 for u,v in G.edges()]\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.2, width=edge_weights)\n",
    "\n",
    "# Nodes\n",
    "nx.draw_networkx_nodes(G, pos, \n",
    "                      node_color='lightblue',\n",
    "                      node_size=2000,\n",
    "                      alpha=0.6)\n",
    "\n",
    "# Labels\n",
    "nx.draw_networkx_labels(G, pos, \n",
    "                       font_size=8,\n",
    "                       font_weight='bold')\n",
    "\n",
    "# Add title and remove axes\n",
    "plt.title(\"Industry-Field Relationships\", fontsize=16, pad=20)\n",
    "plt.axis('off')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5381afd0-b099-490d-9f02-b562c03dceba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create filtered version\n",
    "# Calculate threshold\n",
    "threshold = relationship_counts['count'].median()\n",
    "\n",
    "# Create filtered graph\n",
    "G_filtered = nx.Graph()\n",
    "\n",
    "for _, row in relationship_counts[relationship_counts['count'] > threshold].iterrows():\n",
    "    G_filtered.add_edge(row['field'], row['industry'], weight=row['count'])\n",
    "\n",
    "# Get maximum weight for filtered graph\n",
    "max_weight_filtered = max(dict(G_filtered.edges()).values(), key=lambda x: x['weight'])['weight']\n",
    "\n",
    "# Create filtered visualization\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "pos_filtered = nx.spring_layout(G_filtered, k=1, iterations=50)\n",
    "\n",
    "# Draw edges with fixed width calculation\n",
    "edge_weights_filtered = [G_filtered[u][v]['weight']/max_weight_filtered * 5 for u,v in G_filtered.edges()]\n",
    "nx.draw_networkx_edges(G_filtered, pos_filtered, alpha=0.4, width=edge_weights_filtered)\n",
    "\n",
    "nx.draw_networkx_nodes(G_filtered, pos_filtered,\n",
    "                      node_color='lightblue',\n",
    "                      node_size=2000,\n",
    "                      alpha=0.6)\n",
    "\n",
    "nx.draw_networkx_labels(G_filtered, pos_filtered,\n",
    "                       font_size=10,\n",
    "                       font_weight='bold')\n",
    "\n",
    "plt.title(\"Industry-Field Relationships (Strong Connections Only)\", fontsize=16, pad=20)\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3213a8e7-c0c9-4a57-89a2-069ea1ca414a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, avg\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "job_data = job_skills_spark # Read Data from CSV\n",
    "\n",
    "# Helper Functions\n",
    "def preprocess_skills(data):\n",
    "    \"\"\"Splits skills into individual entries and counts their occurrences.\"\"\"\n",
    "    skills = data.withColumn(\"skill\", explode(split(col(\"skills\"), \", \"))) \\\n",
    "                 .groupBy(\"skill\") \\\n",
    "                 .count() \\\n",
    "                 .orderBy(col(\"count\").desc())\n",
    "    return skills\n",
    "\n",
    "# Most In-Demand Skills\n",
    "def most_in_demand_skills(data):\n",
    "    skills = preprocess_skills(data).toPandas()\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(zip(skills[\"skill\"], skills[\"count\"])))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most In-Demand Skills\")\n",
    "    plt.show()\n",
    "\n",
    "most_in_demand_skills(job_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf530001-a345-4295-9036-646e61d393bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, avg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "code_questions_data = code_questions_spark # Read Data from CSV\n",
    "\n",
    "# Questions by Difficulty\n",
    "def questions_by_difficulty(data):\n",
    "    difficulty_counts = data.groupBy(\"difficulty\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "    difficulty_counts.plot(kind=\"pie\", y=\"count\", labels=difficulty_counts[\"difficulty\"], colors=sns.color_palette('Set2'), autopct=\"%1.1f%%\")\n",
    "    plt.title(\"Questions by Difficulty\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()\n",
    "questions_by_difficulty(code_questions_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242e662c-6ebc-4f6d-9c9c-d3c1431b614e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Visualizations for the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "bfb3c3d6-01dd-4dad-97e3-701e6992cce5",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1738174766487}",
       "tableResultIndex": 0
      },
      "1": {
       "filterBlob": "{\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_c15d34f8\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_61510f60\",\"enabled\":true,\"columnId\":\"company_industry\",\"dataType\":\"string\",\"filterType\":\"eq\",\"filterValues\":[]}],\"local\":false,\"updatedAt\":1738186834020}],\"syncTimestamp\":1738186834020}",
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from consts import DATA_PATH, QUESTIONS_PATH, open_csv_file\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Job and Interview Analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "top_code_spark = open_csv_file(spark, DATA_PATH, 'top_code_questions.csv')\n",
    "all_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_code_questions_with_topics.csv').select(\"question_id\", \"difficulty\")\n",
    "\n",
    "top_jointed_code = top_code_spark.join(all_questions_spark, on=\"question_id\", how=\"left\")\n",
    "display(top_jointed_code.limit(5))\n",
    "\n",
    "top_open_spark = open_csv_file(spark, DATA_PATH, 'top_open_questions.csv')\n",
    "display(top_open_spark.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "511cdda9-a66a-404f-8506-70813e92a495",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "explanation for the Code plots below 👇\n",
    "1. Question Difficulty and Compatibility: <br>\n",
    "* There's a clear relationship between question difficulty and compatibility scores\n",
    "* We can see how different difficulty levels map to different seniority requirements\n",
    "\n",
    "2. Seniority Level Distribution: <br>\n",
    "* The distribution of questions across seniority levels shows the focus of hiring\n",
    "* We can see which level has the most compatible questions\n",
    "\n",
    "3. Skills and Topics Analysis: <br>\n",
    "* The top required skills visualization shows which technical skills are most in demand\n",
    "* The question topics distribution reveals what technical areas are most commonly tested\n",
    "* This can help in understanding the alignment between job requirements and interview questions\n",
    "\n",
    "4. Industry and Company Insights: <br>\n",
    "* The top industries visualization shows which sectors are most active in technical hiring\n",
    "* We can see patterns in how different industries approach technical questions\n",
    "* The company analysis reveals which organizations have the highest compatibility scores\n",
    "\n",
    "5. Question Similarity Impact: <br>\n",
    "* The scatter plot shows the relationship between question similarity and compatibility scores\n",
    "* This helps understand if commonly asked questions are more or less compatible with job requirements\n",
    "\n",
    "6. Level-based Insights: <br>\n",
    "* The difficulty distribution by level shows how question complexity varies with seniority\n",
    "* The average compatibility scores by level reveal how well questions match different career stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48770aed-b63d-48ff-944a-a49d919b6883",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "code"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# Read the CSV file\n",
    "df = top_jointed_code.toPandas()\n",
    "\n",
    "# Set the style for better-looking plots\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# 1. Distribution of Question Difficulties vs Average Heuristic Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "difficulty_scores = df.groupby('difficulty')['heuristic_score'].mean().sort_values(ascending=False)\n",
    "sns.barplot(x=difficulty_scores.index, y=difficulty_scores.values)\n",
    "plt.title('Average Compatibility Score by Question Difficulty')\n",
    "plt.ylabel('Average Heuristic Score')\n",
    "plt.show()\n",
    "\n",
    "# 2. Distribution of Seniority Levels\n",
    "plt.figure(figsize=(8, 8))\n",
    "level_counts = df['level'].value_counts().sort_index()\n",
    "plt.pie(level_counts.values, labels=['Junior', 'Mid', 'Senior'], autopct='%1.1f%%')\n",
    "plt.title('Distribution of Seniority Levels')\n",
    "plt.show()\n",
    "\n",
    "# 3. Top 10 Required Skills\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_skills = [skill.strip() for skills in df['skills'].dropna() for skill in skills.split(',')]\n",
    "top_skills = pd.Series(Counter(all_skills)).sort_values(ascending=True)[-10:]\n",
    "sns.barplot(y=top_skills.index, x=top_skills.values)\n",
    "plt.title('Top 10 Required Skills')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 4. Question Topics Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_topics = [topic.strip() for topics in df['topics'].dropna() for topic in topics.split(',')]\n",
    "top_topics = pd.Series(Counter(all_topics)).sort_values(ascending=True)[-10:]\n",
    "sns.barplot(y=top_topics.index, x=top_topics.values)\n",
    "plt.title('Top 10 Question Topics')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 5. Correlation between Number of Similar Questions and Heuristic Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=df, x='no_similar_questions', y='heuristic_score')\n",
    "plt.title('Correlation: Similar Questions vs Compatibility Score')\n",
    "plt.xlabel('Number of Similar Questions')\n",
    "plt.ylabel('Heuristic Score')\n",
    "plt.show()\n",
    "\n",
    "# 6. Top Industries\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_industries = [ind.strip() for industries in df['company_industry'].dropna() for ind in industries.split(',')]\n",
    "top_industries = pd.Series(Counter(all_industries)).sort_values(ascending=True)[-10:]\n",
    "sns.barplot(y=top_industries.index, x=top_industries.values)\n",
    "plt.title('Top 10 Industries')\n",
    "plt.xlabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# 7. Difficulty Distribution by Level\n",
    "plt.figure(figsize=(10, 6))\n",
    "difficulty_level = pd.crosstab(df['difficulty'], df['level'])\n",
    "difficulty_level.plot(kind='bar', stacked=True)\n",
    "plt.title('Question Difficulty Distribution by Seniority Level')\n",
    "plt.xlabel('Difficulty')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(['Junior', 'Mid', 'Senior'])\n",
    "plt.show()\n",
    "\n",
    "# 8. Average Heuristic Score by Level\n",
    "plt.figure(figsize=(10, 6))\n",
    "level_scores = df.groupby('level')['heuristic_score'].mean()\n",
    "sns.barplot(x=['Junior', 'Mid', 'Senior'], y=level_scores.values)\n",
    "plt.title('Average Compatibility Score by Seniority Level')\n",
    "plt.xlabel('Seniority Level')\n",
    "plt.ylabel('Average Heuristic Score')\n",
    "plt.show()\n",
    "\n",
    "# Additional analysis for company and field insights\n",
    "print(\"\\nTop 5 Companies by Average Heuristic Score:\")\n",
    "company_scores = df.groupby('company_name')['heuristic_score'].mean().sort_values(ascending=False).head()\n",
    "display(company_scores)\n",
    "\n",
    "print(\"\\nCorrelation between number of similar questions and heuristic score:\")\n",
    "correlation = df['no_similar_questions'].corr(df['heuristic_score'])\n",
    "display(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c46c0e1-2d02-43ef-80da-280c1a78631b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "explanation for the Open plots below 👇\n",
    "1. Category Distribution: <br>\n",
    "* Shows the proportion of questions between data science and general categories\n",
    "* Helps understand the balance of technical vs. soft skills questions\n",
    "\n",
    "2. Seniority Level Analysis: <br>\n",
    "* Reveals how heuristic scores vary across different seniority levels\n",
    "* Helps identify if certain levels have consistently higher compatibility scores\n",
    "\n",
    "3. Skills Analysis: <br>\n",
    "* Identifies the most frequently required skills across job postings\n",
    "* Useful for understanding which skills are most in demand\n",
    "\n",
    "4. Industry Insights: <br>\n",
    "* Shows which industries have the most relevant questions\n",
    "* Helps identify sectors with specific question patterns\n",
    "\n",
    "5. Level-Score Correlation: <br>\n",
    "* Demonstrates any relationship between seniority and question compatibility\n",
    "* Useful for understanding if question complexity aligns with job level\n",
    "\n",
    "6. Field Distribution: <br>\n",
    "* Reveals the most common job fields in the dataset\n",
    "* Helps understand which areas have the most specialized questions\n",
    "\n",
    "7. Topic Analysis: <br>\n",
    "* Shows the most common question topics\n",
    "* Helps identify patterns in question content\n",
    "\n",
    "8. Score Distribution: <br>\n",
    "* Shows the overall distribution of heuristic scores\n",
    "* Helps understand the general compatibility levels of questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2f3a3ae-cef5-4f95-9ef7-abb276d89ed9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "open"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import explode, split, count\n",
    "\n",
    "pdf = top_open_spark.toPandas()\n",
    "\n",
    "# Set the style for all plots\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "# 1. Distribution of Questions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts = pdf['category'].value_counts()\n",
    "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', \n",
    "        colors=['lightskyblue', 'deepskyblue'], \n",
    "        wedgeprops={'edgecolor': 'white'})\n",
    "plt.title('Distribution of Questions by Category', pad=20, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# 2. Average Heuristic Score by Level\n",
    "plt.figure(figsize=(10, 6))\n",
    "level_scores = pdf.groupby('level')['heuristic_score'].mean().sort_index()\n",
    "seniority_colors = ['thistle', 'plum', 'orchid']\n",
    "bars = plt.bar(level_scores.index, level_scores.values, color=seniority_colors, width=0.4)\n",
    "plt.title('Average Heuristic Score by Seniority Level', pad=20, fontsize=14)\n",
    "plt.xlabel('Seniority Level (0=Junior, 1=Mid, 2=Senior)')\n",
    "plt.ylabel('Average Heuristic Score')\n",
    "# Add value labels on top of bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom')\n",
    "legend_elements = [plt.Rectangle((0,0),1,1, color=color) for color in seniority_colors]\n",
    "plt.legend(legend_elements, ['Junior', 'Mid', 'Senior'], \n",
    "          title='Seniority Level', loc='upper right', bbox_to_anchor=(1.15, 1))\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 3. Top 10 Required Skills\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_skills = pdf['skills'].str.split(',').explode().str.strip()\n",
    "top_skills = all_skills.value_counts().head(10)\n",
    "sns.barplot(x=top_skills.values, y=top_skills.index, palette='RdYlBu')\n",
    "plt.title('Top 10 Required Skills', pad=20, fontsize=14)\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Distribution of Questions by Company Industry\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_industries = pdf['company_industry'].str.split(r', and |, ').explode().str.strip()\n",
    "all_industries = all_industries[all_industries != '-']\n",
    "top_industries = all_industries.value_counts().head(10)\n",
    "sns.barplot(x=top_industries.values, y=top_industries.index, palette='viridis')\n",
    "plt.title('Top 10 Company Industries', pad=20, fontsize=14)\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 5. Correlation between Level and Heuristic Score\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(pdf['level'], pdf['heuristic_score'], color='#FF6B6B', alpha=0.6, s=100)\n",
    "plt.title('Correlation: Seniority Level vs Heuristic Score', pad=20, fontsize=14)\n",
    "plt.xlabel('Seniority Level')\n",
    "plt.ylabel('Heuristic Score')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# 6. Distribution of Questions by Field\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_fields = pdf['field'].str.split(r', and |, ').explode().str.strip()\n",
    "top_fields = all_fields.value_counts().head(10)\n",
    "sns.barplot(x=top_fields.values, y=top_fields.index, palette='mako')\n",
    "plt.title('Top 10 Job Fields', pad=20, fontsize=14)\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 7. Distribution of Topics\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_topics = pdf['topics'].str.split(',').explode().str.strip()\n",
    "top_topics = all_topics.value_counts().head(10)\n",
    "sns.barplot(x=top_topics.values, y=top_topics.index, palette='husl')\n",
    "plt.title('Top 10 Question Topics', pad=20, fontsize=14)\n",
    "plt.xlabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 8. Heuristic Score Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(pdf['heuristic_score'], bins=20, color='pink', alpha=0.7)\n",
    "plt.title('Distribution of Heuristic Scores', pad=20, fontsize=14)\n",
    "plt.xlabel('Heuristic Score')\n",
    "plt.ylabel('Count')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "visualizations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d09861e-d767-4e7d-a2d8-f5452c58b032",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“ŠVisualizations for the unprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e8382d6-2c2b-4ded-83fd-2f5611fb0bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from consts import QUESTIONS_PATH, JOBS_PATH, open_csv_file\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Job and Interview Analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "job_skills_spark = open_csv_file(spark, JOBS_PATH, 'all_jobpostings_with_skills.csv')\n",
    "open_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_open_questions_with_topics.csv')\n",
    "code_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_code_questions_with_topics.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8ce12ce-5be8-4cd5-aff6-91d7331d1ccb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Top 10 most common job titles for each seniority level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425a83eb-1efa-4f0b-926d-60af70ccc5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import when, col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import seaborn as sns\n",
    "\n",
    "# Map levels back to seniority level\n",
    "seniority_map = {\n",
    "    \"0\": \"Internship\",\n",
    "    \"1\": \"Entry level/Associate\",\n",
    "    \"2\": \"Mid-Senior level/Manager and above\"\n",
    "}\n",
    "\n",
    "# Function to normalize job titles (remove seniority/level indicators like \"Senior\", \"I\", etc.)\n",
    "def normalize_job_title(title):\n",
    "    # Define common seniority indicators and level indicators\n",
    "    seniority_terms = ['senior', 'junior', 'lead', 'principal', 'assistant', 'associate']\n",
    "    level_terms = ['i', 'ii', 'iii', 'iv', 'v']  # Handles \"I\", \"II\", etc.\n",
    "\n",
    "    # Normalize case and remove special characters except spaces\n",
    "    title = re.sub(r\"[^a-zA-Z\\s]\", \"\", title.lower())\n",
    "\n",
    "    # Remove seniority and level terms\n",
    "    words = title.split()\n",
    "    filtered_words = [word for word in words if word not in seniority_terms and word not in level_terms]\n",
    "\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# Register the function as a UDF for Spark\n",
    "normalize_job_title_udf = udf(normalize_job_title, StringType())\n",
    "\n",
    "# Clean Data (drop NaN and lowercase)\n",
    "job_skills_cleaned = job_skills_spark \\\n",
    "    .dropna(subset=[\"skills\", \"job_summary\", \"company_industry\", \"field\", \"job_title\"]) \\\n",
    "    .withColumn(\"job_title_normalized\", normalize_job_title_udf(col(\"job_title\"))) \\\n",
    "    .withColumn(\"field\", F.lower(col(\"field\"))) \\\n",
    "    .withColumn(\"seniority_level\",\n",
    "        when(col(\"level\") == \"0\", seniority_map[\"0\"])\n",
    "        .when(col(\"level\") == \"1\", seniority_map[\"1\"])\n",
    "        .when(col(\"level\") == \"2\", seniority_map[\"2\"])\n",
    "    ).drop(\"level\").cache()\n",
    "\n",
    "# Group by normalized job title and seniority level, then count occurrences\n",
    "level_by_title = job_skills_cleaned.groupBy(\"job_title_normalized\", \"seniority_level\") \\\n",
    "    .count().toPandas()\n",
    "\n",
    "# Define the correct order for seniority levels\n",
    "seniority_order = [\"Internship\", \"Entry level/Associate\", \"Mid-Senior level/Manager and above\"]\n",
    "\n",
    "# Convert the seniority_level column into a categorical type with the defined order\n",
    "level_by_title[\"seniority_level\"] = pd.Categorical(\n",
    "    level_by_title[\"seniority_level\"], \n",
    "    categories=seniority_order, \n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Find the **top 10** most common job titles for each seniority level\n",
    "top_titles_per_level = (\n",
    "    level_by_title.sort_values(by=\"count\", ascending=False)\n",
    "    .groupby(\"seniority_level\")\n",
    "    .head(10)  # Select top 10 per level\n",
    ")\n",
    "\n",
    "# Create separate plots for each seniority level\n",
    "fig, axes = plt.subplots(3, 1, figsize=(16, 15))\n",
    "\n",
    "for ax, level in zip(axes, seniority_order):\n",
    "    data = top_titles_per_level[top_titles_per_level[\"seniority_level\"] == level]\n",
    "    \n",
    "    sns.barplot(\n",
    "        x='job_title_normalized', y='count', data=data, \n",
    "        palette=\"Set2\", ax=ax, edgecolor=None\n",
    "    )\n",
    "    \n",
    "    ax.set_title(f\"Top 10 Most Common {level} Job Titles\", fontsize=16)\n",
    "    ax.set_ylabel(\"Count\", fontsize=14)\n",
    "    \n",
    "    # Rotate labels and adjust their position\n",
    "    ax.set_xticklabels(\n",
    "        data['job_title_normalized'],  # Explicitly provide the labels\n",
    "        rotation=45,\n",
    "        ha='right',  # Horizontal alignment\n",
    "        rotation_mode='anchor'  # Rotation point\n",
    "    )\n",
    "    \n",
    "    # Ensure x-axis label is visible\n",
    "    ax.set_xlabel(\"Job Title\", fontsize=14)\n",
    "    \n",
    "    # Adjust layout to prevent label cutoff\n",
    "    ax.tick_params(axis='x', labelsize=10)\n",
    "\n",
    "# Adjust the layout to prevent overlap\n",
    "plt.tight_layout(pad=2.0)  # Increase padding between subplots\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00568f3a-099b-43cd-9ac4-721b9daebacb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### word cloud on the topics for the different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c294f7fe-c414-4f26-99ae-1b0b69c4a55d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lower, split, explode, trim, collect_list, lit, array_except, array\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"WordCloudVisualization\").getOrCreate()\n",
    "\n",
    "# Define stop words, including \"skill\" and \"skills\"\n",
    "stop_words = [\"and\", \"of\", \"the\", \"a\", \"in\", \"to\", \"skill\", \"skills\"]\n",
    "\n",
    "# Process topics column (handle nulls, lowercase, split, explode, trim, and remove stop words)\n",
    "processed_df = (\n",
    "    open_questions_spark\n",
    "    .filter(col(\"topics\").isNotNull())  # Exclude rows with null topics\n",
    "    .withColumn(\"topics\", lower(col(\"topics\")))  # Convert to lowercase\n",
    "    .withColumn(\"topics\", split(col(\"topics\"), \",\\\\s*\"))  # Split by commas\n",
    "    .withColumn(\"topics\", explode(col(\"topics\")))  # Explode into individual sub-topics\n",
    "    .withColumn(\"topics\", trim(col(\"topics\")))  # Trim whitespace\n",
    "    .withColumn(\"topics\", split(col(\"topics\"), \"\\\\s+\"))  # Split multi-word phrases into words\n",
    "    .withColumn(\"topics\", explode(col(\"topics\")))  # Explode words into separate rows\n",
    "    .withColumn(\"topics\", array_except(array(col(\"topics\")), lit(stop_words)))  # Remove stop words\n",
    ")\n",
    "\n",
    "# Aggregate topics for each category\n",
    "category_topics_df = (\n",
    "    processed_df\n",
    "    .groupBy(\"category\")\n",
    "    .agg(collect_list(\"topics\").alias(\"all_topics\"))  # Aggregate topics into a list\n",
    ")\n",
    "\n",
    "# Convert PySpark DataFrame to Pandas for visualization\n",
    "category_topics_pd = category_topics_df.toPandas()\n",
    "\n",
    "# Generate and visualize word clouds\n",
    "for index, row in category_topics_pd.iterrows():\n",
    "    category = row[\"category\"]\n",
    "    topics = \" \".join([str(item) for sublist in row[\"all_topics\"] for item in sublist])\n",
    "\n",
    "    # Generate a word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800,\n",
    "        height=400,\n",
    "        background_color=\"white\",\n",
    "        colormap=\"viridis\",\n",
    "        max_words=100,\n",
    "        contour_color=\"black\",\n",
    "        contour_width=1\n",
    "    ).generate(topics)\n",
    "    \n",
    "    # Plot the word cloud using matplotlib\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(f\"Word Cloud for Category: {category}\", fontsize=16)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2be214e6-ecc1-4d70-b980-547c158c3c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### WordCloud for most in-demand skills\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3213a8e7-c0c9-4a57-89a2-069ea1ca414a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, avg\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Helper Functions\n",
    "def preprocess_skills(data):\n",
    "    \"\"\"Splits skills into individual entries and counts their occurrences.\"\"\"\n",
    "    skills = data.withColumn(\"skill\", explode(split(col(\"skills\"), \", \"))) \\\n",
    "                 .groupBy(\"skill\") \\\n",
    "                 .count() \\\n",
    "                 .orderBy(col(\"count\").desc())\n",
    "    return skills\n",
    "\n",
    "# Most In-Demand Skills\n",
    "def most_in_demand_skills(data):\n",
    "    skills = preprocess_skills(data).toPandas()\n",
    "    wordcloud = WordCloud(background_color=\"white\").generate_from_frequencies(dict(zip(skills[\"skill\"], skills[\"count\"])))\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Most In-Demand Skills\")\n",
    "    plt.show()\n",
    "\n",
    "most_in_demand_skills(job_skills_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f17e46f3-59d2-42ec-9412-f6fa9d1626c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Questions' Difficulty level distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf530001-a345-4295-9036-646e61d393bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, explode, split, count, avg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Questions by Difficulty\n",
    "def questions_by_difficulty(data):\n",
    "    difficulty_counts = data.groupBy(\"difficulty\").count().orderBy(col(\"count\").desc()).toPandas()\n",
    "    difficulty_counts.plot(kind=\"pie\", y=\"count\", labels=difficulty_counts[\"difficulty\"], colors=sns.color_palette('Set2'), autopct=\"%1.1f%%\")\n",
    "    plt.title(\"Questions by Difficulty\")\n",
    "    plt.ylabel(\"\")\n",
    "    plt.show()\n",
    "\n",
    "questions_by_difficulty(code_questions_spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44c6b642-ba4c-433f-b64c-e3901998fcaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### difficulty vs. acceptance + difficulty vs. num of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b65ec77-cec7-4643-8230-9679fc9e2a72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, split, size\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "code_questions_spark = code_questions_spark.withColumn(\"num_topics\", size(split(col(\"topics\"), \",\")))\n",
    "\n",
    "df_grouped = code_questions_spark.groupBy(\"difficulty\").agg(\n",
    "    {\"acceptance\": \"avg\", \"num_topics\": \"avg\"}\n",
    ").withColumnRenamed(\"avg(acceptance)\", \"avg_acceptance\").withColumnRenamed(\"avg(num_topics)\", \"avg_num_topics\")\n",
    "\n",
    "df_grouped_pd = df_grouped.toPandas()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Plot difficulty vs average acceptance\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.barplot(x=\"difficulty\", y=\"avg_acceptance\", data=df_grouped_pd, palette = sns.color_palette(\"Set2\"))\n",
    "plt.title(\"Difficulty vs Average Acceptance\")\n",
    "plt.xlabel(\"Difficulty\")\n",
    "plt.ylabel(\"Average Acceptance\")\n",
    "\n",
    "# Plot difficulty vs average number of topics\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.barplot(x=\"difficulty\", y=\"avg_num_topics\", data=df_grouped_pd, palette = sns.color_palette(\"Set2\"))\n",
    "plt.title(\"Difficulty vs Average Number of Topics\")\n",
    "plt.xlabel(\"Difficulty\")\n",
    "plt.ylabel(\"Average Number of Topics\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "242e662c-6ebc-4f6d-9c9c-d3c1431b614e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ðŸ“ŠVisualizations for the processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfb3c3d6-01dd-4dad-97e3-701e6992cce5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from consts import DATA_PATH, QUESTIONS_PATH, open_csv_file\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Job and Interview Analysis\") \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "top_code_spark = open_csv_file(spark, DATA_PATH, 'top_code_questions.csv')\n",
    "all_questions_spark = open_csv_file(spark, QUESTIONS_PATH, 'all_code_questions_with_topics.csv').select(\"question_id\", \"difficulty\")\n",
    "\n",
    "top_jointed_code = top_code_spark.join(all_questions_spark, on=\"question_id\", how=\"left\")\n",
    "top_open_spark = open_csv_file(spark, DATA_PATH, 'top_open_questions.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "208ac4fc-8509-430f-9506-84245d126995",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Code questions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "48770aed-b63d-48ff-944a-a49d919b6883",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "pd_top_jointed_code = top_jointed_code.toPandas()\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "colors = sns.color_palette(\"husl\", 8)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(20, 20))\n",
    "\n",
    "# Distribution of Seniority Levels\n",
    "level_counts = pd_top_jointed_code['level'].value_counts().sort_index()\n",
    "axes[0,0].pie(level_counts.values, labels=['Junior', 'Mid', 'Senior'], \n",
    "              autopct='%1.1f%%', colors=colors[:3],\n",
    "              wedgeprops=dict(width=0.7))\n",
    "axes[0,0].set_title('Distribution of Seniority Levels', pad=20)\n",
    "\n",
    "# Top 10 Required Skills\n",
    "all_skills = [skill.strip() for skills in pd_top_jointed_code['skills'].dropna() \n",
    "              for skill in skills.split(',')]\n",
    "top_skills = pd.Series(Counter(all_skills)).sort_values(ascending=True)[-10:]\n",
    "sns.barplot(y=top_skills.index, x=top_skills.values, palette=colors, ax=axes[0,1])\n",
    "axes[0,1].set_title('Top 10 Required Skills', pad=20)\n",
    "axes[0,1].set_xlabel('Frequency')\n",
    "\n",
    "# Question Topics Distribution\n",
    "all_topics = [topic.strip() for topics in pd_top_jointed_code['topics'].dropna() \n",
    "              for topic in topics.split(',')]\n",
    "top_topics = pd.Series(Counter(all_topics)).sort_values(ascending=True)[-10:]\n",
    "sns.barplot(y=top_topics.index, x=top_topics.values, palette=colors, ax=axes[1,0])\n",
    "axes[1,0].set_title('Top 10 Question Topics', pad=20)\n",
    "axes[1,0].set_xlabel('Frequency')\n",
    "\n",
    "# Top Industries\n",
    "all_industries = [ind.strip() for industries in pd_top_jointed_code['company_industry'].dropna() \n",
    "                 for ind in industries.split(',')]\n",
    "all_industries = [ind for ind in all_industries if ind != \"-\"]\n",
    "top_industries = pd.Series(Counter(all_industries)).sort_values(ascending=True)[-10:]\n",
    "sns.barplot(y=top_industries.index, x=top_industries.values, palette=colors, ax=axes[1,1])\n",
    "axes[1,1].set_title('Top 10 Industries', pad=20)\n",
    "axes[1,1].set_xlabel('Frequency')\n",
    "\n",
    "# Difficulty Distribution by Level\n",
    "difficulty_level = pd.crosstab(pd_top_jointed_code['difficulty'], \n",
    "                             pd_top_jointed_code['level'])\n",
    "# Reorder the index\n",
    "difficulty_level = difficulty_level.reindex(['Easy', 'Medium', 'Hard'])\n",
    "\n",
    "difficulty_level.plot(kind='bar', stacked=True, color=colors[:3], ax=axes[2,0])\n",
    "axes[2,0].set_title('Question Difficulty Distribution by Seniority Level', pad=20)\n",
    "axes[2,0].set_xlabel('Difficulty')\n",
    "axes[2,0].set_ylabel('Count')\n",
    "axes[2,0].tick_params(axis='x', rotation=0)\n",
    "axes[2,0].legend(['Junior', 'Mid', 'Senior'], bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Top Companies by Average Score\n",
    "company_scores = pd_top_jointed_code.groupby('company_name')['heuristic_score'].mean()\\\n",
    "                .sort_values(ascending=True).tail(10)\n",
    "sns.barplot(y=company_scores.index, x=company_scores.values, palette=colors, ax=axes[2,1])\n",
    "axes[2,1].set_title('Top 10 Companies by Average Heuristic Score', pad=20)\n",
    "axes[2,1].set_xlabel('Average Score')\n",
    "\n",
    "# Add labels on bars for better readability\n",
    "for bar, score in zip(axes[2,1].containers[0], company_scores.values):\n",
    "    axes[2,1].bar_label(axes[2,1].containers[0], fmt='%.3f', label_type='center', padding=5)\n",
    "\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8c7dd00-947f-4699-b592-c45052496f14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Open questions analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a70d423-dd90-4625-95df-de74705dbcd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pyspark.sql.functions import explode, split, count\n",
    "\n",
    "# Convert Spark DataFrame to Pandas DataFrame\n",
    "top_open_pd = top_open_spark.toPandas()\n",
    "\n",
    "# Set the seaborn theme for better aesthetics\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "# Distribution of Questions by Category\n",
    "plt.figure(figsize=(10, 6))\n",
    "category_counts = top_open_pd['category'].value_counts()\n",
    "colors = sns.color_palette(\"Blues\", len(category_counts))\n",
    "plt.pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%', \n",
    "        colors=colors, wedgeprops={'edgecolor': 'white'})\n",
    "plt.title('Distribution of Questions by Category', pad=20, fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Questions by Field\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_fields = top_open_pd['field'].str.split(r', and |, ').explode().str.strip()\n",
    "top_fields = all_fields.value_counts().head(10)\n",
    "sns.barplot(x=top_fields.values, y=top_fields.index, palette='husl')\n",
    "plt.title('Top 10 Job Fields', pad=20, fontsize=14)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Fields')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Distribution of Topics\n",
    "plt.figure(figsize=(10, 6))\n",
    "all_topics = top_open_pd['topics'].str.split(',').explode().str.strip()\n",
    "top_topics = all_topics.value_counts().head(10)\n",
    "sns.barplot(x=top_topics.values, y=top_topics.index, palette='husl')\n",
    "plt.title('Top 10 Question Topics', pad=20, fontsize=14)\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Topics')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "visualizations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
